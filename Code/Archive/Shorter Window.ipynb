{"cells":[{"cell_type":"markdown","id":"00ca84cc","metadata":{"id":"00ca84cc"},"source":["## Imports"]},{"cell_type":"code","execution_count":1,"id":"a079e172","metadata":{"id":"a079e172"},"outputs":[],"source":["import csv\n","import obspy\n","from obspy import signal\n","import obspy.signal.filter\n","from tqdm import tqdm\n","import tensorflow as tf\n","import numpy as np\n","import random\n","import pandas as pd\n","from joblib import dump, load\n","import os\n","from datetime import datetime"]},{"cell_type":"code","execution_count":3,"id":"853e6a94","metadata":{},"outputs":[],"source":["CONTEXT_WINDOW = 90\n","SAMPLES = 7201"]},{"cell_type":"markdown","id":"85a6b85c","metadata":{},"source":["## Positive Examples"]},{"cell_type":"code","execution_count":4,"id":"900b0503","metadata":{},"outputs":[],"source":["#get all events\n","def get_full_event_list(path = \"../Data/allevents.csv\"):\n","    all = pd.read_csv(path)\n","    keep = ['year', 'day', 'date', 'oTime', 'spTime', 'distance', 'duration', 'quality']\n","    events = all[keep]\n","    events = events[events['year']> 1999]\n","    return events[4:]"]},{"cell_type":"code","execution_count":5,"id":"94c9e7de","metadata":{},"outputs":[],"source":["def slice_and_filter(time, stream):\n","    stream = stream.slice(starttime=time, endtime=time+CONTEXT_WINDOW)\n","\n","    #filtered_data = obspy.signal.filter.highpass(stream, freq=2.5, df=5.0001)\n","\n","    #stream.data = filtered_data\n","    return stream"]},{"cell_type":"code","execution_count":6,"id":"38fd1f0d","metadata":{"id":"38fd1f0d","scrolled":true},"outputs":[],"source":["def get_positive_data(path, event_list):\n","    clean_data = []\n","    \n","    for _, row in event_list.iterrows():\n","        HHE = path+f\"{row['year']}/MN/WDD/HHE.D/MN.WDD..HHE.D.{row['year']}.{str(row['day']).zfill(3)}\"\n","        HHN = path+f\"{row['year']}/MN/WDD/HHN.D/MN.WDD..HHN.D.{row['year']}.{str(row['day']).zfill(3)}\"\n","        HHZ = path+f\"{row['year']}/MN/WDD/HHZ.D/MN.WDD..HHZ.D.{row['year']}.{str(row['day']).zfill(3)}\"\n","        \n","        try:    \n","            st = obspy.read(HHE)\n","            st += obspy.read(HHN)\n","            st += obspy.read(HHZ)\n","            quakeTime = obspy.UTCDateTime(row['date']+row['oTime'])\n","            st = slice_and_filter(quakeTime, st)\n","            if len(st) == 3 and len(st[0]) == SAMPLES and len(st[1]) == SAMPLES and len(st[2]) == SAMPLES:\n","                clean_data.append(st)\n","    \n","        except Exception as e:\n","            print(f\"Missing File {row['year']}, {row['day']}\")\n","            continue\n","\n","    return clean_data"]},{"cell_type":"code","execution_count":7,"id":"c6893cbe-1651-4c6a-b778-29a999290fa4","metadata":{"id":"c6893cbe-1651-4c6a-b778-29a999290fa4"},"outputs":[],"source":["def make_utc_list(events):\n","    utc_times = []\n","    for _, row in events.iterrows():\n","        utc_times.append(obspy.UTCDateTime(row['date']+row['oTime']))\n","    return utc_times"]},{"cell_type":"code","execution_count":8,"id":"e1037eae-f616-4b12-80e0-d1d53a2403a6","metadata":{"id":"e1037eae-f616-4b12-80e0-d1d53a2403a6"},"outputs":[],"source":["def create_neg_examples(path, utc_times, amount, years):\n","    # Initialize an empty list to store the negative examples\n","    negative_examples = []\n","\n","    # Loop to generate random negative examples\n","    while len(negative_examples) < amount:\n","\n","        ran_day = random.randint(1, 365)\n","        ran_hr = random.randint(0, 23)\n","        ran_min = random.randint(0, 59)\n","        ran_sec = random.randint(0, 59)\n","        ran_year = random.choice(years)\n","\n","        if ran_day == 97 and ran_year == 2010:\n","            continue\n","\n","        # Generate a random UTC time on the selected day\n","        random_utc_time = obspy.UTCDateTime(year=ran_year, julday=ran_day, hour=ran_hr, minute=ran_min, second=ran_sec)\n","\n","        flag = False\n","\n","        for time in utc_times:\n","            #if the window being created is in an earthquake window\n","            if random_utc_time < time + CONTEXT_WINDOW and random_utc_time > time:\n","                flag = True\n","\n","        if not flag:\n","            HHE = path+f\"{ran_year}/MN/WDD/HHE.D/MN.WDD..HHE.D.{ran_year}.{str(ran_day).zfill(3)}\"\n","            HHN = path+f\"{ran_year}/MN/WDD/HHN.D/MN.WDD..HHN.D.{ran_year}.{str(ran_day).zfill(3)}\"\n","            HHZ = path+f\"{ran_year}/MN/WDD/HHZ.D/MN.WDD..HHZ.D.{ran_year}.{str(ran_day).zfill(3)}\"\n","\n","            try:\n","                st = obspy.read(HHE)\n","                st += obspy.read(HHN)\n","                st += obspy.read(HHZ)\n","\n","                st = slice_and_filter(random_utc_time, st)\n","                if len(st) == 3 and len(st[0]) == SAMPLES and len(st[1]) == SAMPLES and len(st[2]) == SAMPLES:\n","                    negative_examples.append(st)\n","\n","            except Exception as e:\n","                continue\n","    return negative_examples"]},{"cell_type":"markdown","id":"f853ec7a","metadata":{},"source":["# Run The Above"]},{"cell_type":"code","execution_count":9,"id":"01adcbd8","metadata":{"collapsed":true,"id":"01adcbd8","jupyter":{"outputs_hidden":true},"outputId":"f4405f0a-8974-4c0e-8fbc-c35aacd970b5","tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Missing File 2003, 124\n","Missing File 2002, 355\n","Missing File 2002, 229\n","Missing File 2002, 205\n","Missing File 2001, 290\n","Missing File 2001, 242\n","Missing File 2001, 242\n","Missing File 2001, 240\n","Missing File 2001, 231\n","Missing File 2001, 231\n","Missing File 2001, 230\n","Missing File 2001, 226\n","Missing File 2001, 71\n","Missing File 2001, 15\n","Missing File 2000, 261\n","Missing File 2000, 204\n","Missing File 2000, 129\n","Missing File 2000, 126\n","Missing File 2003, 245\n","Missing File 2002, 354\n","Missing File 2002, 328\n","Missing File 2002, 305\n","Missing File 2002, 194\n","Missing File 2001, 273\n","Missing File 2001, 245\n","Missing File 2001, 242\n","Missing File 2001, 229\n","Missing File 2000, 244\n","Missing File 2000, 239\n"]}],"source":["#define all years\n","years = list(range(2000, 2015))\n","#get whole event list\n","all_events = get_full_event_list('../Data/allevents.csv')\n","\n","local_events = all_events[all_events['spTime'] <= 20]\n","distant_events = all_events[all_events['spTime'] > 20]\n","\n","#get +ve examples\n","local_earthquake_data = np.array(get_positive_data('../Data/', local_events))\n","#generate +ve labels\n","local_earthquake_labels = np.asarray([1] * len(local_earthquake_data))\n","\n","distant_earthquake_data = np.array(get_positive_data('../Data/', distant_events))\n","#generate +ve labels\n","distant_earthquake_labels = np.asarray([2] * len(distant_earthquake_data))\n","\n","utc_times = make_utc_list(all_events)\n","\n","neg_examples = create_neg_examples('../Data/', utc_times, len(distant_earthquake_labels)+len(local_earthquake_labels), years)\n","negative_labels = np.asarray([0] * len(neg_examples))\n","\n","combined_data = np.concatenate((local_earthquake_data, distant_earthquake_data, neg_examples), axis=0)\n","combined_labels = np.concatenate((local_earthquake_labels, distant_earthquake_labels, negative_labels), axis=0)"]},{"cell_type":"markdown","id":"1c4cebcf","metadata":{},"source":["FFT"]},{"cell_type":"code","execution_count":40,"id":"f13ac7a2","metadata":{},"outputs":[],"source":["def apply_fft(data):\n","    fft_data = np.fft.fft(data)\n","    return np.abs(fft_data)"]},{"cell_type":"code","execution_count":39,"id":"e824ac9c","metadata":{},"outputs":[{"ename":"NameError","evalue":"name 'combined_data' is not defined","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[1;32mIn[39], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mabs(fft_data)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Apply FFT to combined_data\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m combined_data_fft \u001b[38;5;241m=\u001b[39m [apply_fft(sample) \u001b[38;5;28;01mfor\u001b[39;00m sample \u001b[38;5;129;01min\u001b[39;00m \u001b[43mcombined_data\u001b[49m]\n","\u001b[1;31mNameError\u001b[0m: name 'combined_data' is not defined"]}],"source":["# Apply FFT to combined_data\n","combined_data_fft = [apply_fft(sample) for sample in combined_data]"]},{"cell_type":"markdown","id":"058b7779-a95b-47e4-930f-2f7946c22de9","metadata":{"id":"058b7779-a95b-47e4-930f-2f7946c22de9"},"source":["Dump Data"]},{"cell_type":"code","execution_count":11,"id":"9a0bf8a3","metadata":{},"outputs":[{"data":{"text/plain":["['../DataDumps/rawishdata.joblib']"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["dump(combined_data, '../DataDumps/rawishdata.joblib')"]},{"cell_type":"markdown","id":"b9b424fd","metadata":{},"source":["Dump 2-Class"]},{"cell_type":"code","execution_count":57,"id":"97341931-a822-4faa-b0ab-3390ddc0bf32","metadata":{"id":"97341931-a822-4faa-b0ab-3390ddc0bf32","outputId":"9933dae1-f75a-4667-d5b7-40106a167a4e"},"outputs":[{"data":{"text/plain":["['../DataDumps/2class_labels.joblib']"]},"execution_count":57,"metadata":{},"output_type":"execute_result"}],"source":["#combined_labels[combined_labels == 2] = 1\n","# Save the combined_data and combined_labels\n","dump(np.array(combined_data_fft), '../DataDumps/all_data_fft.joblib')\n","dump(combined_labels, '../DataDumps/2class_labels.joblib')"]},{"cell_type":"markdown","id":"59ef4dd5","metadata":{},"source":["Dump 3-Class"]},{"cell_type":"code","execution_count":33,"id":"f95cd333","metadata":{},"outputs":[],"source":["def balance_data(data, labels):\n","    # Find the indices corresponding to each label\n","    indices_0 = np.where(labels == 0)[0]\n","    indices_1 = np.where(labels == 1)[0]\n","    indices_2 = np.where(labels == 2)[0]\n","    \n","    # Find the minimum number of samples among all labels\n","    min_samples = min(len(indices_0), len(indices_1), len(indices_2))\n","    \n","    # Randomly select min_samples for each label\n","    selected_indices_0 = np.random.choice(indices_0, min_samples, replace=False)\n","    selected_indices_1 = np.random.choice(indices_1, min_samples, replace=False)\n","    selected_indices_2 = np.random.choice(indices_2, min_samples, replace=False)\n","    \n","    # Concatenate the selected indices\n","    selected_indices = np.concatenate([selected_indices_0, selected_indices_1, selected_indices_2])\n","    \n","    # Extract the balanced data and labels\n","    balanced_data = data[selected_indices]\n","    balanced_labels = labels[selected_indices]\n","    \n","    return balanced_data, balanced_labels"]},{"cell_type":"code","execution_count":28,"id":"fcec694d","metadata":{},"outputs":[],"source":["raw = load('../DataDumps/rawishdata.joblib')\n","labels = load('../DataDumps/all_labels.joblib')"]},{"cell_type":"code","execution_count":46,"id":"639ed9c6","metadata":{},"outputs":[{"data":{"text/plain":["['../DataDumps/3class_labels.joblib']"]},"execution_count":46,"metadata":{},"output_type":"execute_result"}],"source":["balanced_data, balanced_labels = balance_data(raw, labels)\n","# Save the combined_data and combined_labels\n","dump(np.array(balanced_data), '../DataDumps/3classraw.joblib')\n","dump(balanced_labels, '../DataDumps/3class_labels.joblib')"]},{"cell_type":"code","execution_count":43,"id":"a0efdde3","metadata":{},"outputs":[],"source":["balanced_data_fft = [apply_fft(sample) for sample in balanced_data]\n"]},{"cell_type":"code","execution_count":47,"id":"3d595a58","metadata":{},"outputs":[{"data":{"text/plain":["['../DataDumps/3classfft.joblib']"]},"execution_count":47,"metadata":{},"output_type":"execute_result"}],"source":["dump(np.array(balanced_data_fft), '../DataDumps/3classfft.joblib')"]},{"cell_type":"markdown","id":"a07c06fb","metadata":{},"source":["# Spectral Subtraction"]},{"cell_type":"code","execution_count":46,"id":"3e062766","metadata":{},"outputs":[],"source":["neg = combined_data_fft[1037:]"]},{"cell_type":"code","execution_count":47,"id":"d157c980","metadata":{},"outputs":[],"source":["avg_noise_spectrum = np.mean(neg, axis=0)"]},{"cell_type":"code","execution_count":48,"id":"1ce566bf","metadata":{},"outputs":[],"source":["# Define function for spectral subtraction\n","def spectral_subtraction(earthquake_data_fft, avg_noise_spectrum):\n","    # Subtract noise spectrum from earthquake data spectrum\n","    cleaned_data_fft = earthquake_data_fft - avg_noise_spectrum\n","    return cleaned_data_fft\n","\n","# Apply spectral subtraction to earthquake data\n","cleaned_data_fft = [spectral_subtraction(sample, avg_noise_spectrum) for sample in combined_data_fft]\n","\n","# Dump cleaned data to file\n"]},{"cell_type":"code","execution_count":49,"id":"70818e4c","metadata":{},"outputs":[{"data":{"text/plain":["['../DataDumps/SpectralSubt.joblib']"]},"execution_count":49,"metadata":{},"output_type":"execute_result"}],"source":["dump(np.array(cleaned_data_fft), '../DataDumps/SpectralSubt.joblib')"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.18"}},"nbformat":4,"nbformat_minor":5}
