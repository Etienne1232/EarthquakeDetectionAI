{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-24 01:15:19,016] A new study created in memory with name: no-name-e16c7edf-e2a6-4364-a5d3-6270977b9271\n",
      "[I 2024-04-24 01:15:38,852] Trial 0 finished with value: 0.6008928775787353 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 82, 'dropout_1': 0.45402343195199063, 'optimizer': 'RMSprop', 'lstm_units_2': 39, 'dropout_2': 0.331961180864218}. Best is trial 0 with value: 0.6008928775787353.\n",
      "[I 2024-04-24 01:15:58,760] Trial 1 finished with value: 0.5562500119209289 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 58, 'dropout_1': 0.21821507332346118, 'optimizer': 'RMSprop', 'lstm_units_2': 89, 'dropout_2': 0.4592522889047801}. Best is trial 0 with value: 0.6008928775787353.\n",
      "[I 2024-04-24 01:16:25,678] Trial 2 finished with value: 0.5973214387893677 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 58, 'dropout_1': 0.23171983563400173, 'optimizer': 'RMSprop', 'lstm_units_2': 52, 'dropout_2': 0.12020686750201609, 'lstm_units_3': 110, 'dropout_3': 0.22828176644524417}. Best is trial 0 with value: 0.6008928775787353.\n",
      "[I 2024-04-24 01:16:52,713] Trial 3 finished with value: 0.5946428537368774 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 70, 'dropout_1': 0.011527240129772187, 'optimizer': 'RMSprop', 'lstm_units_2': 123, 'dropout_2': 0.037585675233458626, 'lstm_units_3': 34, 'dropout_3': 0.15387432472524926}. Best is trial 0 with value: 0.6008928775787353.\n",
      "[I 2024-04-24 01:17:16,390] Trial 4 finished with value: 0.47053571343421935 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 51, 'dropout_1': 0.21456803615654269, 'optimizer': 'SGD', 'lstm_units_2': 79, 'dropout_2': 0.26184509371909276, 'lstm_units_3': 51, 'dropout_3': 0.30758597192881615}. Best is trial 0 with value: 0.6008928775787353.\n",
      "[I 2024-04-24 01:17:35,492] Trial 5 finished with value: 0.5508928418159484 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 42, 'dropout_1': 0.09249401287218734, 'optimizer': 'RMSprop', 'lstm_units_2': 46, 'dropout_2': 0.0372974006250803}. Best is trial 0 with value: 0.6008928775787353.\n",
      "[I 2024-04-24 01:17:58,679] Trial 6 finished with value: 0.48571428656578064 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 93, 'dropout_1': 0.05108790777046901, 'optimizer': 'SGD', 'lstm_units_2': 55, 'dropout_2': 0.2023286126613542, 'lstm_units_3': 100, 'dropout_3': 0.49679273387349127}. Best is trial 0 with value: 0.6008928775787353.\n",
      "[I 2024-04-24 01:18:22,230] Trial 7 finished with value: 0.4714285731315613 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 46, 'dropout_1': 0.2684622656799592, 'optimizer': 'SGD', 'lstm_units_2': 77, 'dropout_2': 0.18113785479487243, 'lstm_units_3': 78, 'dropout_3': 0.036594988605630874}. Best is trial 0 with value: 0.6008928775787353.\n",
      "[I 2024-04-24 01:18:46,038] Trial 8 finished with value: 0.5785714387893677 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 72, 'dropout_1': 0.35382308571017346, 'optimizer': 'Adam', 'lstm_units_2': 126, 'dropout_2': 0.35269104145884306, 'lstm_units_3': 125, 'dropout_3': 0.20921276319406684}. Best is trial 0 with value: 0.6008928775787353.\n",
      "[I 2024-04-24 01:18:57,066] Trial 9 finished with value: 0.5339285671710968 and parameters: {'num_lstm_layers': 1, 'lstm_units_1': 84, 'dropout_1': 0.4201222244715553, 'optimizer': 'SGD'}. Best is trial 0 with value: 0.6008928775787353.\n",
      "[I 2024-04-24 01:19:09,819] Trial 10 finished with value: 0.5446428537368775 and parameters: {'num_lstm_layers': 1, 'lstm_units_1': 120, 'dropout_1': 0.49066168564877294, 'optimizer': 'Adam'}. Best is trial 0 with value: 0.6008928775787353.\n",
      "[I 2024-04-24 01:19:29,593] Trial 11 finished with value: 0.5883928418159485 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 103, 'dropout_1': 0.31355483995537403, 'optimizer': 'RMSprop', 'lstm_units_2': 32, 'dropout_2': 0.3586765285055682}. Best is trial 0 with value: 0.6008928775787353.\n",
      "[I 2024-04-24 01:19:48,864] Trial 12 finished with value: 0.575 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 65, 'dropout_1': 0.13307721582306073, 'optimizer': 'RMSprop', 'lstm_units_2': 56, 'dropout_2': 0.1299880533045381}. Best is trial 0 with value: 0.6008928775787353.\n",
      "[I 2024-04-24 01:20:01,353] Trial 13 finished with value: 0.4651785671710968 and parameters: {'num_lstm_layers': 1, 'lstm_units_1': 35, 'dropout_1': 0.39457842765194895, 'optimizer': 'RMSprop'}. Best is trial 0 with value: 0.6008928775787353.\n",
      "[I 2024-04-24 01:20:20,668] Trial 14 finished with value: 0.5875000119209289 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 86, 'dropout_1': 0.15298101806183692, 'optimizer': 'RMSprop', 'lstm_units_2': 34, 'dropout_2': 0.3123795127927393}. Best is trial 0 with value: 0.6008928775787353.\n",
      "[I 2024-04-24 01:20:40,720] Trial 15 finished with value: 0.5785714268684388 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 110, 'dropout_1': 0.4921944629569742, 'optimizer': 'RMSprop', 'lstm_units_2': 62, 'dropout_2': 0.45676711815754134}. Best is trial 0 with value: 0.6008928775787353.\n",
      "[I 2024-04-24 01:21:04,443] Trial 16 finished with value: 0.5741071462631225 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 76, 'dropout_1': 0.2959055798105613, 'optimizer': 'Adam', 'lstm_units_2': 44, 'dropout_2': 0.11752234537402444, 'lstm_units_3': 119, 'dropout_3': 0.3606161050396697}. Best is trial 0 with value: 0.6008928775787353.\n",
      "[I 2024-04-24 01:21:17,094] Trial 17 finished with value: 0.5062499940395355 and parameters: {'num_lstm_layers': 1, 'lstm_units_1': 57, 'dropout_1': 0.20923355388247672, 'optimizer': 'RMSprop'}. Best is trial 0 with value: 0.6008928775787353.\n",
      "[I 2024-04-24 01:21:36,454] Trial 18 finished with value: 0.5910714149475098 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 96, 'dropout_1': 0.42418669778680684, 'optimizer': 'RMSprop', 'lstm_units_2': 98, 'dropout_2': 0.26554223025087537}. Best is trial 0 with value: 0.6008928775787353.\n",
      "[I 2024-04-24 01:22:01,664] Trial 19 finished with value: 0.5982142925262451 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 128, 'dropout_1': 0.3525624497073515, 'optimizer': 'Adam', 'lstm_units_2': 66, 'dropout_2': 0.10804482978686902, 'lstm_units_3': 90, 'dropout_3': 0.07731094952928419}. Best is trial 0 with value: 0.6008928775787353.\n",
      "[I 2024-04-24 01:22:20,165] Trial 20 finished with value: 0.575 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 128, 'dropout_1': 0.36105726822529943, 'optimizer': 'Adam', 'lstm_units_2': 68, 'dropout_2': 0.41499764566679365}. Best is trial 0 with value: 0.6008928775787353.\n",
      "[I 2024-04-24 01:22:44,521] Trial 21 finished with value: 0.60625 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 112, 'dropout_1': 0.45535610182105085, 'optimizer': 'Adam', 'lstm_units_2': 48, 'dropout_2': 0.11701635247717844, 'lstm_units_3': 89, 'dropout_3': 0.0070212088941573325}. Best is trial 21 with value: 0.60625.\n",
      "[I 2024-04-24 01:23:09,343] Trial 22 finished with value: 0.6035714387893677 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 114, 'dropout_1': 0.4482419687197178, 'optimizer': 'Adam', 'lstm_units_2': 41, 'dropout_2': 0.0069461812479276325, 'lstm_units_3': 82, 'dropout_3': 0.010420373450098473}. Best is trial 21 with value: 0.60625.\n",
      "[I 2024-04-24 01:23:33,960] Trial 23 finished with value: 0.6133928537368775 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 113, 'dropout_1': 0.45136839711506527, 'optimizer': 'Adam', 'lstm_units_2': 42, 'dropout_2': 0.009385838043657944, 'lstm_units_3': 73, 'dropout_3': 0.01997881924812}. Best is trial 23 with value: 0.6133928537368775.\n",
      "[I 2024-04-24 01:23:58,286] Trial 24 finished with value: 0.5517857074737549 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 112, 'dropout_1': 0.45739593677881013, 'optimizer': 'Adam', 'lstm_units_2': 47, 'dropout_2': 0.0002077310816576694, 'lstm_units_3': 73, 'dropout_3': 0.002446579066715516}. Best is trial 23 with value: 0.6133928537368775.\n",
      "[I 2024-04-24 01:24:22,417] Trial 25 finished with value: 0.5794642806053162 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 108, 'dropout_1': 0.3927362725399614, 'optimizer': 'Adam', 'lstm_units_2': 39, 'dropout_2': 0.05950243878631861, 'lstm_units_3': 66, 'dropout_3': 0.101736242863678}. Best is trial 23 with value: 0.6133928537368775.\n",
      "[I 2024-04-24 01:24:47,458] Trial 26 finished with value: 0.5562500119209289 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 119, 'dropout_1': 0.45571586661673713, 'optimizer': 'Adam', 'lstm_units_2': 105, 'dropout_2': 0.007126819210159207, 'lstm_units_3': 87, 'dropout_3': 0.0027665882659029693}. Best is trial 23 with value: 0.6133928537368775.\n",
      "[I 2024-04-24 01:25:12,437] Trial 27 finished with value: 0.5687500059604644 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 95, 'dropout_1': 0.4931323444376847, 'optimizer': 'Adam', 'lstm_units_2': 57, 'dropout_2': 0.08417132453279298, 'lstm_units_3': 67, 'dropout_3': 0.10705462276326619}. Best is trial 23 with value: 0.6133928537368775.\n",
      "[I 2024-04-24 01:25:36,960] Trial 28 finished with value: 0.5848214387893677 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 118, 'dropout_1': 0.41504114720633767, 'optimizer': 'Adam', 'lstm_units_2': 42, 'dropout_2': 0.17282651200728233, 'lstm_units_3': 96, 'dropout_3': 0.06147243432219593}. Best is trial 23 with value: 0.6133928537368775.\n",
      "[I 2024-04-24 01:26:00,898] Trial 29 finished with value: 0.5723214149475098 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 99, 'dropout_1': 0.4545623320460435, 'optimizer': 'Adam', 'lstm_units_2': 71, 'dropout_2': 0.07244584368267747, 'lstm_units_3': 57, 'dropout_3': 0.13369263810089063}. Best is trial 23 with value: 0.6133928537368775.\n",
      "[I 2024-04-24 01:26:25,187] Trial 30 finished with value: 0.5642856955528259 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 104, 'dropout_1': 0.3760008378646549, 'optimizer': 'Adam', 'lstm_units_2': 33, 'dropout_2': 0.03542663523792223, 'lstm_units_3': 83, 'dropout_3': 0.04433532597171685}. Best is trial 23 with value: 0.6133928537368775.\n",
      "[I 2024-04-24 01:26:42,679] Trial 31 finished with value: 0.5705357193946838 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 88, 'dropout_1': 0.4541745526177652, 'optimizer': 'Adam', 'lstm_units_2': 39, 'dropout_2': 0.32938909005256045}. Best is trial 23 with value: 0.6133928537368775.\n",
      "[I 2024-04-24 01:27:01,382] Trial 32 finished with value: 0.5928571462631226 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 114, 'dropout_1': 0.3204944272944441, 'optimizer': 'Adam', 'lstm_units_2': 50, 'dropout_2': 0.22770093000348063}. Best is trial 23 with value: 0.6133928537368775.\n",
      "[I 2024-04-24 01:27:19,670] Trial 33 finished with value: 0.6116071462631225 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 122, 'dropout_1': 0.4302443183944383, 'optimizer': 'Adam', 'lstm_units_2': 41, 'dropout_2': 0.39189898528479805}. Best is trial 23 with value: 0.6133928537368775.\n",
      "[I 2024-04-24 01:27:44,580] Trial 34 finished with value: 0.6044642925262451 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 123, 'dropout_1': 0.42604216157052804, 'optimizer': 'Adam', 'lstm_units_2': 61, 'dropout_2': 0.14628129048355326, 'lstm_units_3': 105, 'dropout_3': 0.15196397194308817}. Best is trial 23 with value: 0.6133928537368775.\n",
      "[I 2024-04-24 01:28:03,132] Trial 35 finished with value: 0.5937500119209289 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 123, 'dropout_1': 0.4216630152721006, 'optimizer': 'Adam', 'lstm_units_2': 60, 'dropout_2': 0.15415684035743052}. Best is trial 23 with value: 0.6133928537368775.\n",
      "[I 2024-04-24 01:28:28,293] Trial 36 finished with value: 0.56875 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 122, 'dropout_1': 0.33731747720928085, 'optimizer': 'Adam', 'lstm_units_2': 50, 'dropout_2': 0.40040207499636304, 'lstm_units_3': 104, 'dropout_3': 0.1587663211194545}. Best is trial 23 with value: 0.6133928537368775.\n",
      "[I 2024-04-24 01:28:46,841] Trial 37 finished with value: 0.5964285731315613 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 104, 'dropout_1': 0.3967538660395449, 'optimizer': 'Adam', 'lstm_units_2': 89, 'dropout_2': 0.499579012149041}. Best is trial 23 with value: 0.6133928537368775.\n",
      "[I 2024-04-24 01:29:11,364] Trial 38 finished with value: 0.46785714030265807 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 125, 'dropout_1': 0.4682031366523982, 'optimizer': 'SGD', 'lstm_units_2': 62, 'dropout_2': 0.28950189911500135, 'lstm_units_3': 113, 'dropout_3': 0.18340401691942518}. Best is trial 23 with value: 0.6133928537368775.\n",
      "[I 2024-04-24 01:29:35,372] Trial 39 finished with value: 0.5883928656578064 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 108, 'dropout_1': 0.25801906870619196, 'optimizer': 'Adam', 'lstm_units_2': 52, 'dropout_2': 0.21842111193629823, 'lstm_units_3': 97, 'dropout_3': 0.29473963993592556}. Best is trial 23 with value: 0.6133928537368775.\n",
      "[I 2024-04-24 01:29:59,094] Trial 40 finished with value: 0.45 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 115, 'dropout_1': 0.4337762767371959, 'optimizer': 'SGD', 'lstm_units_2': 45, 'dropout_2': 0.14904633195980835, 'lstm_units_3': 55, 'dropout_3': 0.08976874911118746}. Best is trial 23 with value: 0.6133928537368775.\n",
      "[I 2024-04-24 01:30:24,407] Trial 41 finished with value: 0.5741071343421936 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 116, 'dropout_1': 0.47911363367478754, 'optimizer': 'Adam', 'lstm_units_2': 39, 'dropout_2': 0.08665832341677415, 'lstm_units_3': 79, 'dropout_3': 0.009515643105005411}. Best is trial 23 with value: 0.6133928537368775.\n",
      "[I 2024-04-24 01:30:49,232] Trial 42 finished with value: 0.5883928418159485 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 123, 'dropout_1': 0.44032086024844486, 'optimizer': 'Adam', 'lstm_units_2': 37, 'dropout_2': 0.02637249991892675, 'lstm_units_3': 90, 'dropout_3': 0.043539062040111876}. Best is trial 23 with value: 0.6133928537368775.\n",
      "[I 2024-04-24 01:31:13,674] Trial 43 finished with value: 0.5955357313156128 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 111, 'dropout_1': 0.38937951517908226, 'optimizer': 'Adam', 'lstm_units_2': 48, 'dropout_2': 0.04622435221674237, 'lstm_units_3': 71, 'dropout_3': 0.11911462888270885}. Best is trial 23 with value: 0.6133928537368775.\n",
      "[I 2024-04-24 01:31:38,379] Trial 44 finished with value: 0.573214304447174 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 101, 'dropout_1': 0.4984159515815123, 'optimizer': 'Adam', 'lstm_units_2': 53, 'dropout_2': 0.08918202116213546, 'lstm_units_3': 108, 'dropout_3': 0.05123809109336214}. Best is trial 23 with value: 0.6133928537368775.\n",
      "[I 2024-04-24 01:32:03,303] Trial 45 finished with value: 0.6044642925262451 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 119, 'dropout_1': 0.40814946262180024, 'optimizer': 'Adam', 'lstm_units_2': 42, 'dropout_2': 0.020931296493440225, 'lstm_units_3': 92, 'dropout_3': 0.005116673025521686}. Best is trial 23 with value: 0.6133928537368775.\n",
      "[I 2024-04-24 01:32:28,552] Trial 46 finished with value: 0.5660714268684387 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 119, 'dropout_1': 0.37142398875783805, 'optimizer': 'Adam', 'lstm_units_2': 73, 'dropout_2': 0.1905148309363145, 'lstm_units_3': 95, 'dropout_3': 0.07807631472226706}. Best is trial 23 with value: 0.6133928537368775.\n",
      "[I 2024-04-24 01:32:45,806] Trial 47 finished with value: 0.5348214209079742 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 90, 'dropout_1': 0.40737410455652, 'optimizer': 'SGD', 'lstm_units_2': 83, 'dropout_2': 0.14234850425407003}. Best is trial 23 with value: 0.6133928537368775.\n",
      "[I 2024-04-24 01:33:09,944] Trial 48 finished with value: 0.5901785731315613 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 80, 'dropout_1': 0.28712364461671913, 'optimizer': 'Adam', 'lstm_units_2': 58, 'dropout_2': 0.0569684968904435, 'lstm_units_3': 117, 'dropout_3': 0.396083927923303}. Best is trial 23 with value: 0.6133928537368775.\n",
      "[I 2024-04-24 01:33:28,025] Trial 49 finished with value: 0.5696428537368774 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 107, 'dropout_1': 0.47674521398978376, 'optimizer': 'Adam', 'lstm_units_2': 36, 'dropout_2': 0.10104095426195432}. Best is trial 23 with value: 0.6133928537368775.\n",
      "[I 2024-04-24 01:33:40,899] Trial 50 finished with value: 0.5526785731315613 and parameters: {'num_lstm_layers': 1, 'lstm_units_1': 126, 'dropout_1': 0.43383381703579893, 'optimizer': 'Adam'}. Best is trial 23 with value: 0.6133928537368775.\n",
      "[I 2024-04-24 01:34:05,846] Trial 51 finished with value: 0.5839285731315613 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 115, 'dropout_1': 0.43555942671398346, 'optimizer': 'Adam', 'lstm_units_2': 42, 'dropout_2': 0.019269702390171655, 'lstm_units_3': 84, 'dropout_3': 0.02104227726118884}. Best is trial 23 with value: 0.6133928537368775.\n",
      "[I 2024-04-24 01:34:31,132] Trial 52 finished with value: 0.5767857193946838 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 121, 'dropout_1': 0.4724496765195764, 'optimizer': 'Adam', 'lstm_units_2': 43, 'dropout_2': 0.0012653816019818492, 'lstm_units_3': 105, 'dropout_3': 0.0017888521438128024}. Best is trial 23 with value: 0.6133928537368775.\n",
      "[I 2024-04-24 01:34:57,082] Trial 53 finished with value: 0.6044642925262451 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 117, 'dropout_1': 0.4060808569121611, 'optimizer': 'Adam', 'lstm_units_2': 47, 'dropout_2': 0.06241032101056754, 'lstm_units_3': 76, 'dropout_3': 0.04017544821557102}. Best is trial 23 with value: 0.6133928537368775.\n",
      "[I 2024-04-24 01:35:23,095] Trial 54 finished with value: 0.5901785731315613 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 118, 'dropout_1': 0.0271047638012602, 'optimizer': 'Adam', 'lstm_units_2': 47, 'dropout_2': 0.06754851164702433, 'lstm_units_3': 74, 'dropout_3': 0.06559428895269122}. Best is trial 23 with value: 0.6133928537368775.\n",
      "[I 2024-04-24 01:35:46,332] Trial 55 finished with value: 0.4848214328289032 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 128, 'dropout_1': 0.18989997145915677, 'optimizer': 'SGD', 'lstm_units_2': 54, 'dropout_2': 0.12126345389912951, 'lstm_units_3': 64, 'dropout_3': 0.24115300774088766}. Best is trial 23 with value: 0.6133928537368775.\n",
      "[I 2024-04-24 01:36:09,338] Trial 56 finished with value: 0.5910714387893676 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 110, 'dropout_1': 0.336202392888182, 'optimizer': 'Adam', 'lstm_units_2': 64, 'dropout_2': 0.046629431985995104, 'lstm_units_3': 40, 'dropout_3': 0.14216177705608973}. Best is trial 23 with value: 0.6133928537368775.\n",
      "[I 2024-04-24 01:36:27,336] Trial 57 finished with value: 0.5955357193946839 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 106, 'dropout_1': 0.40953408976635214, 'optimizer': 'Adam', 'lstm_units_2': 34, 'dropout_2': 0.03310066414140321}. Best is trial 23 with value: 0.6133928537368775.\n",
      "[I 2024-04-24 01:36:54,168] Trial 58 finished with value: 0.5767857074737549 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 124, 'dropout_1': 0.377620605778904, 'optimizer': 'RMSprop', 'lstm_units_2': 46, 'dropout_2': 0.364378242958685, 'lstm_units_3': 91, 'dropout_3': 0.03852120884991116}. Best is trial 23 with value: 0.6133928537368775.\n",
      "[I 2024-04-24 01:37:17,204] Trial 59 finished with value: 0.6008928656578064 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 98, 'dropout_1': 0.07876518086920342, 'optimizer': 'Adam', 'lstm_units_2': 32, 'dropout_2': 0.2457668520034597, 'lstm_units_3': 99, 'dropout_3': 0.1848980575676284}. Best is trial 23 with value: 0.6133928537368775.\n",
      "[I 2024-04-24 01:37:35,137] Trial 60 finished with value: 0.5839285731315613 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 117, 'dropout_1': 0.3518669950467802, 'optimizer': 'Adam', 'lstm_units_2': 120, 'dropout_2': 0.17101079525092652}. Best is trial 23 with value: 0.6133928537368775.\n",
      "[I 2024-04-24 01:37:58,700] Trial 61 finished with value: 0.5482142865657806 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 113, 'dropout_1': 0.44254247041030825, 'optimizer': 'Adam', 'lstm_units_2': 40, 'dropout_2': 0.019582989197768734, 'lstm_units_3': 80, 'dropout_3': 0.0347173988684934}. Best is trial 23 with value: 0.6133928537368775.\n",
      "[I 2024-04-24 01:38:22,996] Trial 62 finished with value: 0.6125 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 121, 'dropout_1': 0.4115630437531847, 'optimizer': 'Adam', 'lstm_units_2': 50, 'dropout_2': 0.06950678094405106, 'lstm_units_3': 78, 'dropout_3': 0.02672868343580123}. Best is trial 23 with value: 0.6133928537368775.\n",
      "[I 2024-04-24 01:38:46,981] Trial 63 finished with value: 0.5946428537368774 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 121, 'dropout_1': 0.40415380173016374, 'optimizer': 'Adam', 'lstm_units_2': 50, 'dropout_2': 0.10365204581223755, 'lstm_units_3': 73, 'dropout_3': 0.06909563452470593}. Best is trial 23 with value: 0.6133928537368775.\n",
      "[I 2024-04-24 01:39:10,417] Trial 64 finished with value: 0.56875 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 126, 'dropout_1': 0.425911899580632, 'optimizer': 'Adam', 'lstm_units_2': 56, 'dropout_2': 0.0780666731021503, 'lstm_units_3': 77, 'dropout_3': 0.02865593211550998}. Best is trial 23 with value: 0.6133928537368775.\n",
      "[I 2024-04-24 01:39:33,113] Trial 65 finished with value: 0.575 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 69, 'dropout_1': 0.38220137350226435, 'optimizer': 'Adam', 'lstm_units_2': 45, 'dropout_2': 0.054967599451941414, 'lstm_units_3': 87, 'dropout_3': 0.0892841663097915}. Best is trial 23 with value: 0.6133928537368775.\n",
      "[I 2024-04-24 01:39:57,123] Trial 66 finished with value: 0.60625 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 110, 'dropout_1': 0.4146480248468632, 'optimizer': 'Adam', 'lstm_units_2': 37, 'dropout_2': 0.09866299788909456, 'lstm_units_3': 128, 'dropout_3': 0.2859864130227916}. Best is trial 23 with value: 0.6133928537368775.\n",
      "[I 2024-04-24 01:40:24,129] Trial 67 finished with value: 0.5875000119209289 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 111, 'dropout_1': 0.4629592226990312, 'optimizer': 'RMSprop', 'lstm_units_2': 36, 'dropout_2': 0.12449833318836274, 'lstm_units_3': 120, 'dropout_3': 0.29934514826165504}. Best is trial 23 with value: 0.6133928537368775.\n",
      "[I 2024-04-24 01:40:49,183] Trial 68 finished with value: 0.5625 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 120, 'dropout_1': 0.48183990624176026, 'optimizer': 'Adam', 'lstm_units_2': 38, 'dropout_2': 0.1387297476346024, 'lstm_units_3': 127, 'dropout_3': 0.26290573100051956}. Best is trial 23 with value: 0.6133928537368775.\n",
      "[I 2024-04-24 01:41:14,695] Trial 69 finished with value: 0.5839285850524902 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 102, 'dropout_1': 0.36832693448187237, 'optimizer': 'Adam', 'lstm_units_2': 42, 'dropout_2': 0.4287827861630628, 'lstm_units_3': 102, 'dropout_3': 0.3449051056424157}. Best is trial 23 with value: 0.6133928537368775.\n",
      "[I 2024-04-24 01:41:33,996] Trial 70 finished with value: 0.5839285731315613 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 109, 'dropout_1': 0.42084466999922765, 'optimizer': 'Adam', 'lstm_units_2': 52, 'dropout_2': 0.09729605720424411}. Best is trial 23 with value: 0.6133928537368775.\n",
      "[I 2024-04-24 01:41:59,524] Trial 71 finished with value: 0.59375 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 118, 'dropout_1': 0.39731070044396194, 'optimizer': 'Adam', 'lstm_units_2': 49, 'dropout_2': 0.07097172677719485, 'lstm_units_3': 61, 'dropout_3': 0.02716299502459166}. Best is trial 23 with value: 0.6133928537368775.\n",
      "[I 2024-04-24 01:42:24,647] Trial 72 finished with value: 0.5723214268684387 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 115, 'dropout_1': 0.4502860331633701, 'optimizer': 'Adam', 'lstm_units_2': 44, 'dropout_2': 0.16284398908070302, 'lstm_units_3': 68, 'dropout_3': 0.2179899273355721}. Best is trial 23 with value: 0.6133928537368775.\n",
      "[I 2024-04-24 01:42:50,049] Trial 73 finished with value: 0.5875 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 124, 'dropout_1': 0.4139861358386181, 'optimizer': 'Adam', 'lstm_units_2': 58, 'dropout_2': 0.10809392858964473, 'lstm_units_3': 76, 'dropout_3': 0.27157800198198845}. Best is trial 23 with value: 0.6133928537368775.\n",
      "[I 2024-04-24 01:43:14,985] Trial 74 finished with value: 0.59375 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 106, 'dropout_1': 0.230782383221986, 'optimizer': 'Adam', 'lstm_units_2': 35, 'dropout_2': 0.03824569351492943, 'lstm_units_3': 92, 'dropout_3': 0.33668897720563506}. Best is trial 23 with value: 0.6133928537368775.\n",
      "[I 2024-04-24 01:43:39,108] Trial 75 finished with value: 0.4660714268684387 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 113, 'dropout_1': 0.45899913359108774, 'optimizer': 'SGD', 'lstm_units_2': 41, 'dropout_2': 0.0643178529002836, 'lstm_units_3': 86, 'dropout_3': 0.059231724835837246}. Best is trial 23 with value: 0.6133928537368775.\n",
      "[I 2024-04-24 01:44:04,615] Trial 76 finished with value: 0.5696428656578064 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 128, 'dropout_1': 0.34417378370718493, 'optimizer': 'Adam', 'lstm_units_2': 46, 'dropout_2': 0.02097086518044511, 'lstm_units_3': 113, 'dropout_3': 0.4344494776598376}. Best is trial 23 with value: 0.6133928537368775.\n",
      "[I 2024-04-24 01:44:28,298] Trial 77 finished with value: 0.5589285612106323 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 49, 'dropout_1': 0.38783820336497726, 'optimizer': 'Adam', 'lstm_units_2': 51, 'dropout_2': 0.4972218217421299, 'lstm_units_3': 82, 'dropout_3': 0.012445041053222215}. Best is trial 23 with value: 0.6133928537368775.\n",
      "[I 2024-04-24 01:44:45,791] Trial 78 finished with value: 0.49821428656578065 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 34, 'dropout_1': 0.42854788045200787, 'optimizer': 'Adam', 'lstm_units_2': 61, 'dropout_2': 0.30083578024765617}. Best is trial 23 with value: 0.6133928537368775.\n",
      "[I 2024-04-24 01:45:13,904] Trial 79 finished with value: 0.5991071343421936 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 121, 'dropout_1': 0.3201740393789713, 'optimizer': 'RMSprop', 'lstm_units_2': 55, 'dropout_2': 0.08506719379427644, 'lstm_units_3': 70, 'dropout_3': 0.00035076911487706697}. Best is trial 23 with value: 0.6133928537368775.\n",
      "[I 2024-04-24 01:45:37,718] Trial 80 finished with value: 0.5491071522235871 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 60, 'dropout_1': 0.444786791906178, 'optimizer': 'Adam', 'lstm_units_2': 38, 'dropout_2': 0.19850176858132076, 'lstm_units_3': 121, 'dropout_3': 0.11887349186648234}. Best is trial 23 with value: 0.6133928537368775.\n",
      "[I 2024-04-24 01:46:02,904] Trial 81 finished with value: 0.5919642806053161 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 116, 'dropout_1': 0.4827728614561174, 'optimizer': 'Adam', 'lstm_units_2': 43, 'dropout_2': 0.011698664114613636, 'lstm_units_3': 82, 'dropout_3': 0.023207110490372555}. Best is trial 23 with value: 0.6133928537368775.\n",
      "[I 2024-04-24 01:46:27,091] Trial 82 finished with value: 0.5821428418159484 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 112, 'dropout_1': 0.4035059365049477, 'optimizer': 'Adam', 'lstm_units_2': 41, 'dropout_2': 0.045423482538725135, 'lstm_units_3': 77, 'dropout_3': 0.05060113647432292}. Best is trial 23 with value: 0.6133928537368775.\n",
      "[I 2024-04-24 01:46:51,157] Trial 83 finished with value: 0.5312500119209289 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 40, 'dropout_1': 0.44582282042488075, 'optimizer': 'Adam', 'lstm_units_2': 48, 'dropout_2': 0.012129327684203984, 'lstm_units_3': 94, 'dropout_3': 0.020462336443341417}. Best is trial 23 with value: 0.6133928537368775.\n",
      "[I 2024-04-24 01:47:15,806] Trial 84 finished with value: 0.587499988079071 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 118, 'dropout_1': 0.42055874278158345, 'optimizer': 'Adam', 'lstm_units_2': 34, 'dropout_2': 0.02934825683815794, 'lstm_units_3': 88, 'dropout_3': 0.09617997243551177}. Best is trial 23 with value: 0.6133928537368775.\n",
      "[I 2024-04-24 01:47:40,928] Trial 85 finished with value: 0.5883928537368774 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 123, 'dropout_1': 0.4678329493568877, 'optimizer': 'Adam', 'lstm_units_2': 39, 'dropout_2': 0.001169140447431393, 'lstm_units_3': 84, 'dropout_3': 0.047147856751200895}. Best is trial 23 with value: 0.6133928537368775.\n",
      "[I 2024-04-24 01:48:05,941] Trial 86 finished with value: 0.5839285731315613 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 114, 'dropout_1': 0.4373234323133056, 'optimizer': 'Adam', 'lstm_units_2': 32, 'dropout_2': 0.05116613024723314, 'lstm_units_3': 109, 'dropout_3': 0.0004525076331016002}. Best is trial 23 with value: 0.6133928537368775.\n",
      "[I 2024-04-24 01:48:19,328] Trial 87 finished with value: 0.532142847776413 and parameters: {'num_lstm_layers': 1, 'lstm_units_1': 125, 'dropout_1': 0.4931663048527007, 'optimizer': 'Adam'}. Best is trial 23 with value: 0.6133928537368775.\n",
      "[I 2024-04-24 01:48:36,961] Trial 88 finished with value: 0.5580357134342193 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 109, 'dropout_1': 0.4127840420970579, 'optimizer': 'SGD', 'lstm_units_2': 68, 'dropout_2': 0.11416489204597391}. Best is trial 23 with value: 0.6133928537368775.\n",
      "[I 2024-04-24 01:49:02,385] Trial 89 finished with value: 0.5785714268684388 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 121, 'dropout_1': 0.36012403586168634, 'optimizer': 'Adam', 'lstm_units_2': 45, 'dropout_2': 0.03549805184215282, 'lstm_units_3': 81, 'dropout_3': 0.07936412744682145}. Best is trial 23 with value: 0.6133928537368775.\n",
      "[I 2024-04-24 01:49:26,292] Trial 90 finished with value: 0.5803571462631225 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 104, 'dropout_1': 0.4622391670411657, 'optimizer': 'Adam', 'lstm_units_2': 48, 'dropout_2': 0.09240307672373685, 'lstm_units_3': 64, 'dropout_3': 0.2828928594439829}. Best is trial 23 with value: 0.6133928537368775.\n",
      "[I 2024-04-24 01:49:46,904] Trial 91 finished with value: 0.5803571581840515 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 116, 'dropout_1': 0.45191983183284434, 'optimizer': 'RMSprop', 'lstm_units_2': 37, 'dropout_2': 0.33478522818138307}. Best is trial 23 with value: 0.6133928537368775.\n",
      "[I 2024-04-24 01:50:07,653] Trial 92 finished with value: 0.5839285612106323 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 93, 'dropout_1': 0.1390000828551205, 'optimizer': 'RMSprop', 'lstm_units_2': 41, 'dropout_2': 0.3837697525835311}. Best is trial 23 with value: 0.6133928537368775.\n",
      "[I 2024-04-24 01:50:28,772] Trial 93 finished with value: 0.5955357193946839 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 119, 'dropout_1': 0.4306450142319551, 'optimizer': 'RMSprop', 'lstm_units_2': 98, 'dropout_2': 0.4319513649997176}. Best is trial 23 with value: 0.6133928537368775.\n",
      "[I 2024-04-24 01:50:48,638] Trial 94 finished with value: 0.5946428656578064 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 83, 'dropout_1': 0.38977672598473806, 'optimizer': 'RMSprop', 'lstm_units_2': 44, 'dropout_2': 0.3848604402285326}. Best is trial 23 with value: 0.6133928537368775.\n",
      "[I 2024-04-24 01:51:16,010] Trial 95 finished with value: 0.5785714268684388 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 78, 'dropout_1': 0.47029577410580076, 'optimizer': 'RMSprop', 'lstm_units_2': 53, 'dropout_2': 0.13393415623961566, 'lstm_units_3': 71, 'dropout_3': 0.17428036256834684}. Best is trial 23 with value: 0.6133928537368775.\n",
      "[I 2024-04-24 01:51:34,818] Trial 96 finished with value: 0.5883928656578064 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 111, 'dropout_1': 0.3966215779578644, 'optimizer': 'Adam', 'lstm_units_2': 40, 'dropout_2': 0.2704744270751003}. Best is trial 23 with value: 0.6133928537368775.\n",
      "[I 2024-04-24 01:51:59,970] Trial 97 finished with value: 0.5678571462631226 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 100, 'dropout_1': 0.48565128014513775, 'optimizer': 'Adam', 'lstm_units_2': 35, 'dropout_2': 0.3286444258805327, 'lstm_units_3': 99, 'dropout_3': 0.01567096284379638}. Best is trial 23 with value: 0.6133928537368775.\n",
      "[I 2024-04-24 01:52:24,451] Trial 98 finished with value: 0.5571428418159485 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 74, 'dropout_1': 0.4408471100902204, 'optimizer': 'Adam', 'lstm_units_2': 76, 'dropout_2': 0.07428034988485015, 'lstm_units_3': 74, 'dropout_3': 0.037886348323157924}. Best is trial 23 with value: 0.6133928537368775.\n",
      "[I 2024-04-24 01:52:49,061] Trial 99 finished with value: 0.5883928537368774 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 126, 'dropout_1': 0.4990293029170727, 'optimizer': 'Adam', 'lstm_units_2': 50, 'dropout_2': 0.061983585770901774, 'lstm_units_3': 93, 'dropout_3': 0.3192716239842296}. Best is trial 23 with value: 0.6133928537368775.\n",
      "[I 2024-04-24 01:53:07,908] Trial 100 finished with value: 0.5955357074737548 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 117, 'dropout_1': 0.42729679992768643, 'optimizer': 'Adam', 'lstm_units_2': 47, 'dropout_2': 0.22148201441121748}. Best is trial 23 with value: 0.6133928537368775.\n",
      "[I 2024-04-24 01:53:31,460] Trial 101 finished with value: 0.5919642925262452 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 97, 'dropout_1': 0.050507312811505195, 'optimizer': 'Adam', 'lstm_units_2': 32, 'dropout_2': 0.2467409577412449, 'lstm_units_3': 106, 'dropout_3': 0.19616036448664823}. Best is trial 23 with value: 0.6133928537368775.\n",
      "[I 2024-04-24 01:53:56,513] Trial 102 finished with value: 0.5589285731315613 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 106, 'dropout_1': 0.09600883897904983, 'optimizer': 'Adam', 'lstm_units_2': 37, 'dropout_2': 0.243872399439844, 'lstm_units_3': 102, 'dropout_3': 0.23280176413071157}. Best is trial 23 with value: 0.6133928537368775.\n",
      "[I 2024-04-24 01:54:21,486] Trial 103 finished with value: 0.5973214268684387 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 113, 'dropout_1': 0.16736492515505827, 'optimizer': 'Adam', 'lstm_units_2': 43, 'dropout_2': 0.02073509093775828, 'lstm_units_3': 96, 'dropout_3': 0.1620073165025307}. Best is trial 23 with value: 0.6133928537368775.\n",
      "[I 2024-04-24 01:54:45,811] Trial 104 finished with value: 0.5758928656578064 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 122, 'dropout_1': 0.4131907681781029, 'optimizer': 'Adam', 'lstm_units_2': 39, 'dropout_2': 0.2876560491180462, 'lstm_units_3': 88, 'dropout_3': 0.06133021463297333}. Best is trial 23 with value: 0.6133928537368775.\n",
      "[I 2024-04-24 01:55:10,367] Trial 105 finished with value: 0.5794642925262451 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 89, 'dropout_1': 0.267826124353421, 'optimizer': 'Adam', 'lstm_units_2': 35, 'dropout_2': 0.3691274264051168, 'lstm_units_3': 99, 'dropout_3': 0.1213522469661003}. Best is trial 23 with value: 0.6133928537368775.\n",
      "[I 2024-04-24 01:55:34,390] Trial 106 finished with value: 0.45267857909202575 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 119, 'dropout_1': 0.45106953098959, 'optimizer': 'SGD', 'lstm_units_2': 33, 'dropout_2': 0.04397063393729653, 'lstm_units_3': 112, 'dropout_3': 0.017430818605146937}. Best is trial 23 with value: 0.6133928537368775.\n",
      "[I 2024-04-24 01:55:58,044] Trial 107 finished with value: 0.5616071462631226 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 69, 'dropout_1': 0.38016879478223986, 'optimizer': 'Adam', 'lstm_units_2': 46, 'dropout_2': 0.34153878011378763, 'lstm_units_3': 85, 'dropout_3': 0.25251472453938273}. Best is trial 23 with value: 0.6133928537368775.\n",
      "[I 2024-04-24 01:56:22,935] Trial 108 finished with value: 0.5830357074737549 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 108, 'dropout_1': 0.4724270786074881, 'optimizer': 'Adam', 'lstm_units_2': 42, 'dropout_2': 0.31555532031896916, 'lstm_units_3': 89, 'dropout_3': 0.20374214479201858}. Best is trial 23 with value: 0.6133928537368775.\n",
      "[I 2024-04-24 01:56:50,599] Trial 109 finished with value: 0.6089285731315612 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 94, 'dropout_1': 0.1182139348838982, 'optimizer': 'RMSprop', 'lstm_units_2': 37, 'dropout_2': 0.45448317522424936, 'lstm_units_3': 78, 'dropout_3': 0.03525640465347861}. Best is trial 23 with value: 0.6133928537368775.\n",
      "[I 2024-04-24 01:57:10,535] Trial 110 finished with value: 0.5964285612106324 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 92, 'dropout_1': 0.40762114339376715, 'optimizer': 'RMSprop', 'lstm_units_2': 37, 'dropout_2': 0.48494389554062145}. Best is trial 23 with value: 0.6133928537368775.\n",
      "[I 2024-04-24 01:57:37,691] Trial 111 finished with value: 0.5848214387893677 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 84, 'dropout_1': 0.4602913839084003, 'optimizer': 'RMSprop', 'lstm_units_2': 39, 'dropout_2': 0.4108632671170401, 'lstm_units_3': 78, 'dropout_3': 0.03905229240761226}. Best is trial 23 with value: 0.6133928537368775.\n",
      "[I 2024-04-24 01:58:05,030] Trial 112 finished with value: 0.6258928656578064 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 95, 'dropout_1': 0.09942568774280543, 'optimizer': 'RMSprop', 'lstm_units_2': 43, 'dropout_2': 0.4472100856295779, 'lstm_units_3': 80, 'dropout_3': 0.031246258214857263}. Best is trial 112 with value: 0.6258928656578064.\n",
      "[I 2024-04-24 01:58:32,666] Trial 113 finished with value: 0.5883928656578064 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 95, 'dropout_1': 0.07685473484003258, 'optimizer': 'RMSprop', 'lstm_units_2': 85, 'dropout_2': 0.4579413887785593, 'lstm_units_3': 80, 'dropout_3': 0.030085917189245304}. Best is trial 112 with value: 0.6258928656578064.\n",
      "[I 2024-04-24 01:59:03,764] Trial 114 finished with value: 0.5741071343421936 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 87, 'dropout_1': 0.1072299923510542, 'optimizer': 'RMSprop', 'lstm_units_2': 45, 'dropout_2': 0.44495502331286285, 'lstm_units_3': 76, 'dropout_3': 0.0744394558388391}. Best is trial 112 with value: 0.6258928656578064.\n",
      "[I 2024-04-24 01:59:32,050] Trial 115 finished with value: 0.6125 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 115, 'dropout_1': 0.11843751583205386, 'optimizer': 'RMSprop', 'lstm_units_2': 43, 'dropout_2': 0.44935656402346996, 'lstm_units_3': 70, 'dropout_3': 0.05378765073679098}. Best is trial 112 with value: 0.6258928656578064.\n",
      "[I 2024-04-24 02:00:00,362] Trial 116 finished with value: 0.6026785612106323 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 115, 'dropout_1': 0.09849890604646848, 'optimizer': 'RMSprop', 'lstm_units_2': 49, 'dropout_2': 0.4400560593299683, 'lstm_units_3': 73, 'dropout_3': 0.05795180355680786}. Best is trial 112 with value: 0.6258928656578064.\n",
      "[I 2024-04-24 02:00:27,429] Trial 117 finished with value: 0.5803571462631225 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 112, 'dropout_1': 0.11907857967375571, 'optimizer': 'RMSprop', 'lstm_units_2': 43, 'dropout_2': 0.4616554773661519, 'lstm_units_3': 70, 'dropout_3': 0.017873882236767384}. Best is trial 112 with value: 0.6258928656578064.\n",
      "[I 2024-04-24 02:00:55,045] Trial 118 finished with value: 0.6053571462631225 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 120, 'dropout_1': 0.13146082453602048, 'optimizer': 'RMSprop', 'lstm_units_2': 52, 'dropout_2': 0.4716916215152199, 'lstm_units_3': 67, 'dropout_3': 0.04799883869560654}. Best is trial 112 with value: 0.6258928656578064.\n",
      "[I 2024-04-24 02:01:23,501] Trial 119 finished with value: 0.6116071462631225 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 120, 'dropout_1': 0.13158484288821368, 'optimizer': 'RMSprop', 'lstm_units_2': 52, 'dropout_2': 0.46890748863462894, 'lstm_units_3': 67, 'dropout_3': 0.04913566801018856}. Best is trial 112 with value: 0.6258928656578064.\n",
      "[I 2024-04-24 02:01:51,660] Trial 120 finished with value: 0.6169642686843873 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 124, 'dropout_1': 0.14918089209514343, 'optimizer': 'RMSprop', 'lstm_units_2': 54, 'dropout_2': 0.47594346855389624, 'lstm_units_3': 62, 'dropout_3': 0.08389439261918905}. Best is trial 112 with value: 0.6258928656578064.\n",
      "[I 2024-04-24 02:02:19,743] Trial 121 finished with value: 0.6196428537368774 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 124, 'dropout_1': 0.1372474640335675, 'optimizer': 'RMSprop', 'lstm_units_2': 54, 'dropout_2': 0.47244790492882094, 'lstm_units_3': 58, 'dropout_3': 0.050905852675318136}. Best is trial 112 with value: 0.6258928656578064.\n",
      "[I 2024-04-24 02:02:47,378] Trial 122 finished with value: 0.5964285731315613 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 123, 'dropout_1': 0.15718122819340946, 'optimizer': 'RMSprop', 'lstm_units_2': 59, 'dropout_2': 0.47108783764109874, 'lstm_units_3': 57, 'dropout_3': 0.08073749313791297}. Best is trial 112 with value: 0.6258928656578064.\n",
      "[I 2024-04-24 02:03:15,024] Trial 123 finished with value: 0.6071428656578064 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 126, 'dropout_1': 0.12200570287148726, 'optimizer': 'RMSprop', 'lstm_units_2': 56, 'dropout_2': 0.47762800672270245, 'lstm_units_3': 63, 'dropout_3': 0.04665117726614905}. Best is trial 112 with value: 0.6258928656578064.\n",
      "[I 2024-04-24 02:03:43,676] Trial 124 finished with value: 0.5973214149475098 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 127, 'dropout_1': 0.12646358487747184, 'optimizer': 'RMSprop', 'lstm_units_2': 55, 'dropout_2': 0.4745933175223276, 'lstm_units_3': 52, 'dropout_3': 0.05295002156753334}. Best is trial 112 with value: 0.6258928656578064.\n",
      "[I 2024-04-24 02:04:11,597] Trial 125 finished with value: 0.6241071343421936 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 125, 'dropout_1': 0.14528922428988789, 'optimizer': 'RMSprop', 'lstm_units_2': 52, 'dropout_2': 0.488164955035415, 'lstm_units_3': 60, 'dropout_3': 0.06988404461130851}. Best is trial 112 with value: 0.6258928656578064.\n",
      "[I 2024-04-24 02:04:38,882] Trial 126 finished with value: 0.5946428537368774 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 125, 'dropout_1': 0.14536304020819302, 'optimizer': 'RMSprop', 'lstm_units_2': 64, 'dropout_2': 0.48675001574175514, 'lstm_units_3': 60, 'dropout_3': 0.06576999510026196}. Best is trial 112 with value: 0.6258928656578064.\n",
      "[I 2024-04-24 02:05:08,665] Trial 127 finished with value: 0.6071428418159485 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 124, 'dropout_1': 0.11193539790623253, 'optimizer': 'RMSprop', 'lstm_units_2': 51, 'dropout_2': 0.4535233673092985, 'lstm_units_3': 63, 'dropout_3': 0.08970560492172464}. Best is trial 112 with value: 0.6258928656578064.\n",
      "[I 2024-04-24 02:05:48,878] Trial 128 finished with value: 0.6107142806053162 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 128, 'dropout_1': 0.17906802331757163, 'optimizer': 'RMSprop', 'lstm_units_2': 57, 'dropout_2': 0.4526181084954846, 'lstm_units_3': 61, 'dropout_3': 0.10385236623443511}. Best is trial 112 with value: 0.6258928656578064.\n",
      "[I 2024-04-24 02:06:17,642] Trial 129 finished with value: 0.6098214149475097 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 128, 'dropout_1': 0.17821059352476792, 'optimizer': 'RMSprop', 'lstm_units_2': 54, 'dropout_2': 0.4542226927035152, 'lstm_units_3': 61, 'dropout_3': 0.10859190603732038}. Best is trial 112 with value: 0.6258928656578064.\n",
      "[I 2024-04-24 02:06:45,528] Trial 130 finished with value: 0.5982142925262451 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 128, 'dropout_1': 0.15266437777633413, 'optimizer': 'RMSprop', 'lstm_units_2': 58, 'dropout_2': 0.41973294532158417, 'lstm_units_3': 59, 'dropout_3': 0.11583792667546944}. Best is trial 112 with value: 0.6258928656578064.\n",
      "[I 2024-04-24 02:07:13,610] Trial 131 finished with value: 0.6232142925262452 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 126, 'dropout_1': 0.17715231913482132, 'optimizer': 'RMSprop', 'lstm_units_2': 56, 'dropout_2': 0.4484111541508694, 'lstm_units_3': 63, 'dropout_3': 0.10353944904130002}. Best is trial 112 with value: 0.6258928656578064.\n",
      "[I 2024-04-24 02:07:41,767] Trial 132 finished with value: 0.5991071462631226 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 126, 'dropout_1': 0.18969097445636462, 'optimizer': 'RMSprop', 'lstm_units_2': 55, 'dropout_2': 0.4888041833068181, 'lstm_units_3': 55, 'dropout_3': 0.12917761172134107}. Best is trial 112 with value: 0.6258928656578064.\n",
      "[I 2024-04-24 02:08:09,687] Trial 133 finished with value: 0.58125 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 128, 'dropout_1': 0.17823393051204942, 'optimizer': 'RMSprop', 'lstm_units_2': 57, 'dropout_2': 0.4464881968552241, 'lstm_units_3': 62, 'dropout_3': 0.10682364128047155}. Best is trial 112 with value: 0.6258928656578064.\n",
      "[I 2024-04-24 02:08:36,820] Trial 134 finished with value: 0.6035714268684387 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 123, 'dropout_1': 0.20893806022466907, 'optimizer': 'RMSprop', 'lstm_units_2': 54, 'dropout_2': 0.466980037939262, 'lstm_units_3': 65, 'dropout_3': 0.10459820989000955}. Best is trial 112 with value: 0.6258928656578064.\n",
      "[I 2024-04-24 02:09:04,592] Trial 135 finished with value: 0.5875 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 125, 'dropout_1': 0.16933787277529266, 'optimizer': 'RMSprop', 'lstm_units_2': 60, 'dropout_2': 0.47890866376388636, 'lstm_units_3': 57, 'dropout_3': 0.08351233176861181}. Best is trial 112 with value: 0.6258928656578064.\n",
      "[I 2024-04-24 02:09:33,664] Trial 136 finished with value: 0.6026785731315613 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 122, 'dropout_1': 0.08274358066120452, 'optimizer': 'RMSprop', 'lstm_units_2': 63, 'dropout_2': 0.43446167247056955, 'lstm_units_3': 52, 'dropout_3': 0.07009967998364752}. Best is trial 112 with value: 0.6258928656578064.\n",
      "[I 2024-04-24 02:10:03,710] Trial 137 finished with value: 0.6178571462631226 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 126, 'dropout_1': 0.12097212384973698, 'optimizer': 'RMSprop', 'lstm_units_2': 56, 'dropout_2': 0.4513507930616052, 'lstm_units_3': 66, 'dropout_3': 0.03288289584530226}. Best is trial 112 with value: 0.6258928656578064.\n",
      "[I 2024-04-24 02:10:31,371] Trial 138 finished with value: 0.6116071462631225 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 121, 'dropout_1': 0.14511558758605736, 'optimizer': 'RMSprop', 'lstm_units_2': 53, 'dropout_2': 0.41754169249790585, 'lstm_units_3': 69, 'dropout_3': 0.09736312031351196}. Best is trial 112 with value: 0.6258928656578064.\n",
      "[I 2024-04-24 02:11:03,845] Trial 139 finished with value: 0.6169642925262451 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 121, 'dropout_1': 0.13933326148117367, 'optimizer': 'RMSprop', 'lstm_units_2': 53, 'dropout_2': 0.4140140167217115, 'lstm_units_3': 68, 'dropout_3': 0.09696788837078114}. Best is trial 112 with value: 0.6258928656578064.\n",
      "[I 2024-04-24 02:11:30,966] Trial 140 finished with value: 0.5919642686843872 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 121, 'dropout_1': 0.13906159996842268, 'optimizer': 'RMSprop', 'lstm_units_2': 53, 'dropout_2': 0.42311521474557234, 'lstm_units_3': 67, 'dropout_3': 0.0958435136264725}. Best is trial 112 with value: 0.6258928656578064.\n",
      "[I 2024-04-24 02:11:58,879] Trial 141 finished with value: 0.5866071462631226 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 124, 'dropout_1': 0.15419583274866064, 'optimizer': 'RMSprop', 'lstm_units_2': 50, 'dropout_2': 0.4056387694313261, 'lstm_units_3': 68, 'dropout_3': 0.10054319471888737}. Best is trial 112 with value: 0.6258928656578064.\n",
      "[I 2024-04-24 02:12:25,524] Trial 142 finished with value: 0.6178571462631226 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 128, 'dropout_1': 0.18935687782242402, 'optimizer': 'RMSprop', 'lstm_units_2': 53, 'dropout_2': 0.3958519697339016, 'lstm_units_3': 47, 'dropout_3': 0.13833738612037247}. Best is trial 112 with value: 0.6258928656578064.\n",
      "[I 2024-04-24 02:12:52,153] Trial 143 finished with value: 0.5910714387893676 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 120, 'dropout_1': 0.1937940366638026, 'optimizer': 'RMSprop', 'lstm_units_2': 52, 'dropout_2': 0.38734655327327766, 'lstm_units_3': 49, 'dropout_3': 0.07316449701745467}. Best is trial 112 with value: 0.6258928656578064.\n",
      "[I 2024-04-24 02:13:19,626] Trial 144 finished with value: 0.6116071462631225 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 122, 'dropout_1': 0.14170928763512605, 'optimizer': 'RMSprop', 'lstm_units_2': 57, 'dropout_2': 0.44272438999453056, 'lstm_units_3': 46, 'dropout_3': 0.06067869911464059}. Best is trial 112 with value: 0.6258928656578064.\n",
      "[I 2024-04-24 02:13:46,158] Trial 145 finished with value: 0.618749988079071 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 122, 'dropout_1': 0.14464388391764973, 'optimizer': 'RMSprop', 'lstm_units_2': 59, 'dropout_2': 0.43757120101841773, 'lstm_units_3': 44, 'dropout_3': 0.055040155798527605}. Best is trial 112 with value: 0.6258928656578064.\n",
      "[I 2024-04-24 02:14:16,057] Trial 146 finished with value: 0.625 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 118, 'dropout_1': 0.16613606448320162, 'optimizer': 'RMSprop', 'lstm_units_2': 60, 'dropout_2': 0.3973157629490146, 'lstm_units_3': 41, 'dropout_3': 0.1447017707135173}. Best is trial 112 with value: 0.6258928656578064.\n",
      "[I 2024-04-24 02:15:15,796] Trial 147 finished with value: 0.5946428537368774 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 118, 'dropout_1': 0.16691015639541312, 'optimizer': 'RMSprop', 'lstm_units_2': 61, 'dropout_2': 0.39557401413776244, 'lstm_units_3': 39, 'dropout_3': 0.028475498034806958}. Best is trial 112 with value: 0.6258928656578064.\n",
      "[I 2024-04-24 02:16:20,635] Trial 148 finished with value: 0.6276785731315613 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 124, 'dropout_1': 0.10792333328469408, 'optimizer': 'RMSprop', 'lstm_units_2': 65, 'dropout_2': 0.49489384404853165, 'lstm_units_3': 40, 'dropout_3': 0.05397184830120454}. Best is trial 148 with value: 0.6276785731315613.\n",
      "[I 2024-04-24 02:17:21,862] Trial 149 finished with value: 0.5991071462631226 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 125, 'dropout_1': 0.10323071856585915, 'optimizer': 'RMSprop', 'lstm_units_2': 65, 'dropout_2': 0.499445012848685, 'lstm_units_3': 38, 'dropout_3': 0.14562874950072946}. Best is trial 148 with value: 0.6276785731315613.\n",
      "[I 2024-04-24 02:18:26,204] Trial 150 finished with value: 0.6107142925262451 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 124, 'dropout_1': 0.08532550379338319, 'optimizer': 'RMSprop', 'lstm_units_2': 66, 'dropout_2': 0.4063688024303577, 'lstm_units_3': 43, 'dropout_3': 0.08646040045507097}. Best is trial 148 with value: 0.6276785731315613.\n",
      "[I 2024-04-24 02:19:26,953] Trial 151 finished with value: 0.6258928537368774 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 117, 'dropout_1': 0.13071488482179372, 'optimizer': 'RMSprop', 'lstm_units_2': 60, 'dropout_2': 0.42700276529805026, 'lstm_units_3': 33, 'dropout_3': 0.1341566604369494}. Best is trial 148 with value: 0.6276785731315613.\n",
      "[I 2024-04-24 02:20:26,110] Trial 152 finished with value: 0.6169642925262451 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 117, 'dropout_1': 0.15803746066646712, 'optimizer': 'RMSprop', 'lstm_units_2': 68, 'dropout_2': 0.42786462813534415, 'lstm_units_3': 32, 'dropout_3': 0.13127996715470808}. Best is trial 148 with value: 0.6276785731315613.\n",
      "[I 2024-04-24 02:21:29,467] Trial 153 finished with value: 0.5946428656578064 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 117, 'dropout_1': 0.15892918598101882, 'optimizer': 'RMSprop', 'lstm_units_2': 70, 'dropout_2': 0.43086020639429157, 'lstm_units_3': 33, 'dropout_3': 0.13138027531190005}. Best is trial 148 with value: 0.6276785731315613.\n",
      "[I 2024-04-24 02:22:32,415] Trial 154 finished with value: 0.587499988079071 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 116, 'dropout_1': 0.1131202719079689, 'optimizer': 'RMSprop', 'lstm_units_2': 68, 'dropout_2': 0.435228663069173, 'lstm_units_3': 37, 'dropout_3': 0.13757926503868226}. Best is trial 148 with value: 0.6276785731315613.\n",
      "[I 2024-04-24 02:23:35,193] Trial 155 finished with value: 0.5999999880790711 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 119, 'dropout_1': 0.06964097171193637, 'optimizer': 'RMSprop', 'lstm_units_2': 60, 'dropout_2': 0.4251159829023211, 'lstm_units_3': 36, 'dropout_3': 0.15436218897400136}. Best is trial 148 with value: 0.6276785731315613.\n",
      "[I 2024-04-24 02:24:39,155] Trial 156 finished with value: 0.6214285850524902 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 123, 'dropout_1': 0.12687045747430994, 'optimizer': 'RMSprop', 'lstm_units_2': 63, 'dropout_2': 0.4898692920114849, 'lstm_units_3': 42, 'dropout_3': 0.16808670058121034}. Best is trial 148 with value: 0.6276785731315613.\n",
      "[I 2024-04-24 02:25:42,411] Trial 157 finished with value: 0.6133928537368775 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 123, 'dropout_1': 0.13435811965184627, 'optimizer': 'RMSprop', 'lstm_units_2': 59, 'dropout_2': 0.4918498361175219, 'lstm_units_3': 41, 'dropout_3': 0.1436770728061864}. Best is trial 148 with value: 0.6276785731315613.\n",
      "[I 2024-04-24 02:26:45,649] Trial 158 finished with value: 0.6035714268684387 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 126, 'dropout_1': 0.1306138763086392, 'optimizer': 'RMSprop', 'lstm_units_2': 61, 'dropout_2': 0.49327737593752125, 'lstm_units_3': 42, 'dropout_3': 0.14636498432092535}. Best is trial 148 with value: 0.6276785731315613.\n",
      "[I 2024-04-24 02:27:50,075] Trial 159 finished with value: 0.59375 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 123, 'dropout_1': 0.15022429257229228, 'optimizer': 'RMSprop', 'lstm_units_2': 63, 'dropout_2': 0.48250575239425864, 'lstm_units_3': 35, 'dropout_3': 0.16692427445813612}. Best is trial 148 with value: 0.6276785731315613.\n",
      "[I 2024-04-24 02:28:51,178] Trial 160 finished with value: 0.612499988079071 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 126, 'dropout_1': 0.16237018953882965, 'optimizer': 'RMSprop', 'lstm_units_2': 59, 'dropout_2': 0.4612688181025283, 'lstm_units_3': 42, 'dropout_3': 0.1298023945953097}. Best is trial 148 with value: 0.6276785731315613.\n",
      "[I 2024-04-24 02:29:53,927] Trial 161 finished with value: 0.5883928537368774 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 122, 'dropout_1': 0.13470139656451255, 'optimizer': 'RMSprop', 'lstm_units_2': 62, 'dropout_2': 0.4899106859814918, 'lstm_units_3': 44, 'dropout_3': 0.12075197373601768}. Best is trial 148 with value: 0.6276785731315613.\n",
      "[I 2024-04-24 02:31:00,225] Trial 162 finished with value: 0.6107143044471741 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 120, 'dropout_1': 0.20042341699310157, 'optimizer': 'RMSprop', 'lstm_units_2': 58, 'dropout_2': 0.46284282355017053, 'lstm_units_3': 47, 'dropout_3': 0.1752061820717751}. Best is trial 148 with value: 0.6276785731315613.\n",
      "[I 2024-04-24 02:32:02,949] Trial 163 finished with value: 0.5910714268684387 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 124, 'dropout_1': 0.2421319416132416, 'optimizer': 'RMSprop', 'lstm_units_2': 66, 'dropout_2': 0.47850454433492395, 'lstm_units_3': 41, 'dropout_3': 0.13952807344930973}. Best is trial 148 with value: 0.6276785731315613.\n",
      "[I 2024-04-24 02:33:06,216] Trial 164 finished with value: 0.6142857313156128 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 122, 'dropout_1': 0.12591146371696518, 'optimizer': 'RMSprop', 'lstm_units_2': 56, 'dropout_2': 0.49952964301740066, 'lstm_units_3': 45, 'dropout_3': 0.14929338390106678}. Best is trial 148 with value: 0.6276785731315613.\n",
      "[I 2024-04-24 02:34:05,968] Trial 165 finished with value: 0.5937499880790711 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 123, 'dropout_1': 0.10573319666560307, 'optimizer': 'RMSprop', 'lstm_units_2': 72, 'dropout_2': 0.4989172788101011, 'lstm_units_3': 32, 'dropout_3': 0.15213842273708064}. Best is trial 148 with value: 0.6276785731315613.\n",
      "[I 2024-04-24 02:35:06,952] Trial 166 finished with value: 0.6196428418159485 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 126, 'dropout_1': 0.1270288023490127, 'optimizer': 'RMSprop', 'lstm_units_2': 56, 'dropout_2': 0.4859398826098559, 'lstm_units_3': 45, 'dropout_3': 0.11528792854720266}. Best is trial 148 with value: 0.6276785731315613.\n",
      "[I 2024-04-24 02:36:12,526] Trial 167 finished with value: 0.5910714268684387 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 126, 'dropout_1': 0.09324627144054723, 'optimizer': 'RMSprop', 'lstm_units_2': 56, 'dropout_2': 0.4131634324289678, 'lstm_units_3': 45, 'dropout_3': 0.1143413391850293}. Best is trial 148 with value: 0.6276785731315613.\n",
      "[I 2024-04-24 02:37:14,381] Trial 168 finished with value: 0.5892857313156128 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 118, 'dropout_1': 0.1244107048826238, 'optimizer': 'RMSprop', 'lstm_units_2': 75, 'dropout_2': 0.4667918531802815, 'lstm_units_3': 48, 'dropout_3': 0.1265430532869343}. Best is trial 148 with value: 0.6276785731315613.\n",
      "[I 2024-04-24 02:38:17,252] Trial 169 finished with value: 0.6116071462631225 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 128, 'dropout_1': 0.1468929486007301, 'optimizer': 'RMSprop', 'lstm_units_2': 55, 'dropout_2': 0.44076421097011254, 'lstm_units_3': 35, 'dropout_3': 0.16448397624605415}. Best is trial 148 with value: 0.6276785731315613.\n",
      "[I 2024-04-24 02:39:20,771] Trial 170 finished with value: 0.5991071343421936 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 126, 'dropout_1': 0.11181217385271885, 'optimizer': 'RMSprop', 'lstm_units_2': 63, 'dropout_2': 0.48595264806424465, 'lstm_units_3': 49, 'dropout_3': 0.11344881243679383}. Best is trial 148 with value: 0.6276785731315613.\n",
      "[I 2024-04-24 02:40:27,074] Trial 171 finished with value: 0.5973214268684387 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 123, 'dropout_1': 0.13779150281635247, 'optimizer': 'RMSprop', 'lstm_units_2': 59, 'dropout_2': 0.49194304023005847, 'lstm_units_3': 39, 'dropout_3': 0.13896163516779927}. Best is trial 148 with value: 0.6276785731315613.\n",
      "[I 2024-04-24 02:41:30,063] Trial 172 finished with value: 0.6089285731315612 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 121, 'dropout_1': 0.12408656138517443, 'optimizer': 'RMSprop', 'lstm_units_2': 60, 'dropout_2': 0.4767333235817823, 'lstm_units_3': 40, 'dropout_3': 0.15674989961759633}. Best is trial 148 with value: 0.6276785731315613.\n",
      "[I 2024-04-24 02:42:32,320] Trial 173 finished with value: 0.59375 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 124, 'dropout_1': 0.17685653091767603, 'optimizer': 'RMSprop', 'lstm_units_2': 68, 'dropout_2': 0.4823450649637095, 'lstm_units_3': 44, 'dropout_3': 0.1441650949813449}. Best is trial 148 with value: 0.6276785731315613.\n",
      "[I 2024-04-24 02:43:18,036] Trial 174 finished with value: 0.5607142806053161 and parameters: {'num_lstm_layers': 1, 'lstm_units_1': 118, 'dropout_1': 0.15364670253454785, 'optimizer': 'RMSprop'}. Best is trial 148 with value: 0.6276785731315613.\n",
      "[I 2024-04-24 02:44:23,451] Trial 175 finished with value: 0.5973214268684387 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 125, 'dropout_1': 0.13222753487488442, 'optimizer': 'RMSprop', 'lstm_units_2': 57, 'dropout_2': 0.37353044002827374, 'lstm_units_3': 54, 'dropout_3': 0.12817883477066966}. Best is trial 148 with value: 0.6276785731315613.\n",
      "[I 2024-04-24 02:45:27,862] Trial 176 finished with value: 0.5883928418159485 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 127, 'dropout_1': 0.17070160228691783, 'optimizer': 'RMSprop', 'lstm_units_2': 55, 'dropout_2': 0.4015494341064218, 'lstm_units_3': 46, 'dropout_3': 0.173225735583306}. Best is trial 148 with value: 0.6276785731315613.\n",
      "[I 2024-04-24 02:46:31,904] Trial 177 finished with value: 0.5991071581840515 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 120, 'dropout_1': 0.14557203359887955, 'optimizer': 'RMSprop', 'lstm_units_2': 65, 'dropout_2': 0.46594813734128904, 'lstm_units_3': 37, 'dropout_3': 0.09257050748545462}. Best is trial 148 with value: 0.6276785731315613.\n",
      "[I 2024-04-24 02:47:37,008] Trial 178 finished with value: 0.581249988079071 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 122, 'dropout_1': 0.11667819295555065, 'optimizer': 'RMSprop', 'lstm_units_2': 62, 'dropout_2': 0.4984083320050517, 'lstm_units_3': 34, 'dropout_3': 0.07088662945544043}. Best is trial 148 with value: 0.6276785731315613.\n",
      "[I 2024-04-24 02:48:40,356] Trial 179 finished with value: 0.5857142925262451 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 124, 'dropout_1': 0.10075520856259135, 'optimizer': 'RMSprop', 'lstm_units_2': 59, 'dropout_2': 0.4499832697210393, 'lstm_units_3': 65, 'dropout_3': 0.18603734823749993}. Best is trial 148 with value: 0.6276785731315613.\n",
      "[I 2024-04-24 02:49:45,142] Trial 180 finished with value: 0.5857142806053162 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 119, 'dropout_1': 0.16028319162094, 'optimizer': 'RMSprop', 'lstm_units_2': 54, 'dropout_2': 0.4311550945797915, 'lstm_units_3': 58, 'dropout_3': 0.038785314468115166}. Best is trial 148 with value: 0.6276785731315613.\n",
      "[I 2024-04-24 02:50:48,766] Trial 181 finished with value: 0.6107142806053162 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 121, 'dropout_1': 0.13689078024830192, 'optimizer': 'RMSprop', 'lstm_units_2': 49, 'dropout_2': 0.4721323173382785, 'lstm_units_3': 42, 'dropout_3': 0.0088488488020135}. Best is trial 148 with value: 0.6276785731315613.\n",
      "[I 2024-04-24 02:51:54,140] Trial 182 finished with value: 0.6017857313156127 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 128, 'dropout_1': 0.18452236470996805, 'optimizer': 'RMSprop', 'lstm_units_2': 51, 'dropout_2': 0.4193707570670743, 'lstm_units_3': 72, 'dropout_3': 0.027030596160356014}. Best is trial 148 with value: 0.6276785731315613.\n",
      "[I 2024-04-24 02:52:58,995] Trial 183 finished with value: 0.5991071343421936 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 117, 'dropout_1': 0.12574637127378663, 'optimizer': 'RMSprop', 'lstm_units_2': 57, 'dropout_2': 0.4877297981074909, 'lstm_units_3': 40, 'dropout_3': 0.04253140599055775}. Best is trial 148 with value: 0.6276785731315613.\n",
      "[I 2024-04-24 02:54:00,635] Trial 184 finished with value: 0.6133928656578064 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 122, 'dropout_1': 0.14828347039592166, 'optimizer': 'RMSprop', 'lstm_units_2': 54, 'dropout_2': 0.4566801472224531, 'lstm_units_3': 32, 'dropout_3': 0.05982652469539303}. Best is trial 148 with value: 0.6276785731315613.\n",
      "[I 2024-04-24 02:55:04,878] Trial 185 finished with value: 0.6133928537368775 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 125, 'dropout_1': 0.14651579641494822, 'optimizer': 'RMSprop', 'lstm_units_2': 56, 'dropout_2': 0.4593408823293653, 'lstm_units_3': 33, 'dropout_3': 0.0772862508291734}. Best is trial 148 with value: 0.6276785731315613.\n",
      "[I 2024-04-24 02:56:18,183] Trial 186 finished with value: 0.6125 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 122, 'dropout_1': 0.1630944656498973, 'optimizer': 'RMSprop', 'lstm_units_2': 53, 'dropout_2': 0.47684543060016144, 'lstm_units_3': 32, 'dropout_3': 0.05430680946741623}. Best is trial 148 with value: 0.6276785731315613.\n",
      "[I 2024-04-24 02:57:22,283] Trial 187 finished with value: 0.4848214328289032 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 126, 'dropout_1': 0.2941636542300067, 'optimizer': 'SGD', 'lstm_units_2': 61, 'dropout_2': 0.4411211727518015, 'lstm_units_3': 50, 'dropout_3': 0.12147741001080396}. Best is trial 148 with value: 0.6276785731315613.\n",
      "[I 2024-04-24 02:58:26,908] Trial 188 finished with value: 0.5866071462631226 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 124, 'dropout_1': 0.13950605016664938, 'optimizer': 'RMSprop', 'lstm_units_2': 58, 'dropout_2': 0.46091644332123705, 'lstm_units_3': 44, 'dropout_3': 0.06141479810961957}. Best is trial 148 with value: 0.6276785731315613.\n",
      "[I 2024-04-24 02:59:30,718] Trial 189 finished with value: 0.6107142925262451 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 115, 'dropout_1': 0.12724267609439632, 'optimizer': 'RMSprop', 'lstm_units_2': 55, 'dropout_2': 0.49869557932558173, 'lstm_units_3': 38, 'dropout_3': 0.08405299877658541}. Best is trial 148 with value: 0.6276785731315613.\n",
      "[I 2024-04-24 03:00:31,770] Trial 190 finished with value: 0.5741071462631225 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 120, 'dropout_1': 0.15495349826296584, 'optimizer': 'RMSprop', 'lstm_units_2': 64, 'dropout_2': 0.47166160540881613, 'lstm_units_3': 35, 'dropout_3': 0.14957222648019558}. Best is trial 148 with value: 0.6276785731315613.\n",
      "[I 2024-04-24 03:01:35,618] Trial 191 finished with value: 0.6008928656578064 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 125, 'dropout_1': 0.14764223348274644, 'optimizer': 'RMSprop', 'lstm_units_2': 56, 'dropout_2': 0.45119070851641613, 'lstm_units_3': 32, 'dropout_3': 0.07324176942453073}. Best is trial 148 with value: 0.6276785731315613.\n",
      "[I 2024-04-24 03:02:44,149] Trial 192 finished with value: 0.6017857074737549 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 123, 'dropout_1': 0.10982086996302268, 'optimizer': 'RMSprop', 'lstm_units_2': 59, 'dropout_2': 0.4590680230097988, 'lstm_units_3': 34, 'dropout_3': 0.4909381505184965}. Best is trial 148 with value: 0.6276785731315613.\n",
      "[I 2024-04-24 03:03:51,656] Trial 193 finished with value: 0.600000011920929 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 127, 'dropout_1': 0.14171783182047806, 'optimizer': 'RMSprop', 'lstm_units_2': 53, 'dropout_2': 0.42666399159608, 'lstm_units_3': 46, 'dropout_3': 0.07980138106700863}. Best is trial 148 with value: 0.6276785731315613.\n",
      "[I 2024-04-24 03:04:51,788] Trial 194 finished with value: 0.575 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 122, 'dropout_1': 0.13135551885800725, 'optimizer': 'RMSprop', 'lstm_units_2': 57, 'dropout_2': 0.48586502640517115, 'lstm_units_3': 41, 'dropout_3': 0.11104126208468558}. Best is trial 148 with value: 0.6276785731315613.\n",
      "[I 2024-04-24 03:05:55,630] Trial 195 finished with value: 0.6044642925262451 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 125, 'dropout_1': 0.11939304668358686, 'optimizer': 'RMSprop', 'lstm_units_2': 55, 'dropout_2': 0.44042322962475344, 'lstm_units_3': 36, 'dropout_3': 0.06529995399601723}. Best is trial 148 with value: 0.6276785731315613.\n",
      "[I 2024-04-24 03:07:00,314] Trial 196 finished with value: 0.6080357193946838 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 119, 'dropout_1': 0.1698476630041028, 'optimizer': 'RMSprop', 'lstm_units_2': 115, 'dropout_2': 0.46577836692663827, 'lstm_units_3': 66, 'dropout_3': 0.13485467751793243}. Best is trial 148 with value: 0.6276785731315613.\n",
      "[I 2024-04-24 03:08:06,010] Trial 197 finished with value: 0.6098214268684388 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 128, 'dropout_1': 0.14900509965815348, 'optimizer': 'RMSprop', 'lstm_units_2': 61, 'dropout_2': 0.44986722164982945, 'lstm_units_3': 64, 'dropout_3': 0.08711979913062776}. Best is trial 148 with value: 0.6276785731315613.\n",
      "[I 2024-04-24 03:09:10,674] Trial 198 finished with value: 0.6017857074737549 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 122, 'dropout_1': 0.0900851822384506, 'optimizer': 'RMSprop', 'lstm_units_2': 51, 'dropout_2': 0.4124385076021146, 'lstm_units_3': 39, 'dropout_3': 0.10055277510840976}. Best is trial 148 with value: 0.6276785731315613.\n",
      "[I 2024-04-24 03:10:12,498] Trial 199 finished with value: 0.5910714149475098 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 126, 'dropout_1': 0.15984604209519035, 'optimizer': 'RMSprop', 'lstm_units_2': 59, 'dropout_2': 0.4781421744692441, 'lstm_units_3': 43, 'dropout_3': 0.03530441976158639}. Best is trial 148 with value: 0.6276785731315613.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'num_lstm_layers': 3, 'lstm_units_1': 124, 'dropout_1': 0.10792333328469408, 'optimizer': 'RMSprop', 'lstm_units_2': 65, 'dropout_2': 0.49489384404853165, 'lstm_units_3': 40, 'dropout_3': 0.05397184830120454}\n"
     ]
    }
   ],
   "source": [
    "from joblib import load, dump\n",
    "import numpy as np\n",
    "import tensorflow as tf \n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from tensorflow.keras.optimizers import SGD, Adam, RMSprop\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "import optuna\n",
    "\n",
    "\n",
    "combined_data = load('../../DataDumps/3classfft.joblib')\n",
    "combined_labels = load('../../DataDumps/3class_labels.joblib')\n",
    "\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(combined_data, combined_labels, test_size=0.2, random_state=42)\n",
    "input_sh = X_train_val.shape\n",
    "\n",
    "def create_model(optimizer, num_lstm, lstm_1, dropout_1, lstm_2 = 0, drop_2 = 0, lstm_3 = 0, drop_3 = 0):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units=lstm_1, input_shape=(input_sh[1], input_sh[2]), return_sequences=True if num_lstm > 1 else False)) \n",
    "    model.add(Dropout(dropout_1))\n",
    "    \n",
    "\n",
    "    if num_lstm > 1:\n",
    "        model.add(LSTM(units=lstm_2, input_shape=(input_sh[1], input_sh[2]), return_sequences=True if num_lstm == 3 else False)) \n",
    "        model.add(Dropout(drop_2))\n",
    "    \n",
    "    if num_lstm > 2:\n",
    "        model.add(LSTM(units=lstm_3, input_shape=(input_sh[1], input_sh[2]), return_sequences= False)) \n",
    "        model.add(Dropout(drop_3))\n",
    "    \n",
    "    model.add(Dense(3, activation='softmax')) \n",
    "\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])   \n",
    "    return model\n",
    "\n",
    "\n",
    "# Optuna Objective Function\n",
    "def objective(trial):\n",
    "    lstm_2 = 0\n",
    "    drop_2 = 0\n",
    "    lstm_3 = 0\n",
    "    drop_3 = 0\n",
    "    num_lstm = trial.suggest_int('num_lstm_layers', 1, 3)\n",
    "    lstm_1 = trial.suggest_int('lstm_units_1', 32, 128)\n",
    "    dropout_1 = trial.suggest_float('dropout_1', 0.0, 0.5)\n",
    "    optimizer = trial.suggest_categorical('optimizer', ['SGD', 'RMSprop', 'Adam'])\n",
    "\n",
    "    if num_lstm > 1:\n",
    "        lstm_2 = trial.suggest_int(f'lstm_units_2', 32, 128)\n",
    "        drop_2 = trial.suggest_float(f'dropout_2', 0.0, 0.5)\n",
    "    if num_lstm > 2:\n",
    "        lstm_3 = trial.suggest_int(f'lstm_units_3', 32, 128)\n",
    "        drop_3 = trial.suggest_float(f'dropout_3', 0.0, 0.5)\n",
    "\n",
    "\n",
    "    # Assuming StratifiedKFold, customize if needed\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)  \n",
    "    scores = []\n",
    "    for train_index, test_index in skf.split(X_train_val, y_train_val):\n",
    "        X_tr, X_val = X_train_val[train_index], X_train_val[test_index]\n",
    "        y_tr, y_val = y_train_val[train_index], y_train_val[test_index]\n",
    "\n",
    "        model = create_model(optimizer, num_lstm, lstm_1, dropout_1, lstm_2, drop_2, lstm_3, drop_3)\n",
    "        model.fit(X_tr, y_tr,epochs = 20, verbose = 0) \n",
    "        score = model.evaluate(X_val, y_val, verbose = 0)\n",
    "        scores.append(score[1])\n",
    "    return np.array(scores).mean()\n",
    "\n",
    "# Hyperparameter Optimization\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=200)\n",
    "\n",
    "best_params = study.best_params\n",
    "print(\"Best hyperparameters:\", best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/400\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 1.0586 - accuracy: 0.4539\n",
      "Epoch 1: val_accuracy improved from -inf to 0.51786, saving model to best_model_weights.h5\n",
      "21/21 [==============================] - 3s 45ms/step - loss: 1.0549 - accuracy: 0.4583 - val_loss: 1.0059 - val_accuracy: 0.5179\n",
      "Epoch 2/400\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 0.9612 - accuracy: 0.5437\n",
      "Epoch 2: val_accuracy improved from 0.51786 to 0.55580, saving model to best_model_weights.h5\n",
      "21/21 [==============================] - 0s 14ms/step - loss: 0.9603 - accuracy: 0.5461 - val_loss: 0.9061 - val_accuracy: 0.5558\n",
      "Epoch 3/400\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.8835 - accuracy: 0.5595\n",
      "Epoch 3: val_accuracy improved from 0.55580 to 0.58036, saving model to best_model_weights.h5\n",
      "21/21 [==============================] - 0s 14ms/step - loss: 0.8835 - accuracy: 0.5595 - val_loss: 0.8388 - val_accuracy: 0.5804\n",
      "Epoch 4/400\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.8468 - accuracy: 0.6101\n",
      "Epoch 4: val_accuracy improved from 0.58036 to 0.59375, saving model to best_model_weights.h5\n",
      "21/21 [==============================] - 0s 14ms/step - loss: 0.8468 - accuracy: 0.6101 - val_loss: 0.8245 - val_accuracy: 0.5938\n",
      "Epoch 5/400\n",
      "15/21 [====================>.........] - ETA: 0s - loss: 0.8184 - accuracy: 0.6000\n",
      "Epoch 5: val_accuracy improved from 0.59375 to 0.61161, saving model to best_model_weights.h5\n",
      "21/21 [==============================] - 0s 14ms/step - loss: 0.8220 - accuracy: 0.6012 - val_loss: 0.7826 - val_accuracy: 0.6116\n",
      "Epoch 6/400\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.8035 - accuracy: 0.6295\n",
      "Epoch 6: val_accuracy improved from 0.61161 to 0.62277, saving model to best_model_weights.h5\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.8035 - accuracy: 0.6295 - val_loss: 0.7914 - val_accuracy: 0.6228\n",
      "Epoch 7/400\n",
      "15/21 [====================>.........] - ETA: 0s - loss: 0.7997 - accuracy: 0.6000\n",
      "Epoch 7: val_accuracy did not improve from 0.62277\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.7977 - accuracy: 0.6042 - val_loss: 0.8141 - val_accuracy: 0.6049\n",
      "Epoch 8/400\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 0.8071 - accuracy: 0.6187\n",
      "Epoch 8: val_accuracy did not improve from 0.62277\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.8047 - accuracy: 0.6205 - val_loss: 0.7685 - val_accuracy: 0.6071\n",
      "Epoch 9/400\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.7709 - accuracy: 0.6118\n",
      "Epoch 9: val_accuracy improved from 0.62277 to 0.63170, saving model to best_model_weights.h5\n",
      "21/21 [==============================] - 0s 15ms/step - loss: 0.7741 - accuracy: 0.6161 - val_loss: 0.7683 - val_accuracy: 0.6317\n",
      "Epoch 10/400\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.7702 - accuracy: 0.6384\n",
      "Epoch 10: val_accuracy did not improve from 0.63170\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.7702 - accuracy: 0.6384 - val_loss: 0.7781 - val_accuracy: 0.6183\n",
      "Epoch 11/400\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.7633 - accuracy: 0.6235\n",
      "Epoch 11: val_accuracy did not improve from 0.63170\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.7633 - accuracy: 0.6235 - val_loss: 0.7706 - val_accuracy: 0.6295\n",
      "Epoch 12/400\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 0.7497 - accuracy: 0.6281\n",
      "Epoch 12: val_accuracy did not improve from 0.63170\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.7451 - accuracy: 0.6310 - val_loss: 0.7776 - val_accuracy: 0.6205\n",
      "Epoch 13/400\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.7482 - accuracy: 0.6354\n",
      "Epoch 13: val_accuracy did not improve from 0.63170\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.7482 - accuracy: 0.6354 - val_loss: 0.7767 - val_accuracy: 0.6250\n",
      "Epoch 14/400\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.7261 - accuracy: 0.6628\n",
      "Epoch 14: val_accuracy did not improve from 0.63170\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.7253 - accuracy: 0.6592 - val_loss: 0.7659 - val_accuracy: 0.6317\n",
      "Epoch 15/400\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.7341 - accuracy: 0.6429\n",
      "Epoch 15: val_accuracy did not improve from 0.63170\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.7341 - accuracy: 0.6429 - val_loss: 0.7842 - val_accuracy: 0.6295\n",
      "Epoch 16/400\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.7436 - accuracy: 0.6518\n",
      "Epoch 16: val_accuracy did not improve from 0.63170\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.7436 - accuracy: 0.6518 - val_loss: 0.7677 - val_accuracy: 0.6317\n",
      "Epoch 17/400\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 0.7123 - accuracy: 0.6812\n",
      "Epoch 17: val_accuracy improved from 0.63170 to 0.63616, saving model to best_model_weights.h5\n",
      "21/21 [==============================] - 0s 14ms/step - loss: 0.7146 - accuracy: 0.6756 - val_loss: 0.7664 - val_accuracy: 0.6362\n",
      "Epoch 18/400\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.6984 - accuracy: 0.6743\n",
      "Epoch 18: val_accuracy improved from 0.63616 to 0.64062, saving model to best_model_weights.h5\n",
      "21/21 [==============================] - 0s 14ms/step - loss: 0.7038 - accuracy: 0.6741 - val_loss: 0.7756 - val_accuracy: 0.6406\n",
      "Epoch 19/400\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.7081 - accuracy: 0.6727\n",
      "Epoch 19: val_accuracy did not improve from 0.64062\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.7005 - accuracy: 0.6771 - val_loss: 0.8021 - val_accuracy: 0.6205\n",
      "Epoch 20/400\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 0.6999 - accuracy: 0.6641\n",
      "Epoch 20: val_accuracy did not improve from 0.64062\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.6939 - accuracy: 0.6667 - val_loss: 0.8299 - val_accuracy: 0.6027\n",
      "Epoch 21/400\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.6843 - accuracy: 0.6801\n",
      "Epoch 21: val_accuracy did not improve from 0.64062\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.6843 - accuracy: 0.6801 - val_loss: 0.7999 - val_accuracy: 0.6339\n",
      "Epoch 22/400\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.6768 - accuracy: 0.6949\n",
      "Epoch 22: val_accuracy did not improve from 0.64062\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.6768 - accuracy: 0.6949 - val_loss: 0.8241 - val_accuracy: 0.6295\n",
      "Epoch 23/400\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 0.6826 - accuracy: 0.6828\n",
      "Epoch 23: val_accuracy did not improve from 0.64062\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.6967 - accuracy: 0.6696 - val_loss: 0.8499 - val_accuracy: 0.6027\n",
      "Epoch 24/400\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.6714 - accuracy: 0.6860\n",
      "Epoch 24: val_accuracy improved from 0.64062 to 0.64732, saving model to best_model_weights.h5\n",
      "21/21 [==============================] - 0s 14ms/step - loss: 0.6714 - accuracy: 0.6860 - val_loss: 0.7997 - val_accuracy: 0.6473\n",
      "Epoch 25/400\n",
      "15/21 [====================>.........] - ETA: 0s - loss: 0.6676 - accuracy: 0.6792\n",
      "Epoch 25: val_accuracy did not improve from 0.64732\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.6830 - accuracy: 0.6667 - val_loss: 0.8158 - val_accuracy: 0.6429\n",
      "Epoch 26/400\n",
      "15/21 [====================>.........] - ETA: 0s - loss: 0.6733 - accuracy: 0.6854\n",
      "Epoch 26: val_accuracy did not improve from 0.64732\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.6738 - accuracy: 0.6875 - val_loss: 0.8141 - val_accuracy: 0.6473\n",
      "Epoch 27/400\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.6646 - accuracy: 0.6935\n",
      "Epoch 27: val_accuracy did not improve from 0.64732\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.6646 - accuracy: 0.6935 - val_loss: 0.8208 - val_accuracy: 0.6429\n",
      "Epoch 28/400\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 0.6722 - accuracy: 0.6766\n",
      "Epoch 28: val_accuracy did not improve from 0.64732\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.6697 - accuracy: 0.6771 - val_loss: 0.8119 - val_accuracy: 0.6384\n",
      "Epoch 29/400\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.6812 - accuracy: 0.6776\n",
      "Epoch 29: val_accuracy did not improve from 0.64732\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.6738 - accuracy: 0.6830 - val_loss: 0.8312 - val_accuracy: 0.6295\n",
      "Epoch 30/400\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.6514 - accuracy: 0.7128\n",
      "Epoch 30: val_accuracy did not improve from 0.64732\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.6514 - accuracy: 0.7128 - val_loss: 0.8393 - val_accuracy: 0.6317\n",
      "Epoch 31/400\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.6411 - accuracy: 0.7098\n",
      "Epoch 31: val_accuracy did not improve from 0.64732\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.6411 - accuracy: 0.7098 - val_loss: 0.8680 - val_accuracy: 0.6362\n",
      "Epoch 32/400\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 0.6425 - accuracy: 0.7188\n",
      "Epoch 32: val_accuracy did not improve from 0.64732\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.6414 - accuracy: 0.7173 - val_loss: 0.8415 - val_accuracy: 0.6384\n",
      "Epoch 33/400\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 0.6345 - accuracy: 0.6875\n",
      "Epoch 33: val_accuracy did not improve from 0.64732\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.6359 - accuracy: 0.6860 - val_loss: 0.8368 - val_accuracy: 0.6317\n",
      "Epoch 34/400\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.6401 - accuracy: 0.6994\n",
      "Epoch 34: val_accuracy did not improve from 0.64732\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.6401 - accuracy: 0.6994 - val_loss: 0.8387 - val_accuracy: 0.6272\n",
      "Epoch 35/400\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.6401 - accuracy: 0.6979\n",
      "Epoch 35: val_accuracy did not improve from 0.64732\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.6401 - accuracy: 0.6979 - val_loss: 0.8350 - val_accuracy: 0.6451\n",
      "Epoch 36/400\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.6220 - accuracy: 0.7138\n",
      "Epoch 36: val_accuracy did not improve from 0.64732\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.6310 - accuracy: 0.7143 - val_loss: 0.8430 - val_accuracy: 0.6384\n",
      "Epoch 37/400\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.6060 - accuracy: 0.7188\n",
      "Epoch 37: val_accuracy did not improve from 0.64732\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.6060 - accuracy: 0.7188 - val_loss: 0.8495 - val_accuracy: 0.6384\n",
      "Epoch 38/400\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.6117 - accuracy: 0.7173\n",
      "Epoch 38: val_accuracy did not improve from 0.64732\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.6117 - accuracy: 0.7173 - val_loss: 0.8711 - val_accuracy: 0.6317\n",
      "Epoch 39/400\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.6182 - accuracy: 0.7098\n",
      "Epoch 39: val_accuracy improved from 0.64732 to 0.65625, saving model to best_model_weights.h5\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.6182 - accuracy: 0.7098 - val_loss: 0.8393 - val_accuracy: 0.6562\n",
      "Epoch 40/400\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.6209 - accuracy: 0.7024\n",
      "Epoch 40: val_accuracy did not improve from 0.65625\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.6209 - accuracy: 0.7024 - val_loss: 0.8874 - val_accuracy: 0.6562\n",
      "Epoch 41/400\n",
      "14/21 [===================>..........] - ETA: 0s - loss: 0.5752 - accuracy: 0.7411\n",
      "Epoch 41: val_accuracy did not improve from 0.65625\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.6050 - accuracy: 0.7158 - val_loss: 0.8517 - val_accuracy: 0.6384\n",
      "Epoch 42/400\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.6047 - accuracy: 0.7143\n",
      "Epoch 42: val_accuracy did not improve from 0.65625\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.6047 - accuracy: 0.7143 - val_loss: 0.8663 - val_accuracy: 0.6362\n",
      "Epoch 43/400\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 0.5946 - accuracy: 0.7297\n",
      "Epoch 43: val_accuracy did not improve from 0.65625\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.5911 - accuracy: 0.7321 - val_loss: 0.8803 - val_accuracy: 0.6562\n",
      "Epoch 44/400\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 0.6052 - accuracy: 0.7203\n",
      "Epoch 44: val_accuracy did not improve from 0.65625\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.6051 - accuracy: 0.7202 - val_loss: 0.8791 - val_accuracy: 0.6540\n",
      "Epoch 45/400\n",
      "15/21 [====================>.........] - ETA: 0s - loss: 0.5821 - accuracy: 0.7354\n",
      "Epoch 45: val_accuracy did not improve from 0.65625\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.5727 - accuracy: 0.7411 - val_loss: 0.9037 - val_accuracy: 0.6451\n",
      "Epoch 46/400\n",
      "15/21 [====================>.........] - ETA: 0s - loss: 0.6010 - accuracy: 0.7167\n",
      "Epoch 46: val_accuracy did not improve from 0.65625\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.5872 - accuracy: 0.7158 - val_loss: 0.9061 - val_accuracy: 0.6496\n",
      "Epoch 47/400\n",
      "15/21 [====================>.........] - ETA: 0s - loss: 0.5743 - accuracy: 0.7437\n",
      "Epoch 47: val_accuracy improved from 0.65625 to 0.66071, saving model to best_model_weights.h5\n",
      "21/21 [==============================] - 0s 14ms/step - loss: 0.5789 - accuracy: 0.7396 - val_loss: 0.8848 - val_accuracy: 0.6607\n",
      "Epoch 48/400\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 0.5852 - accuracy: 0.7188\n",
      "Epoch 48: val_accuracy did not improve from 0.66071\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.5917 - accuracy: 0.7158 - val_loss: 0.9397 - val_accuracy: 0.6518\n",
      "Epoch 49/400\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.5952 - accuracy: 0.7336\n",
      "Epoch 49: val_accuracy did not improve from 0.66071\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.5952 - accuracy: 0.7336 - val_loss: 0.8914 - val_accuracy: 0.6384\n",
      "Epoch 50/400\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 0.5786 - accuracy: 0.7141\n",
      "Epoch 50: val_accuracy did not improve from 0.66071\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.5754 - accuracy: 0.7173 - val_loss: 0.9143 - val_accuracy: 0.6518\n",
      "Epoch 51/400\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 0.5723 - accuracy: 0.7281\n",
      "Epoch 51: val_accuracy improved from 0.66071 to 0.66964, saving model to best_model_weights.h5\n",
      "21/21 [==============================] - 0s 14ms/step - loss: 0.5756 - accuracy: 0.7277 - val_loss: 0.9154 - val_accuracy: 0.6696\n",
      "Epoch 52/400\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 0.5633 - accuracy: 0.7266\n",
      "Epoch 52: val_accuracy did not improve from 0.66964\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.5631 - accuracy: 0.7277 - val_loss: 0.9098 - val_accuracy: 0.6496\n",
      "Epoch 53/400\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 0.5616 - accuracy: 0.7422\n",
      "Epoch 53: val_accuracy did not improve from 0.66964\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.5699 - accuracy: 0.7336 - val_loss: 0.9352 - val_accuracy: 0.6384\n",
      "Epoch 54/400\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.5461 - accuracy: 0.7366\n",
      "Epoch 54: val_accuracy did not improve from 0.66964\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.5461 - accuracy: 0.7366 - val_loss: 0.9357 - val_accuracy: 0.6629\n",
      "Epoch 55/400\n",
      "15/21 [====================>.........] - ETA: 0s - loss: 0.5227 - accuracy: 0.7667\n",
      "Epoch 55: val_accuracy did not improve from 0.66964\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.5668 - accuracy: 0.7485 - val_loss: 0.9363 - val_accuracy: 0.6496\n",
      "Epoch 56/400\n",
      "16/21 [=====================>........] - ETA: 0s - loss: 0.5481 - accuracy: 0.7559\n",
      "Epoch 56: val_accuracy did not improve from 0.66964\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.5592 - accuracy: 0.7455 - val_loss: 0.9120 - val_accuracy: 0.6629\n",
      "Epoch 57/400\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 0.5463 - accuracy: 0.7547\n",
      "Epoch 57: val_accuracy did not improve from 0.66964\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.5465 - accuracy: 0.7560 - val_loss: 0.9534 - val_accuracy: 0.6652\n",
      "Epoch 58/400\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 0.5450 - accuracy: 0.7375\n",
      "Epoch 58: val_accuracy did not improve from 0.66964\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.5502 - accuracy: 0.7336 - val_loss: 0.9553 - val_accuracy: 0.6585\n",
      "Epoch 59/400\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.5247 - accuracy: 0.7664\n",
      "Epoch 59: val_accuracy did not improve from 0.66964\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.5247 - accuracy: 0.7664 - val_loss: 0.9873 - val_accuracy: 0.6629\n",
      "Epoch 60/400\n",
      "15/21 [====================>.........] - ETA: 0s - loss: 0.5305 - accuracy: 0.7208\n",
      "Epoch 60: val_accuracy did not improve from 0.66964\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.5333 - accuracy: 0.7381 - val_loss: 0.9326 - val_accuracy: 0.6607\n",
      "Epoch 61/400\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.5211 - accuracy: 0.7434\n",
      "Epoch 61: val_accuracy did not improve from 0.66964\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.5254 - accuracy: 0.7440 - val_loss: 0.9928 - val_accuracy: 0.6451\n",
      "Epoch 62/400\n",
      "15/21 [====================>.........] - ETA: 0s - loss: 0.5443 - accuracy: 0.7521\n",
      "Epoch 62: val_accuracy did not improve from 0.66964\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.5430 - accuracy: 0.7426 - val_loss: 0.9414 - val_accuracy: 0.6674\n",
      "Epoch 63/400\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.5461 - accuracy: 0.7307\n",
      "Epoch 63: val_accuracy did not improve from 0.66964\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.5461 - accuracy: 0.7307 - val_loss: 1.0047 - val_accuracy: 0.6339\n",
      "Epoch 64/400\n",
      "15/21 [====================>.........] - ETA: 0s - loss: 0.5207 - accuracy: 0.7563\n",
      "Epoch 64: val_accuracy did not improve from 0.66964\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.5509 - accuracy: 0.7381 - val_loss: 0.9702 - val_accuracy: 0.6518\n",
      "Epoch 65/400\n",
      "15/21 [====================>.........] - ETA: 0s - loss: 0.5210 - accuracy: 0.7583\n",
      "Epoch 65: val_accuracy did not improve from 0.66964\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.5256 - accuracy: 0.7440 - val_loss: 0.9832 - val_accuracy: 0.6518\n",
      "Epoch 66/400\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.5320 - accuracy: 0.7470\n",
      "Epoch 66: val_accuracy did not improve from 0.66964\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.5320 - accuracy: 0.7470 - val_loss: 0.9729 - val_accuracy: 0.6607\n",
      "Epoch 67/400\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.5006 - accuracy: 0.7589\n",
      "Epoch 67: val_accuracy did not improve from 0.66964\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.5006 - accuracy: 0.7589 - val_loss: 0.9687 - val_accuracy: 0.6585\n",
      "Epoch 68/400\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.5197 - accuracy: 0.7566\n",
      "Epoch 68: val_accuracy did not improve from 0.66964\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.5260 - accuracy: 0.7500 - val_loss: 0.9937 - val_accuracy: 0.6406\n",
      "Epoch 69/400\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 0.5442 - accuracy: 0.7312\n",
      "Epoch 69: val_accuracy did not improve from 0.66964\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.5323 - accuracy: 0.7366 - val_loss: 0.9787 - val_accuracy: 0.6629\n",
      "Epoch 70/400\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 0.5371 - accuracy: 0.7453\n",
      "Epoch 70: val_accuracy did not improve from 0.66964\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.5325 - accuracy: 0.7500 - val_loss: 0.9908 - val_accuracy: 0.6496\n",
      "Epoch 71/400\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.5162 - accuracy: 0.7467\n",
      "Epoch 71: val_accuracy did not improve from 0.66964\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.5157 - accuracy: 0.7515 - val_loss: 1.0186 - val_accuracy: 0.6652\n",
      "Epoch 72/400\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 0.5358 - accuracy: 0.7437\n",
      "Epoch 72: val_accuracy did not improve from 0.66964\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.5329 - accuracy: 0.7485 - val_loss: 1.0087 - val_accuracy: 0.6585\n",
      "Epoch 73/400\n",
      "18/21 [========================>.....] - ETA: 0s - loss: 0.5114 - accuracy: 0.7361\n",
      "Epoch 73: val_accuracy did not improve from 0.66964\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.5143 - accuracy: 0.7366 - val_loss: 1.0244 - val_accuracy: 0.6518\n",
      "Epoch 74/400\n",
      "15/21 [====================>.........] - ETA: 0s - loss: 0.5113 - accuracy: 0.7729\n",
      "Epoch 74: val_accuracy did not improve from 0.66964\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.5128 - accuracy: 0.7664 - val_loss: 1.0323 - val_accuracy: 0.6629\n",
      "Epoch 75/400\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 0.4821 - accuracy: 0.7781\n",
      "Epoch 75: val_accuracy did not improve from 0.66964\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.4902 - accuracy: 0.7723 - val_loss: 1.1053 - val_accuracy: 0.6317\n",
      "Epoch 76/400\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.4907 - accuracy: 0.7895\n",
      "Epoch 76: val_accuracy did not improve from 0.66964\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.4918 - accuracy: 0.7887 - val_loss: 1.0430 - val_accuracy: 0.6674\n",
      "Epoch 77/400\n",
      "18/21 [========================>.....] - ETA: 0s - loss: 0.4946 - accuracy: 0.7587\n",
      "Epoch 77: val_accuracy did not improve from 0.66964\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.4983 - accuracy: 0.7649 - val_loss: 1.0574 - val_accuracy: 0.6429\n",
      "Epoch 78/400\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.4913 - accuracy: 0.7589\n",
      "Epoch 78: val_accuracy did not improve from 0.66964\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.4913 - accuracy: 0.7589 - val_loss: 1.0653 - val_accuracy: 0.6585\n",
      "Epoch 79/400\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 0.5079 - accuracy: 0.7703\n",
      "Epoch 79: val_accuracy did not improve from 0.66964\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.5057 - accuracy: 0.7693 - val_loss: 1.1144 - val_accuracy: 0.6317\n",
      "Epoch 80/400\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.4999 - accuracy: 0.7697\n",
      "Epoch 80: val_accuracy did not improve from 0.66964\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.5027 - accuracy: 0.7679 - val_loss: 1.0556 - val_accuracy: 0.6562\n",
      "Epoch 81/400\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.5031 - accuracy: 0.7485\n",
      "Epoch 81: val_accuracy did not improve from 0.66964\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.5031 - accuracy: 0.7485 - val_loss: 1.0715 - val_accuracy: 0.6384\n",
      "Epoch 82/400\n",
      "15/21 [====================>.........] - ETA: 0s - loss: 0.4900 - accuracy: 0.7500\n",
      "Epoch 82: val_accuracy did not improve from 0.66964\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.4935 - accuracy: 0.7470 - val_loss: 1.0817 - val_accuracy: 0.6607\n",
      "Epoch 83/400\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.4904 - accuracy: 0.7783\n",
      "Epoch 83: val_accuracy did not improve from 0.66964\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.4904 - accuracy: 0.7783 - val_loss: 1.0823 - val_accuracy: 0.6696\n",
      "Epoch 84/400\n",
      "15/21 [====================>.........] - ETA: 0s - loss: 0.4991 - accuracy: 0.7708\n",
      "Epoch 84: val_accuracy did not improve from 0.66964\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.4942 - accuracy: 0.7708 - val_loss: 1.0520 - val_accuracy: 0.6562\n",
      "Epoch 85/400\n",
      "15/21 [====================>.........] - ETA: 0s - loss: 0.4940 - accuracy: 0.7708\n",
      "Epoch 85: val_accuracy did not improve from 0.66964\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.4959 - accuracy: 0.7693 - val_loss: 1.0969 - val_accuracy: 0.6629\n",
      "Epoch 86/400\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.4612 - accuracy: 0.8006\n",
      "Epoch 86: val_accuracy did not improve from 0.66964\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.4612 - accuracy: 0.8006 - val_loss: 1.0859 - val_accuracy: 0.6562\n",
      "Epoch 87/400\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.4463 - accuracy: 0.8036\n",
      "Epoch 87: val_accuracy did not improve from 0.66964\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.4463 - accuracy: 0.8036 - val_loss: 1.1013 - val_accuracy: 0.6674\n",
      "Epoch 88/400\n",
      "15/21 [====================>.........] - ETA: 0s - loss: 0.4773 - accuracy: 0.7667\n",
      "Epoch 88: val_accuracy did not improve from 0.66964\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.4744 - accuracy: 0.7679 - val_loss: 1.1005 - val_accuracy: 0.6562\n",
      "Epoch 89/400\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.4781 - accuracy: 0.7753\n",
      "Epoch 89: val_accuracy did not improve from 0.66964\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.4781 - accuracy: 0.7753 - val_loss: 1.0440 - val_accuracy: 0.6652\n",
      "Epoch 90/400\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.4903 - accuracy: 0.7730\n",
      "Epoch 90: val_accuracy did not improve from 0.66964\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.4751 - accuracy: 0.7798 - val_loss: 1.0882 - val_accuracy: 0.6540\n",
      "Epoch 91/400\n",
      "15/21 [====================>.........] - ETA: 0s - loss: 0.4559 - accuracy: 0.7833\n",
      "Epoch 91: val_accuracy did not improve from 0.66964\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.4784 - accuracy: 0.7708 - val_loss: 1.1386 - val_accuracy: 0.6362\n",
      "Epoch 92/400\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.4523 - accuracy: 0.8036\n",
      "Epoch 92: val_accuracy did not improve from 0.66964\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.4523 - accuracy: 0.8036 - val_loss: 1.1380 - val_accuracy: 0.6496\n",
      "Epoch 93/400\n",
      "15/21 [====================>.........] - ETA: 0s - loss: 0.4995 - accuracy: 0.7667\n",
      "Epoch 93: val_accuracy did not improve from 0.66964\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.4602 - accuracy: 0.7932 - val_loss: 1.1402 - val_accuracy: 0.6629\n",
      "Epoch 94/400\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 0.4397 - accuracy: 0.7984\n",
      "Epoch 94: val_accuracy did not improve from 0.66964\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.4496 - accuracy: 0.7917 - val_loss: 1.0981 - val_accuracy: 0.6629\n",
      "Epoch 95/400\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 0.4540 - accuracy: 0.8062\n",
      "Epoch 95: val_accuracy did not improve from 0.66964\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.4586 - accuracy: 0.8036 - val_loss: 1.2002 - val_accuracy: 0.6429\n",
      "Epoch 96/400\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.4575 - accuracy: 0.7902\n",
      "Epoch 96: val_accuracy did not improve from 0.66964\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.4575 - accuracy: 0.7902 - val_loss: 1.1482 - val_accuracy: 0.6696\n",
      "Epoch 97/400\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.4558 - accuracy: 0.7946\n",
      "Epoch 97: val_accuracy did not improve from 0.66964\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.4558 - accuracy: 0.7946 - val_loss: 1.1550 - val_accuracy: 0.6696\n",
      "Epoch 98/400\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 0.4524 - accuracy: 0.7797\n",
      "Epoch 98: val_accuracy did not improve from 0.66964\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.4512 - accuracy: 0.7812 - val_loss: 1.1540 - val_accuracy: 0.6540\n",
      "Epoch 99/400\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.4701 - accuracy: 0.7812\n",
      "Epoch 99: val_accuracy did not improve from 0.66964\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.4700 - accuracy: 0.7842 - val_loss: 1.1394 - val_accuracy: 0.6607\n",
      "Epoch 100/400\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.4548 - accuracy: 0.7993\n",
      "Epoch 100: val_accuracy did not improve from 0.66964\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.4492 - accuracy: 0.7991 - val_loss: 1.1362 - val_accuracy: 0.6629\n",
      "Epoch 101/400\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.4795 - accuracy: 0.7845\n",
      "Epoch 101: val_accuracy did not improve from 0.66964\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.4822 - accuracy: 0.7812 - val_loss: 1.1489 - val_accuracy: 0.6473\n",
      "Epoch 102/400\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.4260 - accuracy: 0.8043\n",
      "Epoch 102: val_accuracy did not improve from 0.66964\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.4298 - accuracy: 0.8051 - val_loss: 1.1390 - val_accuracy: 0.6585\n",
      "Epoch 103/400\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.4412 - accuracy: 0.7895\n",
      "Epoch 103: val_accuracy did not improve from 0.66964\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.4332 - accuracy: 0.7961 - val_loss: 1.1718 - val_accuracy: 0.6585\n",
      "Epoch 104/400\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 0.4394 - accuracy: 0.7828\n",
      "Epoch 104: val_accuracy did not improve from 0.66964\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.4441 - accuracy: 0.7812 - val_loss: 1.1937 - val_accuracy: 0.6317\n",
      "Epoch 105/400\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 0.4445 - accuracy: 0.8062\n",
      "Epoch 105: val_accuracy did not improve from 0.66964\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.4483 - accuracy: 0.8051 - val_loss: 1.1540 - val_accuracy: 0.6406\n",
      "Epoch 106/400\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.4321 - accuracy: 0.8059\n",
      "Epoch 106: val_accuracy improved from 0.66964 to 0.68080, saving model to best_model_weights.h5\n",
      "21/21 [==============================] - 0s 15ms/step - loss: 0.4304 - accuracy: 0.8036 - val_loss: 1.1875 - val_accuracy: 0.6808\n",
      "Epoch 107/400\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.4580 - accuracy: 0.7895\n",
      "Epoch 107: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.4608 - accuracy: 0.7827 - val_loss: 1.1839 - val_accuracy: 0.6540\n",
      "Epoch 108/400\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.4595 - accuracy: 0.7961\n",
      "Epoch 108: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.4628 - accuracy: 0.7872 - val_loss: 1.2004 - val_accuracy: 0.6473\n",
      "Epoch 109/400\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.4696 - accuracy: 0.7993\n",
      "Epoch 109: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.4575 - accuracy: 0.8065 - val_loss: 1.2069 - val_accuracy: 0.6607\n",
      "Epoch 110/400\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.4534 - accuracy: 0.7697\n",
      "Epoch 110: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.4402 - accuracy: 0.7798 - val_loss: 1.2076 - val_accuracy: 0.6518\n",
      "Epoch 111/400\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.4192 - accuracy: 0.7944\n",
      "Epoch 111: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.4245 - accuracy: 0.7902 - val_loss: 1.1939 - val_accuracy: 0.6362\n",
      "Epoch 112/400\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.4769 - accuracy: 0.7763\n",
      "Epoch 112: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.4693 - accuracy: 0.7812 - val_loss: 1.1808 - val_accuracy: 0.6741\n",
      "Epoch 113/400\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.4210 - accuracy: 0.8170\n",
      "Epoch 113: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.4210 - accuracy: 0.8170 - val_loss: 1.2058 - val_accuracy: 0.6674\n",
      "Epoch 114/400\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.4387 - accuracy: 0.7993\n",
      "Epoch 114: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.4416 - accuracy: 0.7976 - val_loss: 1.1827 - val_accuracy: 0.6652\n",
      "Epoch 115/400\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 0.4158 - accuracy: 0.8094\n",
      "Epoch 115: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.4178 - accuracy: 0.8125 - val_loss: 1.2505 - val_accuracy: 0.6696\n",
      "Epoch 116/400\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.4131 - accuracy: 0.7961\n",
      "Epoch 116: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.4105 - accuracy: 0.7946 - val_loss: 1.2167 - val_accuracy: 0.6406\n",
      "Epoch 117/400\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.4373 - accuracy: 0.8289\n",
      "Epoch 117: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.4452 - accuracy: 0.8259 - val_loss: 1.2338 - val_accuracy: 0.6429\n",
      "Epoch 118/400\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.4187 - accuracy: 0.8065\n",
      "Epoch 118: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.4187 - accuracy: 0.8065 - val_loss: 1.2139 - val_accuracy: 0.6607\n",
      "Epoch 119/400\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 0.4191 - accuracy: 0.8000\n",
      "Epoch 119: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.4151 - accuracy: 0.8006 - val_loss: 1.3153 - val_accuracy: 0.6429\n",
      "Epoch 120/400\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.4175 - accuracy: 0.8199\n",
      "Epoch 120: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.4175 - accuracy: 0.8199 - val_loss: 1.2620 - val_accuracy: 0.6518\n",
      "Epoch 121/400\n",
      "16/21 [=====================>........] - ETA: 0s - loss: 0.4461 - accuracy: 0.7891\n",
      "Epoch 121: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.4332 - accuracy: 0.7961 - val_loss: 1.2183 - val_accuracy: 0.6629\n",
      "Epoch 122/400\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.4185 - accuracy: 0.8065\n",
      "Epoch 122: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.4185 - accuracy: 0.8065 - val_loss: 1.1974 - val_accuracy: 0.6540\n",
      "Epoch 123/400\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.4342 - accuracy: 0.8170\n",
      "Epoch 123: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.4342 - accuracy: 0.8170 - val_loss: 1.2267 - val_accuracy: 0.6719\n",
      "Epoch 124/400\n",
      "15/21 [====================>.........] - ETA: 0s - loss: 0.3913 - accuracy: 0.8354\n",
      "Epoch 124: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.4085 - accuracy: 0.8229 - val_loss: 1.2080 - val_accuracy: 0.6473\n",
      "Epoch 125/400\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.3979 - accuracy: 0.8095\n",
      "Epoch 125: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.3979 - accuracy: 0.8095 - val_loss: 1.2466 - val_accuracy: 0.6518\n",
      "Epoch 126/400\n",
      "15/21 [====================>.........] - ETA: 0s - loss: 0.4120 - accuracy: 0.8083\n",
      "Epoch 126: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.4130 - accuracy: 0.8110 - val_loss: 1.2559 - val_accuracy: 0.6763\n",
      "Epoch 127/400\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.4247 - accuracy: 0.8155\n",
      "Epoch 127: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.4247 - accuracy: 0.8155 - val_loss: 1.2641 - val_accuracy: 0.6406\n",
      "Epoch 128/400\n",
      "15/21 [====================>.........] - ETA: 0s - loss: 0.3583 - accuracy: 0.8500\n",
      "Epoch 128: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.3960 - accuracy: 0.8244 - val_loss: 1.2392 - val_accuracy: 0.6518\n",
      "Epoch 129/400\n",
      "15/21 [====================>.........] - ETA: 0s - loss: 0.4223 - accuracy: 0.7875\n",
      "Epoch 129: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.4219 - accuracy: 0.7917 - val_loss: 1.3078 - val_accuracy: 0.6429\n",
      "Epoch 130/400\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.3956 - accuracy: 0.8006\n",
      "Epoch 130: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.3956 - accuracy: 0.8006 - val_loss: 1.2509 - val_accuracy: 0.6674\n",
      "Epoch 131/400\n",
      "15/21 [====================>.........] - ETA: 0s - loss: 0.3755 - accuracy: 0.8271\n",
      "Epoch 131: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.3920 - accuracy: 0.8244 - val_loss: 1.2432 - val_accuracy: 0.6451\n",
      "Epoch 132/400\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.3908 - accuracy: 0.8080\n",
      "Epoch 132: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.3908 - accuracy: 0.8080 - val_loss: 1.3433 - val_accuracy: 0.6250\n",
      "Epoch 133/400\n",
      "15/21 [====================>.........] - ETA: 0s - loss: 0.3757 - accuracy: 0.8417\n",
      "Epoch 133: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.3907 - accuracy: 0.8274 - val_loss: 1.2653 - val_accuracy: 0.6406\n",
      "Epoch 134/400\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.3862 - accuracy: 0.8199\n",
      "Epoch 134: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.3862 - accuracy: 0.8199 - val_loss: 1.2810 - val_accuracy: 0.6429\n",
      "Epoch 135/400\n",
      "15/21 [====================>.........] - ETA: 0s - loss: 0.4105 - accuracy: 0.8104\n",
      "Epoch 135: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.4105 - accuracy: 0.8080 - val_loss: 1.3372 - val_accuracy: 0.6496\n",
      "Epoch 136/400\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 0.4101 - accuracy: 0.8016\n",
      "Epoch 136: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.4184 - accuracy: 0.7976 - val_loss: 1.2616 - val_accuracy: 0.6473\n",
      "Epoch 137/400\n",
      "17/21 [=======================>......] - ETA: 0s - loss: 0.3915 - accuracy: 0.8438\n",
      "Epoch 137: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.3990 - accuracy: 0.8304 - val_loss: 1.2684 - val_accuracy: 0.6540\n",
      "Epoch 138/400\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.3893 - accuracy: 0.8214\n",
      "Epoch 138: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.3893 - accuracy: 0.8214 - val_loss: 1.2725 - val_accuracy: 0.6362\n",
      "Epoch 139/400\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.4000 - accuracy: 0.8158\n",
      "Epoch 139: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.4062 - accuracy: 0.8095 - val_loss: 1.2879 - val_accuracy: 0.6339\n",
      "Epoch 140/400\n",
      "15/21 [====================>.........] - ETA: 0s - loss: 0.4179 - accuracy: 0.8000\n",
      "Epoch 140: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.4112 - accuracy: 0.8065 - val_loss: 1.3022 - val_accuracy: 0.6429\n",
      "Epoch 141/400\n",
      "15/21 [====================>.........] - ETA: 0s - loss: 0.3690 - accuracy: 0.8479\n",
      "Epoch 141: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.3590 - accuracy: 0.8497 - val_loss: 1.2958 - val_accuracy: 0.6384\n",
      "Epoch 142/400\n",
      "16/21 [=====================>........] - ETA: 0s - loss: 0.3659 - accuracy: 0.8301\n",
      "Epoch 142: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.3785 - accuracy: 0.8289 - val_loss: 1.3349 - val_accuracy: 0.6585\n",
      "Epoch 143/400\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.3636 - accuracy: 0.8408\n",
      "Epoch 143: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.3636 - accuracy: 0.8408 - val_loss: 1.4608 - val_accuracy: 0.6339\n",
      "Epoch 144/400\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 0.3897 - accuracy: 0.8297\n",
      "Epoch 144: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.3957 - accuracy: 0.8244 - val_loss: 1.3325 - val_accuracy: 0.6562\n",
      "Epoch 145/400\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.3695 - accuracy: 0.8378\n",
      "Epoch 145: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.3695 - accuracy: 0.8378 - val_loss: 1.3722 - val_accuracy: 0.6540\n",
      "Epoch 146/400\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.3802 - accuracy: 0.8348\n",
      "Epoch 146: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.3802 - accuracy: 0.8348 - val_loss: 1.3420 - val_accuracy: 0.6339\n",
      "Epoch 147/400\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.3663 - accuracy: 0.8318\n",
      "Epoch 147: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.3663 - accuracy: 0.8318 - val_loss: 1.3239 - val_accuracy: 0.6629\n",
      "Epoch 148/400\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.4021 - accuracy: 0.8051\n",
      "Epoch 148: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.4021 - accuracy: 0.8051 - val_loss: 1.3342 - val_accuracy: 0.6540\n",
      "Epoch 149/400\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 0.3882 - accuracy: 0.8219\n",
      "Epoch 149: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.3900 - accuracy: 0.8199 - val_loss: 1.2702 - val_accuracy: 0.6518\n",
      "Epoch 150/400\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.4028 - accuracy: 0.8199\n",
      "Epoch 150: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.4028 - accuracy: 0.8199 - val_loss: 1.2846 - val_accuracy: 0.6473\n",
      "Epoch 151/400\n",
      "16/21 [=====================>........] - ETA: 0s - loss: 0.3939 - accuracy: 0.8027\n",
      "Epoch 151: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.3722 - accuracy: 0.8170 - val_loss: 1.3416 - val_accuracy: 0.6540\n",
      "Epoch 152/400\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.3789 - accuracy: 0.8244\n",
      "Epoch 152: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.3789 - accuracy: 0.8244 - val_loss: 1.3196 - val_accuracy: 0.6518\n",
      "Epoch 153/400\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 0.3639 - accuracy: 0.8313\n",
      "Epoch 153: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.3629 - accuracy: 0.8304 - val_loss: 1.3860 - val_accuracy: 0.6540\n",
      "Epoch 154/400\n",
      "15/21 [====================>.........] - ETA: 0s - loss: 0.3603 - accuracy: 0.8458\n",
      "Epoch 154: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.3706 - accuracy: 0.8482 - val_loss: 1.3387 - val_accuracy: 0.6473\n",
      "Epoch 155/400\n",
      "14/21 [===================>..........] - ETA: 0s - loss: 0.3431 - accuracy: 0.8527\n",
      "Epoch 155: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.3524 - accuracy: 0.8452 - val_loss: 1.2967 - val_accuracy: 0.6496\n",
      "Epoch 156/400\n",
      "15/21 [====================>.........] - ETA: 0s - loss: 0.3698 - accuracy: 0.8271\n",
      "Epoch 156: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.3817 - accuracy: 0.8229 - val_loss: 1.3901 - val_accuracy: 0.6384\n",
      "Epoch 157/400\n",
      "15/21 [====================>.........] - ETA: 0s - loss: 0.3619 - accuracy: 0.8229\n",
      "Epoch 157: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.3820 - accuracy: 0.8244 - val_loss: 1.2945 - val_accuracy: 0.6562\n",
      "Epoch 158/400\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.3798 - accuracy: 0.8318\n",
      "Epoch 158: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.3798 - accuracy: 0.8318 - val_loss: 1.2646 - val_accuracy: 0.6607\n",
      "Epoch 159/400\n",
      "16/21 [=====================>........] - ETA: 0s - loss: 0.3827 - accuracy: 0.8203\n",
      "Epoch 159: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.3678 - accuracy: 0.8289 - val_loss: 1.3546 - val_accuracy: 0.6607\n",
      "Epoch 160/400\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 0.3849 - accuracy: 0.8406\n",
      "Epoch 160: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.3746 - accuracy: 0.8467 - val_loss: 1.3344 - val_accuracy: 0.6496\n",
      "Epoch 161/400\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.3386 - accuracy: 0.8795\n",
      "Epoch 161: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.3386 - accuracy: 0.8795 - val_loss: 1.3488 - val_accuracy: 0.6362\n",
      "Epoch 162/400\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.3667 - accuracy: 0.8423\n",
      "Epoch 162: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.3667 - accuracy: 0.8423 - val_loss: 1.3700 - val_accuracy: 0.6518\n",
      "Epoch 163/400\n",
      "14/21 [===================>..........] - ETA: 0s - loss: 0.3711 - accuracy: 0.8326\n",
      "Epoch 163: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.3915 - accuracy: 0.8155 - val_loss: 1.3525 - val_accuracy: 0.6562\n",
      "Epoch 164/400\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.3505 - accuracy: 0.8527\n",
      "Epoch 164: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.3505 - accuracy: 0.8527 - val_loss: 1.3774 - val_accuracy: 0.6317\n",
      "Epoch 165/400\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.3685 - accuracy: 0.8467\n",
      "Epoch 165: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.3685 - accuracy: 0.8467 - val_loss: 1.3590 - val_accuracy: 0.6607\n",
      "Epoch 166/400\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.3760 - accuracy: 0.8333\n",
      "Epoch 166: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.3760 - accuracy: 0.8333 - val_loss: 1.3788 - val_accuracy: 0.6652\n",
      "Epoch 167/400\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.3428 - accuracy: 0.8438\n",
      "Epoch 167: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.3428 - accuracy: 0.8438 - val_loss: 1.4413 - val_accuracy: 0.6585\n",
      "Epoch 168/400\n",
      "16/21 [=====================>........] - ETA: 0s - loss: 0.3529 - accuracy: 0.8477\n",
      "Epoch 168: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.3575 - accuracy: 0.8467 - val_loss: 1.3695 - val_accuracy: 0.6429\n",
      "Epoch 169/400\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 0.4005 - accuracy: 0.8219\n",
      "Epoch 169: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.3961 - accuracy: 0.8229 - val_loss: 1.3756 - val_accuracy: 0.6518\n",
      "Epoch 170/400\n",
      "15/21 [====================>.........] - ETA: 0s - loss: 0.3334 - accuracy: 0.8438\n",
      "Epoch 170: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.3331 - accuracy: 0.8438 - val_loss: 1.4963 - val_accuracy: 0.6317\n",
      "Epoch 171/400\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.3464 - accuracy: 0.8542\n",
      "Epoch 171: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.3464 - accuracy: 0.8542 - val_loss: 1.4158 - val_accuracy: 0.6652\n",
      "Epoch 172/400\n",
      "15/21 [====================>.........] - ETA: 0s - loss: 0.3652 - accuracy: 0.8313\n",
      "Epoch 172: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.3658 - accuracy: 0.8289 - val_loss: 1.4241 - val_accuracy: 0.6473\n",
      "Epoch 173/400\n",
      "15/21 [====================>.........] - ETA: 0s - loss: 0.3575 - accuracy: 0.8313\n",
      "Epoch 173: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.3619 - accuracy: 0.8348 - val_loss: 1.4050 - val_accuracy: 0.6496\n",
      "Epoch 174/400\n",
      "15/21 [====================>.........] - ETA: 0s - loss: 0.3432 - accuracy: 0.8521\n",
      "Epoch 174: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.3639 - accuracy: 0.8304 - val_loss: 1.4141 - val_accuracy: 0.6607\n",
      "Epoch 175/400\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.3493 - accuracy: 0.8393\n",
      "Epoch 175: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.3493 - accuracy: 0.8393 - val_loss: 1.4165 - val_accuracy: 0.6518\n",
      "Epoch 176/400\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 0.3621 - accuracy: 0.8406\n",
      "Epoch 176: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.3520 - accuracy: 0.8438 - val_loss: 1.4831 - val_accuracy: 0.6652\n",
      "Epoch 177/400\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.3567 - accuracy: 0.8571\n",
      "Epoch 177: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.3567 - accuracy: 0.8571 - val_loss: 1.5141 - val_accuracy: 0.6138\n",
      "Epoch 178/400\n",
      "15/21 [====================>.........] - ETA: 0s - loss: 0.3827 - accuracy: 0.8417\n",
      "Epoch 178: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.3619 - accuracy: 0.8497 - val_loss: 1.4265 - val_accuracy: 0.6562\n",
      "Epoch 179/400\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.3593 - accuracy: 0.8348\n",
      "Epoch 179: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.3593 - accuracy: 0.8348 - val_loss: 1.3472 - val_accuracy: 0.6473\n",
      "Epoch 180/400\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 0.3261 - accuracy: 0.8469\n",
      "Epoch 180: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.3331 - accuracy: 0.8438 - val_loss: 1.5023 - val_accuracy: 0.6562\n",
      "Epoch 181/400\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.3545 - accuracy: 0.8452\n",
      "Epoch 181: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.3545 - accuracy: 0.8452 - val_loss: 1.3925 - val_accuracy: 0.6272\n",
      "Epoch 182/400\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.3163 - accuracy: 0.8503\n",
      "Epoch 182: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.3234 - accuracy: 0.8497 - val_loss: 1.4575 - val_accuracy: 0.6406\n",
      "Epoch 183/400\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.3590 - accuracy: 0.8438\n",
      "Epoch 183: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.3599 - accuracy: 0.8438 - val_loss: 1.3764 - val_accuracy: 0.6451\n",
      "Epoch 184/400\n",
      "15/21 [====================>.........] - ETA: 0s - loss: 0.3171 - accuracy: 0.8708\n",
      "Epoch 184: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.3187 - accuracy: 0.8646 - val_loss: 1.4944 - val_accuracy: 0.6384\n",
      "Epoch 185/400\n",
      "16/21 [=====================>........] - ETA: 0s - loss: 0.3355 - accuracy: 0.8555\n",
      "Epoch 185: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.3301 - accuracy: 0.8482 - val_loss: 1.5101 - val_accuracy: 0.6317\n",
      "Epoch 186/400\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.3261 - accuracy: 0.8571\n",
      "Epoch 186: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.3261 - accuracy: 0.8571 - val_loss: 1.4935 - val_accuracy: 0.6339\n",
      "Epoch 187/400\n",
      "15/21 [====================>.........] - ETA: 0s - loss: 0.3240 - accuracy: 0.8646\n",
      "Epoch 187: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.3299 - accuracy: 0.8482 - val_loss: 1.3853 - val_accuracy: 0.6540\n",
      "Epoch 188/400\n",
      "14/21 [===================>..........] - ETA: 0s - loss: 0.3516 - accuracy: 0.8415\n",
      "Epoch 188: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.3322 - accuracy: 0.8452 - val_loss: 1.4251 - val_accuracy: 0.6496\n",
      "Epoch 189/400\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.3209 - accuracy: 0.8527\n",
      "Epoch 189: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.3209 - accuracy: 0.8527 - val_loss: 1.4506 - val_accuracy: 0.6473\n",
      "Epoch 190/400\n",
      "15/21 [====================>.........] - ETA: 0s - loss: 0.3201 - accuracy: 0.8625\n",
      "Epoch 190: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.3009 - accuracy: 0.8750 - val_loss: 1.4878 - val_accuracy: 0.6629\n",
      "Epoch 191/400\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.3333 - accuracy: 0.8553\n",
      "Epoch 191: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.3235 - accuracy: 0.8586 - val_loss: 1.5155 - val_accuracy: 0.6518\n",
      "Epoch 192/400\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 0.3347 - accuracy: 0.8578\n",
      "Epoch 192: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.3313 - accuracy: 0.8586 - val_loss: 1.5007 - val_accuracy: 0.6518\n",
      "Epoch 193/400\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.3147 - accuracy: 0.8616\n",
      "Epoch 193: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.3147 - accuracy: 0.8616 - val_loss: 1.4543 - val_accuracy: 0.6496\n",
      "Epoch 194/400\n",
      "15/21 [====================>.........] - ETA: 0s - loss: 0.3160 - accuracy: 0.8521\n",
      "Epoch 194: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.3244 - accuracy: 0.8497 - val_loss: 1.4898 - val_accuracy: 0.6473\n",
      "Epoch 195/400\n",
      "14/21 [===================>..........] - ETA: 0s - loss: 0.3538 - accuracy: 0.8527\n",
      "Epoch 195: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.3414 - accuracy: 0.8482 - val_loss: 1.4596 - val_accuracy: 0.6562\n",
      "Epoch 196/400\n",
      "15/21 [====================>.........] - ETA: 0s - loss: 0.3035 - accuracy: 0.8792\n",
      "Epoch 196: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.3267 - accuracy: 0.8646 - val_loss: 1.4078 - val_accuracy: 0.6429\n",
      "Epoch 197/400\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.3348 - accuracy: 0.8527\n",
      "Epoch 197: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.3348 - accuracy: 0.8527 - val_loss: 1.4345 - val_accuracy: 0.6451\n",
      "Epoch 198/400\n",
      "14/21 [===================>..........] - ETA: 0s - loss: 0.3611 - accuracy: 0.8415\n",
      "Epoch 198: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.3359 - accuracy: 0.8497 - val_loss: 1.4907 - val_accuracy: 0.6451\n",
      "Epoch 199/400\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.3361 - accuracy: 0.8616\n",
      "Epoch 199: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.3361 - accuracy: 0.8616 - val_loss: 1.4107 - val_accuracy: 0.6473\n",
      "Epoch 200/400\n",
      "15/21 [====================>.........] - ETA: 0s - loss: 0.2879 - accuracy: 0.8792\n",
      "Epoch 200: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.3146 - accuracy: 0.8631 - val_loss: 1.4546 - val_accuracy: 0.6295\n",
      "Epoch 201/400\n",
      "15/21 [====================>.........] - ETA: 0s - loss: 0.3253 - accuracy: 0.8500\n",
      "Epoch 201: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.3177 - accuracy: 0.8616 - val_loss: 1.5058 - val_accuracy: 0.6562\n",
      "Epoch 202/400\n",
      "15/21 [====================>.........] - ETA: 0s - loss: 0.3246 - accuracy: 0.8542\n",
      "Epoch 202: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.3241 - accuracy: 0.8586 - val_loss: 1.4947 - val_accuracy: 0.6384\n",
      "Epoch 203/400\n",
      "15/21 [====================>.........] - ETA: 0s - loss: 0.3461 - accuracy: 0.8396\n",
      "Epoch 203: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.3386 - accuracy: 0.8467 - val_loss: 1.3901 - val_accuracy: 0.6629\n",
      "Epoch 204/400\n",
      "15/21 [====================>.........] - ETA: 0s - loss: 0.3177 - accuracy: 0.8479\n",
      "Epoch 204: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.3013 - accuracy: 0.8527 - val_loss: 1.4435 - val_accuracy: 0.6652\n",
      "Epoch 205/400\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.3334 - accuracy: 0.8542\n",
      "Epoch 205: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.3334 - accuracy: 0.8542 - val_loss: 1.4511 - val_accuracy: 0.6518\n",
      "Epoch 206/400\n",
      "15/21 [====================>.........] - ETA: 0s - loss: 0.3266 - accuracy: 0.8583\n",
      "Epoch 206: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.3389 - accuracy: 0.8557 - val_loss: 1.4480 - val_accuracy: 0.6429\n",
      "Epoch 207/400\n",
      "15/21 [====================>.........] - ETA: 0s - loss: 0.3010 - accuracy: 0.8729\n",
      "Epoch 207: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.3110 - accuracy: 0.8631 - val_loss: 1.4177 - val_accuracy: 0.6496\n",
      "Epoch 208/400\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.3554 - accuracy: 0.8527\n",
      "Epoch 208: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.3554 - accuracy: 0.8527 - val_loss: 1.4088 - val_accuracy: 0.6607\n",
      "Epoch 209/400\n",
      "16/21 [=====================>........] - ETA: 0s - loss: 0.3203 - accuracy: 0.8633\n",
      "Epoch 209: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.3205 - accuracy: 0.8557 - val_loss: 1.4443 - val_accuracy: 0.6496\n",
      "Epoch 210/400\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.3180 - accuracy: 0.8735\n",
      "Epoch 210: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.3180 - accuracy: 0.8735 - val_loss: 1.4782 - val_accuracy: 0.6384\n",
      "Epoch 211/400\n",
      "15/21 [====================>.........] - ETA: 0s - loss: 0.2864 - accuracy: 0.8792\n",
      "Epoch 211: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.3057 - accuracy: 0.8765 - val_loss: 1.4797 - val_accuracy: 0.6496\n",
      "Epoch 212/400\n",
      "15/21 [====================>.........] - ETA: 0s - loss: 0.3038 - accuracy: 0.8708\n",
      "Epoch 212: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.3270 - accuracy: 0.8661 - val_loss: 1.5232 - val_accuracy: 0.6540\n",
      "Epoch 213/400\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.3145 - accuracy: 0.8646\n",
      "Epoch 213: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.3145 - accuracy: 0.8646 - val_loss: 1.5306 - val_accuracy: 0.6339\n",
      "Epoch 214/400\n",
      "16/21 [=====================>........] - ETA: 0s - loss: 0.3256 - accuracy: 0.8652\n",
      "Epoch 214: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.3198 - accuracy: 0.8646 - val_loss: 1.5413 - val_accuracy: 0.6496\n",
      "Epoch 215/400\n",
      "15/21 [====================>.........] - ETA: 0s - loss: 0.2692 - accuracy: 0.8813\n",
      "Epoch 215: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.2825 - accuracy: 0.8765 - val_loss: 1.4737 - val_accuracy: 0.6429\n",
      "Epoch 216/400\n",
      "16/21 [=====================>........] - ETA: 0s - loss: 0.3267 - accuracy: 0.8594\n",
      "Epoch 216: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.3238 - accuracy: 0.8542 - val_loss: 1.5131 - val_accuracy: 0.6585\n",
      "Epoch 217/400\n",
      "15/21 [====================>.........] - ETA: 0s - loss: 0.2799 - accuracy: 0.8813\n",
      "Epoch 217: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.2831 - accuracy: 0.8750 - val_loss: 1.5328 - val_accuracy: 0.6540\n",
      "Epoch 218/400\n",
      "15/21 [====================>.........] - ETA: 0s - loss: 0.2603 - accuracy: 0.8833\n",
      "Epoch 218: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.2932 - accuracy: 0.8705 - val_loss: 1.4764 - val_accuracy: 0.6429\n",
      "Epoch 219/400\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.3261 - accuracy: 0.8601\n",
      "Epoch 219: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.3261 - accuracy: 0.8601 - val_loss: 1.5402 - val_accuracy: 0.6317\n",
      "Epoch 220/400\n",
      "15/21 [====================>.........] - ETA: 0s - loss: 0.2998 - accuracy: 0.8729\n",
      "Epoch 220: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.3109 - accuracy: 0.8705 - val_loss: 1.5130 - val_accuracy: 0.6496\n",
      "Epoch 221/400\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.3102 - accuracy: 0.8646\n",
      "Epoch 221: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.3102 - accuracy: 0.8646 - val_loss: 1.4618 - val_accuracy: 0.6429\n",
      "Epoch 222/400\n",
      "15/21 [====================>.........] - ETA: 0s - loss: 0.3098 - accuracy: 0.8500\n",
      "Epoch 222: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.3095 - accuracy: 0.8571 - val_loss: 1.5523 - val_accuracy: 0.6518\n",
      "Epoch 223/400\n",
      "15/21 [====================>.........] - ETA: 0s - loss: 0.3057 - accuracy: 0.8667\n",
      "Epoch 223: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.3128 - accuracy: 0.8661 - val_loss: 1.5261 - val_accuracy: 0.6473\n",
      "Epoch 224/400\n",
      "15/21 [====================>.........] - ETA: 0s - loss: 0.2897 - accuracy: 0.8667\n",
      "Epoch 224: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.2959 - accuracy: 0.8690 - val_loss: 1.6347 - val_accuracy: 0.6295\n",
      "Epoch 225/400\n",
      "15/21 [====================>.........] - ETA: 0s - loss: 0.2980 - accuracy: 0.8646\n",
      "Epoch 225: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.3113 - accuracy: 0.8661 - val_loss: 1.5609 - val_accuracy: 0.6496\n",
      "Epoch 226/400\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 0.3238 - accuracy: 0.8672\n",
      "Epoch 226: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.3180 - accuracy: 0.8690 - val_loss: 1.6027 - val_accuracy: 0.6473\n",
      "Epoch 227/400\n",
      "16/21 [=====================>........] - ETA: 0s - loss: 0.3186 - accuracy: 0.8613\n",
      "Epoch 227: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.3158 - accuracy: 0.8646 - val_loss: 1.5868 - val_accuracy: 0.6384\n",
      "Epoch 228/400\n",
      "15/21 [====================>.........] - ETA: 0s - loss: 0.2582 - accuracy: 0.8875\n",
      "Epoch 228: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.3021 - accuracy: 0.8705 - val_loss: 1.6067 - val_accuracy: 0.6496\n",
      "Epoch 229/400\n",
      "15/21 [====================>.........] - ETA: 0s - loss: 0.2704 - accuracy: 0.8833\n",
      "Epoch 229: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.2839 - accuracy: 0.8780 - val_loss: 1.5734 - val_accuracy: 0.6406\n",
      "Epoch 230/400\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.3007 - accuracy: 0.8661\n",
      "Epoch 230: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.3007 - accuracy: 0.8661 - val_loss: 1.5423 - val_accuracy: 0.6429\n",
      "Epoch 231/400\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 0.3101 - accuracy: 0.8687\n",
      "Epoch 231: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.3106 - accuracy: 0.8705 - val_loss: 1.4760 - val_accuracy: 0.6295\n",
      "Epoch 232/400\n",
      "15/21 [====================>.........] - ETA: 0s - loss: 0.3196 - accuracy: 0.8708\n",
      "Epoch 232: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.3160 - accuracy: 0.8705 - val_loss: 1.4931 - val_accuracy: 0.6451\n",
      "Epoch 233/400\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.2681 - accuracy: 0.8795\n",
      "Epoch 233: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.2681 - accuracy: 0.8795 - val_loss: 1.5550 - val_accuracy: 0.6607\n",
      "Epoch 234/400\n",
      "16/21 [=====================>........] - ETA: 0s - loss: 0.2981 - accuracy: 0.8594\n",
      "Epoch 234: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.2773 - accuracy: 0.8705 - val_loss: 1.5451 - val_accuracy: 0.6362\n",
      "Epoch 235/400\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.2841 - accuracy: 0.8795\n",
      "Epoch 235: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.2841 - accuracy: 0.8795 - val_loss: 1.4632 - val_accuracy: 0.6585\n",
      "Epoch 236/400\n",
      "15/21 [====================>.........] - ETA: 0s - loss: 0.2848 - accuracy: 0.8729\n",
      "Epoch 236: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.2764 - accuracy: 0.8795 - val_loss: 1.5632 - val_accuracy: 0.6406\n",
      "Epoch 237/400\n",
      "16/21 [=====================>........] - ETA: 0s - loss: 0.2466 - accuracy: 0.8887\n",
      "Epoch 237: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.2765 - accuracy: 0.8810 - val_loss: 1.6387 - val_accuracy: 0.6406\n",
      "Epoch 238/400\n",
      "16/21 [=====================>........] - ETA: 0s - loss: 0.2894 - accuracy: 0.8711\n",
      "Epoch 238: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.2938 - accuracy: 0.8676 - val_loss: 1.6322 - val_accuracy: 0.6473\n",
      "Epoch 239/400\n",
      "16/21 [=====================>........] - ETA: 0s - loss: 0.2753 - accuracy: 0.8691\n",
      "Epoch 239: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.2767 - accuracy: 0.8661 - val_loss: 1.6483 - val_accuracy: 0.6429\n",
      "Epoch 240/400\n",
      "15/21 [====================>.........] - ETA: 0s - loss: 0.2951 - accuracy: 0.8813\n",
      "Epoch 240: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.2912 - accuracy: 0.8810 - val_loss: 1.5903 - val_accuracy: 0.6496\n",
      "Epoch 241/400\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.2932 - accuracy: 0.8720\n",
      "Epoch 241: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.2932 - accuracy: 0.8720 - val_loss: 1.5831 - val_accuracy: 0.6429\n",
      "Epoch 242/400\n",
      "15/21 [====================>.........] - ETA: 0s - loss: 0.2781 - accuracy: 0.8750\n",
      "Epoch 242: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.2829 - accuracy: 0.8676 - val_loss: 1.5131 - val_accuracy: 0.6473\n",
      "Epoch 243/400\n",
      "14/21 [===================>..........] - ETA: 0s - loss: 0.2714 - accuracy: 0.8862\n",
      "Epoch 243: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.2662 - accuracy: 0.8854 - val_loss: 1.6569 - val_accuracy: 0.6317\n",
      "Epoch 244/400\n",
      "15/21 [====================>.........] - ETA: 0s - loss: 0.3480 - accuracy: 0.8687\n",
      "Epoch 244: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.3177 - accuracy: 0.8795 - val_loss: 1.5326 - val_accuracy: 0.6496\n",
      "Epoch 245/400\n",
      "15/21 [====================>.........] - ETA: 0s - loss: 0.3201 - accuracy: 0.8562\n",
      "Epoch 245: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.2920 - accuracy: 0.8780 - val_loss: 1.5793 - val_accuracy: 0.6205\n",
      "Epoch 246/400\n",
      "14/21 [===================>..........] - ETA: 0s - loss: 0.2733 - accuracy: 0.8795\n",
      "Epoch 246: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.2838 - accuracy: 0.8735 - val_loss: 1.5701 - val_accuracy: 0.6429\n",
      "Epoch 247/400\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.2780 - accuracy: 0.8705\n",
      "Epoch 247: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.2780 - accuracy: 0.8705 - val_loss: 1.5523 - val_accuracy: 0.6629\n",
      "Epoch 248/400\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.3003 - accuracy: 0.8720\n",
      "Epoch 248: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.3003 - accuracy: 0.8720 - val_loss: 1.6531 - val_accuracy: 0.6406\n",
      "Epoch 249/400\n",
      "15/21 [====================>.........] - ETA: 0s - loss: 0.2786 - accuracy: 0.8792\n",
      "Epoch 249: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.2814 - accuracy: 0.8824 - val_loss: 1.6161 - val_accuracy: 0.6451\n",
      "Epoch 250/400\n",
      "15/21 [====================>.........] - ETA: 0s - loss: 0.2923 - accuracy: 0.8792\n",
      "Epoch 250: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.2887 - accuracy: 0.8780 - val_loss: 1.5708 - val_accuracy: 0.6652\n",
      "Epoch 251/400\n",
      "15/21 [====================>.........] - ETA: 0s - loss: 0.3264 - accuracy: 0.8583\n",
      "Epoch 251: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.3028 - accuracy: 0.8720 - val_loss: 1.5711 - val_accuracy: 0.6562\n",
      "Epoch 252/400\n",
      "15/21 [====================>.........] - ETA: 0s - loss: 0.2685 - accuracy: 0.8792\n",
      "Epoch 252: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.2754 - accuracy: 0.8810 - val_loss: 1.5848 - val_accuracy: 0.6585\n",
      "Epoch 253/400\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.2798 - accuracy: 0.8780\n",
      "Epoch 253: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.2798 - accuracy: 0.8780 - val_loss: 1.6045 - val_accuracy: 0.6562\n",
      "Epoch 254/400\n",
      "15/21 [====================>.........] - ETA: 0s - loss: 0.2933 - accuracy: 0.8729\n",
      "Epoch 254: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.2742 - accuracy: 0.8765 - val_loss: 1.6341 - val_accuracy: 0.6451\n",
      "Epoch 255/400\n",
      "16/21 [=====================>........] - ETA: 0s - loss: 0.2550 - accuracy: 0.8945\n",
      "Epoch 255: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.2752 - accuracy: 0.8914 - val_loss: 1.6563 - val_accuracy: 0.6518\n",
      "Epoch 256/400\n",
      "15/21 [====================>.........] - ETA: 0s - loss: 0.2997 - accuracy: 0.8687\n",
      "Epoch 256: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.2890 - accuracy: 0.8765 - val_loss: 1.6288 - val_accuracy: 0.6540\n",
      "Epoch 257/400\n",
      "15/21 [====================>.........] - ETA: 0s - loss: 0.3105 - accuracy: 0.8458\n",
      "Epoch 257: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.3011 - accuracy: 0.8601 - val_loss: 1.6223 - val_accuracy: 0.6384\n",
      "Epoch 258/400\n",
      "15/21 [====================>.........] - ETA: 0s - loss: 0.2999 - accuracy: 0.8792\n",
      "Epoch 258: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.2832 - accuracy: 0.8824 - val_loss: 1.6184 - val_accuracy: 0.6518\n",
      "Epoch 259/400\n",
      "14/21 [===================>..........] - ETA: 0s - loss: 0.2922 - accuracy: 0.8862\n",
      "Epoch 259: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.2812 - accuracy: 0.8810 - val_loss: 1.6442 - val_accuracy: 0.6518\n",
      "Epoch 260/400\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 0.2730 - accuracy: 0.8938\n",
      "Epoch 260: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.2720 - accuracy: 0.8929 - val_loss: 1.6476 - val_accuracy: 0.6473\n",
      "Epoch 261/400\n",
      "15/21 [====================>.........] - ETA: 0s - loss: 0.2725 - accuracy: 0.8750\n",
      "Epoch 261: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.2622 - accuracy: 0.8780 - val_loss: 1.6420 - val_accuracy: 0.6496\n",
      "Epoch 262/400\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.2809 - accuracy: 0.8765\n",
      "Epoch 262: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.2809 - accuracy: 0.8765 - val_loss: 1.6633 - val_accuracy: 0.6429\n",
      "Epoch 263/400\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.2856 - accuracy: 0.8899\n",
      "Epoch 263: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.2856 - accuracy: 0.8899 - val_loss: 1.5953 - val_accuracy: 0.6607\n",
      "Epoch 264/400\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.2816 - accuracy: 0.8899\n",
      "Epoch 264: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.2816 - accuracy: 0.8899 - val_loss: 1.6534 - val_accuracy: 0.6562\n",
      "Epoch 265/400\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.2535 - accuracy: 0.8929\n",
      "Epoch 265: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.2535 - accuracy: 0.8929 - val_loss: 1.6244 - val_accuracy: 0.6540\n",
      "Epoch 266/400\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.2762 - accuracy: 0.8854\n",
      "Epoch 266: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.2762 - accuracy: 0.8854 - val_loss: 1.6518 - val_accuracy: 0.6562\n",
      "Epoch 267/400\n",
      "15/21 [====================>.........] - ETA: 0s - loss: 0.3104 - accuracy: 0.8833\n",
      "Epoch 267: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.3041 - accuracy: 0.8869 - val_loss: 1.5837 - val_accuracy: 0.6473\n",
      "Epoch 268/400\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.2715 - accuracy: 0.8839\n",
      "Epoch 268: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.2715 - accuracy: 0.8839 - val_loss: 1.7225 - val_accuracy: 0.6384\n",
      "Epoch 269/400\n",
      "16/21 [=====================>........] - ETA: 0s - loss: 0.2693 - accuracy: 0.8906\n",
      "Epoch 269: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.2641 - accuracy: 0.8869 - val_loss: 1.6073 - val_accuracy: 0.6496\n",
      "Epoch 270/400\n",
      "15/21 [====================>.........] - ETA: 0s - loss: 0.3173 - accuracy: 0.8583\n",
      "Epoch 270: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.2978 - accuracy: 0.8661 - val_loss: 1.6478 - val_accuracy: 0.6429\n",
      "Epoch 271/400\n",
      "16/21 [=====================>........] - ETA: 0s - loss: 0.2723 - accuracy: 0.8828\n",
      "Epoch 271: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.2997 - accuracy: 0.8735 - val_loss: 1.6292 - val_accuracy: 0.6272\n",
      "Epoch 272/400\n",
      "15/21 [====================>.........] - ETA: 0s - loss: 0.2569 - accuracy: 0.8708\n",
      "Epoch 272: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.2568 - accuracy: 0.8780 - val_loss: 1.5794 - val_accuracy: 0.6629\n",
      "Epoch 273/400\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 0.2472 - accuracy: 0.8969\n",
      "Epoch 273: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.2490 - accuracy: 0.8988 - val_loss: 1.6268 - val_accuracy: 0.6451\n",
      "Epoch 274/400\n",
      "15/21 [====================>.........] - ETA: 0s - loss: 0.2646 - accuracy: 0.8875\n",
      "Epoch 274: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.2647 - accuracy: 0.8854 - val_loss: 1.6215 - val_accuracy: 0.6629\n",
      "Epoch 275/400\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.2772 - accuracy: 0.8765\n",
      "Epoch 275: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.2772 - accuracy: 0.8765 - val_loss: 1.6454 - val_accuracy: 0.6429\n",
      "Epoch 276/400\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.2402 - accuracy: 0.8899\n",
      "Epoch 276: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.2402 - accuracy: 0.8899 - val_loss: 1.6528 - val_accuracy: 0.6451\n",
      "Epoch 277/400\n",
      "16/21 [=====================>........] - ETA: 0s - loss: 0.2716 - accuracy: 0.8828\n",
      "Epoch 277: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.2913 - accuracy: 0.8750 - val_loss: 1.6601 - val_accuracy: 0.6228\n",
      "Epoch 278/400\n",
      "15/21 [====================>.........] - ETA: 0s - loss: 0.2984 - accuracy: 0.8875\n",
      "Epoch 278: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.2749 - accuracy: 0.8973 - val_loss: 1.7367 - val_accuracy: 0.6496\n",
      "Epoch 279/400\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.2505 - accuracy: 0.8869\n",
      "Epoch 279: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.2505 - accuracy: 0.8869 - val_loss: 1.7026 - val_accuracy: 0.6652\n",
      "Epoch 280/400\n",
      "15/21 [====================>.........] - ETA: 0s - loss: 0.2725 - accuracy: 0.8792\n",
      "Epoch 280: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.2629 - accuracy: 0.8810 - val_loss: 1.7018 - val_accuracy: 0.6451\n",
      "Epoch 281/400\n",
      "15/21 [====================>.........] - ETA: 0s - loss: 0.2570 - accuracy: 0.9000\n",
      "Epoch 281: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.2635 - accuracy: 0.9003 - val_loss: 1.7932 - val_accuracy: 0.6094\n",
      "Epoch 282/400\n",
      "16/21 [=====================>........] - ETA: 0s - loss: 0.2612 - accuracy: 0.8926\n",
      "Epoch 282: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.2750 - accuracy: 0.8854 - val_loss: 1.5992 - val_accuracy: 0.6496\n",
      "Epoch 283/400\n",
      "15/21 [====================>.........] - ETA: 0s - loss: 0.2587 - accuracy: 0.9000\n",
      "Epoch 283: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.2547 - accuracy: 0.8943 - val_loss: 1.6992 - val_accuracy: 0.6406\n",
      "Epoch 284/400\n",
      "15/21 [====================>.........] - ETA: 0s - loss: 0.2497 - accuracy: 0.8979\n",
      "Epoch 284: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.2517 - accuracy: 0.9048 - val_loss: 1.6450 - val_accuracy: 0.6473\n",
      "Epoch 285/400\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.2712 - accuracy: 0.8869\n",
      "Epoch 285: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.2712 - accuracy: 0.8869 - val_loss: 1.6663 - val_accuracy: 0.6518\n",
      "Epoch 286/400\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.2432 - accuracy: 0.8997\n",
      "Epoch 286: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.2438 - accuracy: 0.9018 - val_loss: 1.6337 - val_accuracy: 0.6384\n",
      "Epoch 287/400\n",
      "16/21 [=====================>........] - ETA: 0s - loss: 0.2419 - accuracy: 0.9004\n",
      "Epoch 287: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.2428 - accuracy: 0.9033 - val_loss: 1.7040 - val_accuracy: 0.6339\n",
      "Epoch 288/400\n",
      "15/21 [====================>.........] - ETA: 0s - loss: 0.2911 - accuracy: 0.8687\n",
      "Epoch 288: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.2763 - accuracy: 0.8765 - val_loss: 1.6834 - val_accuracy: 0.6406\n",
      "Epoch 289/400\n",
      "15/21 [====================>.........] - ETA: 0s - loss: 0.3003 - accuracy: 0.8813\n",
      "Epoch 289: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.2876 - accuracy: 0.8914 - val_loss: 1.6873 - val_accuracy: 0.6540\n",
      "Epoch 290/400\n",
      "14/21 [===================>..........] - ETA: 0s - loss: 0.2510 - accuracy: 0.8996\n",
      "Epoch 290: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.2522 - accuracy: 0.8988 - val_loss: 1.6704 - val_accuracy: 0.6339\n",
      "Epoch 291/400\n",
      "15/21 [====================>.........] - ETA: 0s - loss: 0.2365 - accuracy: 0.9021\n",
      "Epoch 291: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.2496 - accuracy: 0.9003 - val_loss: 1.5995 - val_accuracy: 0.6473\n",
      "Epoch 292/400\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 0.2471 - accuracy: 0.8984\n",
      "Epoch 292: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.2529 - accuracy: 0.8973 - val_loss: 1.6278 - val_accuracy: 0.6406\n",
      "Epoch 293/400\n",
      "15/21 [====================>.........] - ETA: 0s - loss: 0.2646 - accuracy: 0.8958\n",
      "Epoch 293: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.2773 - accuracy: 0.8854 - val_loss: 1.7175 - val_accuracy: 0.6138\n",
      "Epoch 294/400\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.2569 - accuracy: 0.8973\n",
      "Epoch 294: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.2569 - accuracy: 0.8973 - val_loss: 1.6725 - val_accuracy: 0.6518\n",
      "Epoch 295/400\n",
      "14/21 [===================>..........] - ETA: 0s - loss: 0.2640 - accuracy: 0.8996\n",
      "Epoch 295: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.2850 - accuracy: 0.8899 - val_loss: 1.5847 - val_accuracy: 0.6295\n",
      "Epoch 296/400\n",
      "15/21 [====================>.........] - ETA: 0s - loss: 0.2922 - accuracy: 0.8729\n",
      "Epoch 296: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.2821 - accuracy: 0.8839 - val_loss: 1.6313 - val_accuracy: 0.6429\n",
      "Epoch 297/400\n",
      "15/21 [====================>.........] - ETA: 0s - loss: 0.2710 - accuracy: 0.8667\n",
      "Epoch 297: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.2611 - accuracy: 0.8795 - val_loss: 1.6068 - val_accuracy: 0.6451\n",
      "Epoch 298/400\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.2434 - accuracy: 0.9003\n",
      "Epoch 298: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.2434 - accuracy: 0.9003 - val_loss: 1.6846 - val_accuracy: 0.6228\n",
      "Epoch 299/400\n",
      "15/21 [====================>.........] - ETA: 0s - loss: 0.2391 - accuracy: 0.8938\n",
      "Epoch 299: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.2523 - accuracy: 0.8884 - val_loss: 1.6796 - val_accuracy: 0.6384\n",
      "Epoch 300/400\n",
      "14/21 [===================>..........] - ETA: 0s - loss: 0.2576 - accuracy: 0.8973\n",
      "Epoch 300: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.2363 - accuracy: 0.9033 - val_loss: 1.6983 - val_accuracy: 0.6562\n",
      "Epoch 301/400\n",
      "16/21 [=====================>........] - ETA: 0s - loss: 0.2457 - accuracy: 0.8926\n",
      "Epoch 301: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.2516 - accuracy: 0.8929 - val_loss: 1.6982 - val_accuracy: 0.6272\n",
      "Epoch 302/400\n",
      "15/21 [====================>.........] - ETA: 0s - loss: 0.2614 - accuracy: 0.8792\n",
      "Epoch 302: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.2583 - accuracy: 0.8869 - val_loss: 1.6676 - val_accuracy: 0.6272\n",
      "Epoch 303/400\n",
      "16/21 [=====================>........] - ETA: 0s - loss: 0.2545 - accuracy: 0.8691\n",
      "Epoch 303: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.2684 - accuracy: 0.8705 - val_loss: 1.7205 - val_accuracy: 0.6496\n",
      "Epoch 304/400\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 0.2767 - accuracy: 0.9031\n",
      "Epoch 304: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.2792 - accuracy: 0.9003 - val_loss: 1.6665 - val_accuracy: 0.6562\n",
      "Epoch 305/400\n",
      "15/21 [====================>.........] - ETA: 0s - loss: 0.2629 - accuracy: 0.8979\n",
      "Epoch 305: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.2581 - accuracy: 0.9033 - val_loss: 1.7017 - val_accuracy: 0.6406\n",
      "Epoch 306/400\n",
      "15/21 [====================>.........] - ETA: 0s - loss: 0.2459 - accuracy: 0.8917\n",
      "Epoch 306: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.2583 - accuracy: 0.8958 - val_loss: 1.6342 - val_accuracy: 0.6585\n",
      "Epoch 307/400\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.2341 - accuracy: 0.8973\n",
      "Epoch 307: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.2341 - accuracy: 0.8973 - val_loss: 1.6947 - val_accuracy: 0.6629\n",
      "Epoch 308/400\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.2392 - accuracy: 0.9137\n",
      "Epoch 308: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.2392 - accuracy: 0.9137 - val_loss: 1.6951 - val_accuracy: 0.6317\n",
      "Epoch 309/400\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.2636 - accuracy: 0.8824\n",
      "Epoch 309: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.2636 - accuracy: 0.8824 - val_loss: 1.7105 - val_accuracy: 0.6384\n",
      "Epoch 310/400\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.2302 - accuracy: 0.9030\n",
      "Epoch 310: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.2205 - accuracy: 0.9062 - val_loss: 1.7050 - val_accuracy: 0.6518\n",
      "Epoch 311/400\n",
      "15/21 [====================>.........] - ETA: 0s - loss: 0.2315 - accuracy: 0.9000\n",
      "Epoch 311: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.2311 - accuracy: 0.8988 - val_loss: 1.6865 - val_accuracy: 0.6473\n",
      "Epoch 312/400\n",
      "15/21 [====================>.........] - ETA: 0s - loss: 0.2516 - accuracy: 0.8813\n",
      "Epoch 312: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.2417 - accuracy: 0.8929 - val_loss: 1.7603 - val_accuracy: 0.6295\n",
      "Epoch 313/400\n",
      "15/21 [====================>.........] - ETA: 0s - loss: 0.2361 - accuracy: 0.9021\n",
      "Epoch 313: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.2312 - accuracy: 0.9062 - val_loss: 1.7014 - val_accuracy: 0.6629\n",
      "Epoch 314/400\n",
      "17/21 [=======================>......] - ETA: 0s - loss: 0.2559 - accuracy: 0.8989\n",
      "Epoch 314: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.2411 - accuracy: 0.9018 - val_loss: 1.7865 - val_accuracy: 0.6384\n",
      "Epoch 315/400\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.2471 - accuracy: 0.8943\n",
      "Epoch 315: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.2471 - accuracy: 0.8943 - val_loss: 1.6415 - val_accuracy: 0.6406\n",
      "Epoch 316/400\n",
      "15/21 [====================>.........] - ETA: 0s - loss: 0.1994 - accuracy: 0.9062\n",
      "Epoch 316: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.2121 - accuracy: 0.9077 - val_loss: 1.6400 - val_accuracy: 0.6384\n",
      "Epoch 317/400\n",
      "15/21 [====================>.........] - ETA: 0s - loss: 0.2369 - accuracy: 0.8854\n",
      "Epoch 317: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.2425 - accuracy: 0.8854 - val_loss: 1.6966 - val_accuracy: 0.6540\n",
      "Epoch 318/400\n",
      "15/21 [====================>.........] - ETA: 0s - loss: 0.2198 - accuracy: 0.9104\n",
      "Epoch 318: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.2388 - accuracy: 0.9003 - val_loss: 1.6772 - val_accuracy: 0.6496\n",
      "Epoch 319/400\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 0.2128 - accuracy: 0.9031\n",
      "Epoch 319: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.2331 - accuracy: 0.8988 - val_loss: 1.6367 - val_accuracy: 0.6272\n",
      "Epoch 320/400\n",
      "16/21 [=====================>........] - ETA: 0s - loss: 0.2044 - accuracy: 0.9121\n",
      "Epoch 320: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.2206 - accuracy: 0.9062 - val_loss: 1.7441 - val_accuracy: 0.6384\n",
      "Epoch 321/400\n",
      "15/21 [====================>.........] - ETA: 0s - loss: 0.2315 - accuracy: 0.8938\n",
      "Epoch 321: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.2658 - accuracy: 0.8780 - val_loss: 1.7908 - val_accuracy: 0.6228\n",
      "Epoch 322/400\n",
      "14/21 [===================>..........] - ETA: 0s - loss: 0.2677 - accuracy: 0.8906\n",
      "Epoch 322: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.2616 - accuracy: 0.8899 - val_loss: 1.7146 - val_accuracy: 0.6272\n",
      "Epoch 323/400\n",
      "15/21 [====================>.........] - ETA: 0s - loss: 0.2524 - accuracy: 0.8938\n",
      "Epoch 323: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.2456 - accuracy: 0.9003 - val_loss: 1.6540 - val_accuracy: 0.6384\n",
      "Epoch 324/400\n",
      "15/21 [====================>.........] - ETA: 0s - loss: 0.2438 - accuracy: 0.8917\n",
      "Epoch 324: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.2363 - accuracy: 0.8929 - val_loss: 1.7819 - val_accuracy: 0.6406\n",
      "Epoch 325/400\n",
      "15/21 [====================>.........] - ETA: 0s - loss: 0.2402 - accuracy: 0.9187\n",
      "Epoch 325: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.2346 - accuracy: 0.9152 - val_loss: 1.8421 - val_accuracy: 0.6540\n",
      "Epoch 326/400\n",
      "15/21 [====================>.........] - ETA: 0s - loss: 0.2537 - accuracy: 0.8792\n",
      "Epoch 326: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.2288 - accuracy: 0.8958 - val_loss: 1.7642 - val_accuracy: 0.6496\n",
      "Epoch 327/400\n",
      "15/21 [====================>.........] - ETA: 0s - loss: 0.2660 - accuracy: 0.9000\n",
      "Epoch 327: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.2467 - accuracy: 0.9122 - val_loss: 1.7036 - val_accuracy: 0.6406\n",
      "Epoch 328/400\n",
      "15/21 [====================>.........] - ETA: 0s - loss: 0.2282 - accuracy: 0.9125\n",
      "Epoch 328: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.2302 - accuracy: 0.9092 - val_loss: 1.7802 - val_accuracy: 0.6473\n",
      "Epoch 329/400\n",
      "15/21 [====================>.........] - ETA: 0s - loss: 0.1816 - accuracy: 0.9229\n",
      "Epoch 329: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.2202 - accuracy: 0.9048 - val_loss: 1.7113 - val_accuracy: 0.6339\n",
      "Epoch 330/400\n",
      "14/21 [===================>..........] - ETA: 0s - loss: 0.2083 - accuracy: 0.9152\n",
      "Epoch 330: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.1994 - accuracy: 0.9196 - val_loss: 1.8411 - val_accuracy: 0.6272\n",
      "Epoch 331/400\n",
      "17/21 [=======================>......] - ETA: 0s - loss: 0.2256 - accuracy: 0.9026\n",
      "Epoch 331: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.2401 - accuracy: 0.8958 - val_loss: 1.8141 - val_accuracy: 0.6496\n",
      "Epoch 332/400\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.2581 - accuracy: 0.8964\n",
      "Epoch 332: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.2628 - accuracy: 0.8943 - val_loss: 1.7616 - val_accuracy: 0.6496\n",
      "Epoch 333/400\n",
      "16/21 [=====================>........] - ETA: 0s - loss: 0.2359 - accuracy: 0.9043\n",
      "Epoch 333: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.2425 - accuracy: 0.9048 - val_loss: 1.7797 - val_accuracy: 0.6429\n",
      "Epoch 334/400\n",
      "15/21 [====================>.........] - ETA: 0s - loss: 0.2547 - accuracy: 0.9062\n",
      "Epoch 334: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.2507 - accuracy: 0.8988 - val_loss: 1.7477 - val_accuracy: 0.6473\n",
      "Epoch 335/400\n",
      "16/21 [=====================>........] - ETA: 0s - loss: 0.2663 - accuracy: 0.9004\n",
      "Epoch 335: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.2428 - accuracy: 0.9062 - val_loss: 1.7126 - val_accuracy: 0.6540\n",
      "Epoch 336/400\n",
      "15/21 [====================>.........] - ETA: 0s - loss: 0.2355 - accuracy: 0.9125\n",
      "Epoch 336: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.2335 - accuracy: 0.9062 - val_loss: 1.7284 - val_accuracy: 0.6451\n",
      "Epoch 337/400\n",
      "16/21 [=====================>........] - ETA: 0s - loss: 0.2453 - accuracy: 0.8984\n",
      "Epoch 337: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.2422 - accuracy: 0.9003 - val_loss: 1.7381 - val_accuracy: 0.6562\n",
      "Epoch 338/400\n",
      "15/21 [====================>.........] - ETA: 0s - loss: 0.2148 - accuracy: 0.9125\n",
      "Epoch 338: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.2231 - accuracy: 0.9107 - val_loss: 1.8728 - val_accuracy: 0.6339\n",
      "Epoch 339/400\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 0.2024 - accuracy: 0.9219\n",
      "Epoch 339: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.2088 - accuracy: 0.9182 - val_loss: 1.7587 - val_accuracy: 0.6362\n",
      "Epoch 340/400\n",
      "16/21 [=====================>........] - ETA: 0s - loss: 0.2559 - accuracy: 0.8887\n",
      "Epoch 340: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.2434 - accuracy: 0.8929 - val_loss: 1.7428 - val_accuracy: 0.6429\n",
      "Epoch 341/400\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.2043 - accuracy: 0.9196\n",
      "Epoch 341: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.2043 - accuracy: 0.9196 - val_loss: 1.7791 - val_accuracy: 0.6339\n",
      "Epoch 342/400\n",
      "15/21 [====================>.........] - ETA: 0s - loss: 0.2592 - accuracy: 0.9104\n",
      "Epoch 342: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.2420 - accuracy: 0.9167 - val_loss: 1.7690 - val_accuracy: 0.6562\n",
      "Epoch 343/400\n",
      "15/21 [====================>.........] - ETA: 0s - loss: 0.2308 - accuracy: 0.9125\n",
      "Epoch 343: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.2136 - accuracy: 0.9211 - val_loss: 1.8355 - val_accuracy: 0.6362\n",
      "Epoch 344/400\n",
      "16/21 [=====================>........] - ETA: 0s - loss: 0.2398 - accuracy: 0.9023\n",
      "Epoch 344: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.2483 - accuracy: 0.8973 - val_loss: 1.7638 - val_accuracy: 0.6339\n",
      "Epoch 345/400\n",
      "17/21 [=======================>......] - ETA: 0s - loss: 0.2234 - accuracy: 0.9118\n",
      "Epoch 345: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.2266 - accuracy: 0.9107 - val_loss: 1.7263 - val_accuracy: 0.6362\n",
      "Epoch 346/400\n",
      "14/21 [===================>..........] - ETA: 0s - loss: 0.1953 - accuracy: 0.9196\n",
      "Epoch 346: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.1961 - accuracy: 0.9196 - val_loss: 1.8819 - val_accuracy: 0.6228\n",
      "Epoch 347/400\n",
      "16/21 [=====================>........] - ETA: 0s - loss: 0.2328 - accuracy: 0.9023\n",
      "Epoch 347: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.2490 - accuracy: 0.8988 - val_loss: 1.7362 - val_accuracy: 0.6295\n",
      "Epoch 348/400\n",
      "15/21 [====================>.........] - ETA: 0s - loss: 0.2431 - accuracy: 0.9021\n",
      "Epoch 348: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.2339 - accuracy: 0.9003 - val_loss: 1.7338 - val_accuracy: 0.6339\n",
      "Epoch 349/400\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.2079 - accuracy: 0.9256\n",
      "Epoch 349: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.2079 - accuracy: 0.9256 - val_loss: 1.7373 - val_accuracy: 0.6496\n",
      "Epoch 350/400\n",
      "15/21 [====================>.........] - ETA: 0s - loss: 0.1949 - accuracy: 0.9167\n",
      "Epoch 350: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.2123 - accuracy: 0.9048 - val_loss: 1.7547 - val_accuracy: 0.6429\n",
      "Epoch 351/400\n",
      "16/21 [=====================>........] - ETA: 0s - loss: 0.2476 - accuracy: 0.9023\n",
      "Epoch 351: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.2368 - accuracy: 0.9048 - val_loss: 1.7583 - val_accuracy: 0.6384\n",
      "Epoch 352/400\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.2252 - accuracy: 0.9077\n",
      "Epoch 352: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.2252 - accuracy: 0.9077 - val_loss: 1.8293 - val_accuracy: 0.6250\n",
      "Epoch 353/400\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.2448 - accuracy: 0.9092\n",
      "Epoch 353: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.2448 - accuracy: 0.9092 - val_loss: 1.7271 - val_accuracy: 0.6250\n",
      "Epoch 354/400\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.2278 - accuracy: 0.9092\n",
      "Epoch 354: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.2278 - accuracy: 0.9092 - val_loss: 1.7178 - val_accuracy: 0.6362\n",
      "Epoch 355/400\n",
      "15/21 [====================>.........] - ETA: 0s - loss: 0.1983 - accuracy: 0.9271\n",
      "Epoch 355: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.1930 - accuracy: 0.9241 - val_loss: 1.8066 - val_accuracy: 0.6250\n",
      "Epoch 356/400\n",
      "15/21 [====================>.........] - ETA: 0s - loss: 0.2364 - accuracy: 0.9062\n",
      "Epoch 356: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.2245 - accuracy: 0.9107 - val_loss: 1.8373 - val_accuracy: 0.6429\n",
      "Epoch 357/400\n",
      "16/21 [=====================>........] - ETA: 0s - loss: 0.2278 - accuracy: 0.9004\n",
      "Epoch 357: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.2254 - accuracy: 0.9033 - val_loss: 1.7607 - val_accuracy: 0.6339\n",
      "Epoch 358/400\n",
      "15/21 [====================>.........] - ETA: 0s - loss: 0.2017 - accuracy: 0.9125\n",
      "Epoch 358: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.2115 - accuracy: 0.9062 - val_loss: 1.8315 - val_accuracy: 0.6384\n",
      "Epoch 359/400\n",
      "15/21 [====================>.........] - ETA: 0s - loss: 0.2213 - accuracy: 0.9125\n",
      "Epoch 359: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.2153 - accuracy: 0.9122 - val_loss: 1.7859 - val_accuracy: 0.6317\n",
      "Epoch 360/400\n",
      "14/21 [===================>..........] - ETA: 0s - loss: 0.2255 - accuracy: 0.9040\n",
      "Epoch 360: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.2328 - accuracy: 0.8988 - val_loss: 1.8894 - val_accuracy: 0.6295\n",
      "Epoch 361/400\n",
      "16/21 [=====================>........] - ETA: 0s - loss: 0.2588 - accuracy: 0.8906\n",
      "Epoch 361: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.2522 - accuracy: 0.8943 - val_loss: 1.8540 - val_accuracy: 0.6295\n",
      "Epoch 362/400\n",
      "15/21 [====================>.........] - ETA: 0s - loss: 0.2017 - accuracy: 0.9187\n",
      "Epoch 362: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.2232 - accuracy: 0.9092 - val_loss: 1.7852 - val_accuracy: 0.6250\n",
      "Epoch 363/400\n",
      "15/21 [====================>.........] - ETA: 0s - loss: 0.2093 - accuracy: 0.9187\n",
      "Epoch 363: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.2070 - accuracy: 0.9182 - val_loss: 1.8726 - val_accuracy: 0.6272\n",
      "Epoch 364/400\n",
      "15/21 [====================>.........] - ETA: 0s - loss: 0.2662 - accuracy: 0.8833\n",
      "Epoch 364: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.2492 - accuracy: 0.8899 - val_loss: 1.7745 - val_accuracy: 0.6384\n",
      "Epoch 365/400\n",
      "15/21 [====================>.........] - ETA: 0s - loss: 0.2427 - accuracy: 0.9042\n",
      "Epoch 365: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.2259 - accuracy: 0.9077 - val_loss: 1.7761 - val_accuracy: 0.6518\n",
      "Epoch 366/400\n",
      "16/21 [=====================>........] - ETA: 0s - loss: 0.2194 - accuracy: 0.9102\n",
      "Epoch 366: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.2232 - accuracy: 0.9122 - val_loss: 1.7954 - val_accuracy: 0.6473\n",
      "Epoch 367/400\n",
      "16/21 [=====================>........] - ETA: 0s - loss: 0.2372 - accuracy: 0.9102\n",
      "Epoch 367: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.2321 - accuracy: 0.9137 - val_loss: 1.7728 - val_accuracy: 0.6518\n",
      "Epoch 368/400\n",
      "15/21 [====================>.........] - ETA: 0s - loss: 0.1967 - accuracy: 0.9062\n",
      "Epoch 368: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.2166 - accuracy: 0.9018 - val_loss: 1.7946 - val_accuracy: 0.6384\n",
      "Epoch 369/400\n",
      "17/21 [=======================>......] - ETA: 0s - loss: 0.1972 - accuracy: 0.9210\n",
      "Epoch 369: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.2080 - accuracy: 0.9122 - val_loss: 1.7956 - val_accuracy: 0.6384\n",
      "Epoch 370/400\n",
      "16/21 [=====================>........] - ETA: 0s - loss: 0.2164 - accuracy: 0.8984\n",
      "Epoch 370: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.2244 - accuracy: 0.9018 - val_loss: 1.8023 - val_accuracy: 0.6473\n",
      "Epoch 371/400\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.2064 - accuracy: 0.9178\n",
      "Epoch 371: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.2009 - accuracy: 0.9226 - val_loss: 1.7991 - val_accuracy: 0.6339\n",
      "Epoch 372/400\n",
      "17/21 [=======================>......] - ETA: 0s - loss: 0.2498 - accuracy: 0.9026\n",
      "Epoch 372: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.2435 - accuracy: 0.9048 - val_loss: 1.8211 - val_accuracy: 0.6205\n",
      "Epoch 373/400\n",
      "16/21 [=====================>........] - ETA: 0s - loss: 0.2261 - accuracy: 0.9199\n",
      "Epoch 373: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.2120 - accuracy: 0.9256 - val_loss: 1.7379 - val_accuracy: 0.6138\n",
      "Epoch 374/400\n",
      "15/21 [====================>.........] - ETA: 0s - loss: 0.2029 - accuracy: 0.9125\n",
      "Epoch 374: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.2015 - accuracy: 0.9196 - val_loss: 1.8626 - val_accuracy: 0.6429\n",
      "Epoch 375/400\n",
      "15/21 [====================>.........] - ETA: 0s - loss: 0.2166 - accuracy: 0.9125\n",
      "Epoch 375: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.2112 - accuracy: 0.9107 - val_loss: 1.8199 - val_accuracy: 0.6205\n",
      "Epoch 376/400\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.1959 - accuracy: 0.9152\n",
      "Epoch 376: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.1959 - accuracy: 0.9152 - val_loss: 1.8526 - val_accuracy: 0.6339\n",
      "Epoch 377/400\n",
      "16/21 [=====================>........] - ETA: 0s - loss: 0.1864 - accuracy: 0.9199\n",
      "Epoch 377: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.1905 - accuracy: 0.9167 - val_loss: 1.8183 - val_accuracy: 0.6362\n",
      "Epoch 378/400\n",
      "16/21 [=====================>........] - ETA: 0s - loss: 0.1815 - accuracy: 0.9258\n",
      "Epoch 378: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.1829 - accuracy: 0.9226 - val_loss: 1.8412 - val_accuracy: 0.6362\n",
      "Epoch 379/400\n",
      "14/21 [===================>..........] - ETA: 0s - loss: 0.2399 - accuracy: 0.9085\n",
      "Epoch 379: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.2320 - accuracy: 0.9137 - val_loss: 1.8186 - val_accuracy: 0.6585\n",
      "Epoch 380/400\n",
      "16/21 [=====================>........] - ETA: 0s - loss: 0.2246 - accuracy: 0.8984\n",
      "Epoch 380: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.2336 - accuracy: 0.9048 - val_loss: 1.8148 - val_accuracy: 0.6362\n",
      "Epoch 381/400\n",
      "16/21 [=====================>........] - ETA: 0s - loss: 0.2266 - accuracy: 0.9082\n",
      "Epoch 381: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.2161 - accuracy: 0.9122 - val_loss: 1.8427 - val_accuracy: 0.6451\n",
      "Epoch 382/400\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.1891 - accuracy: 0.9241\n",
      "Epoch 382: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.1891 - accuracy: 0.9241 - val_loss: 1.8675 - val_accuracy: 0.6384\n",
      "Epoch 383/400\n",
      "15/21 [====================>.........] - ETA: 0s - loss: 0.2160 - accuracy: 0.9042\n",
      "Epoch 383: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.2120 - accuracy: 0.9062 - val_loss: 1.8504 - val_accuracy: 0.6116\n",
      "Epoch 384/400\n",
      "15/21 [====================>.........] - ETA: 0s - loss: 0.1800 - accuracy: 0.9208\n",
      "Epoch 384: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.1815 - accuracy: 0.9256 - val_loss: 1.8655 - val_accuracy: 0.6362\n",
      "Epoch 385/400\n",
      "16/21 [=====================>........] - ETA: 0s - loss: 0.1647 - accuracy: 0.9434\n",
      "Epoch 385: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.1670 - accuracy: 0.9390 - val_loss: 1.9405 - val_accuracy: 0.6183\n",
      "Epoch 386/400\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.2121 - accuracy: 0.9077\n",
      "Epoch 386: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.2121 - accuracy: 0.9077 - val_loss: 1.8591 - val_accuracy: 0.6384\n",
      "Epoch 387/400\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 0.1933 - accuracy: 0.9187\n",
      "Epoch 387: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.1904 - accuracy: 0.9182 - val_loss: 1.8723 - val_accuracy: 0.6339\n",
      "Epoch 388/400\n",
      "16/21 [=====================>........] - ETA: 0s - loss: 0.2189 - accuracy: 0.9062\n",
      "Epoch 388: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.2192 - accuracy: 0.9092 - val_loss: 1.8958 - val_accuracy: 0.6250\n",
      "Epoch 389/400\n",
      "16/21 [=====================>........] - ETA: 0s - loss: 0.1959 - accuracy: 0.9219\n",
      "Epoch 389: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.2042 - accuracy: 0.9196 - val_loss: 1.9184 - val_accuracy: 0.6406\n",
      "Epoch 390/400\n",
      "15/21 [====================>.........] - ETA: 0s - loss: 0.1996 - accuracy: 0.9208\n",
      "Epoch 390: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.1992 - accuracy: 0.9211 - val_loss: 1.8669 - val_accuracy: 0.6473\n",
      "Epoch 391/400\n",
      "15/21 [====================>.........] - ETA: 0s - loss: 0.2106 - accuracy: 0.9083\n",
      "Epoch 391: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.1897 - accuracy: 0.9196 - val_loss: 1.9207 - val_accuracy: 0.6429\n",
      "Epoch 392/400\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.2297 - accuracy: 0.9092\n",
      "Epoch 392: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.2297 - accuracy: 0.9092 - val_loss: 1.8938 - val_accuracy: 0.6228\n",
      "Epoch 393/400\n",
      "16/21 [=====================>........] - ETA: 0s - loss: 0.2226 - accuracy: 0.9199\n",
      "Epoch 393: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.2147 - accuracy: 0.9196 - val_loss: 1.8367 - val_accuracy: 0.6384\n",
      "Epoch 394/400\n",
      "16/21 [=====================>........] - ETA: 0s - loss: 0.1973 - accuracy: 0.9199\n",
      "Epoch 394: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.2154 - accuracy: 0.9137 - val_loss: 1.8120 - val_accuracy: 0.6339\n",
      "Epoch 395/400\n",
      "15/21 [====================>.........] - ETA: 0s - loss: 0.2276 - accuracy: 0.9104\n",
      "Epoch 395: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.2265 - accuracy: 0.9092 - val_loss: 1.8671 - val_accuracy: 0.6272\n",
      "Epoch 396/400\n",
      "16/21 [=====================>........] - ETA: 0s - loss: 0.2333 - accuracy: 0.8965\n",
      "Epoch 396: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.2412 - accuracy: 0.8914 - val_loss: 1.8053 - val_accuracy: 0.6384\n",
      "Epoch 397/400\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 0.2214 - accuracy: 0.9141\n",
      "Epoch 397: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.2194 - accuracy: 0.9137 - val_loss: 1.7974 - val_accuracy: 0.6339\n",
      "Epoch 398/400\n",
      "15/21 [====================>.........] - ETA: 0s - loss: 0.1806 - accuracy: 0.9333\n",
      "Epoch 398: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.1893 - accuracy: 0.9241 - val_loss: 1.8968 - val_accuracy: 0.6272\n",
      "Epoch 399/400\n",
      "15/21 [====================>.........] - ETA: 0s - loss: 0.2041 - accuracy: 0.9229\n",
      "Epoch 399: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.1945 - accuracy: 0.9241 - val_loss: 1.8125 - val_accuracy: 0.6384\n",
      "Epoch 400/400\n",
      "16/21 [=====================>........] - ETA: 0s - loss: 0.2258 - accuracy: 0.9121\n",
      "Epoch 400: val_accuracy did not improve from 0.68080\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.2058 - accuracy: 0.9211 - val_loss: 1.8691 - val_accuracy: 0.6228\n",
      "9/9 [==============================] - 1s 4ms/step - loss: 1.1357 - accuracy: 0.6335\n",
      "Test loss: 1.135701298713684\n",
      "Test accuracy: 0.6334519386291504\n"
     ]
    }
   ],
   "source": [
    "best_model = create_model(\"RMSprop\", 3, 124, 0.10792333328469408,65,0.49489384404853165, 40, 0.05397184830120454)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.4, random_state=42)\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# Define a ModelCheckpoint callback to save the best model weights\n",
    "checkpoint = ModelCheckpoint(filepath='best_model_weights.h5', \n",
    "                              monitor='val_accuracy', \n",
    "                              save_best_only=True,\n",
    "                              mode='max',\n",
    "                              verbose=1)\n",
    "\n",
    "# Train the model with the callback\n",
    "history = best_model.fit(X_train, y_train,\n",
    "                    epochs=400, \n",
    "                    validation_data=(X_val, y_val),\n",
    "                    callbacks=[checkpoint])\n",
    "\n",
    "# After training, load the best weights\n",
    "best_model.load_weights('best_model_weights.h5')\n",
    "\n",
    "# Evaluate the model using the best weights\n",
    "loss, accuracy = best_model.evaluate(X_test, y_test)\n",
    "print(f'Test loss: {loss}')\n",
    "print(f'Test accuracy: {accuracy}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
