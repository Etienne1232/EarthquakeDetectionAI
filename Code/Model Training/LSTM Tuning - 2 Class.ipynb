{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import load, dump\n",
    "import numpy as np\n",
    "import tensorflow as tf \n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from tensorflow.keras.optimizers import SGD, Adam, RMSprop\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "import optuna\n",
    "\n",
    "def create_model(optimizer, num_lstm, lstm_1, dropout_1, lstm_2 = 0, drop_2 = 0, lstm_3 = 0, drop_3 = 0):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units=lstm_1, input_shape=(input_sh[1], input_sh[2]), return_sequences=True if num_lstm > 1 else False)) \n",
    "    model.add(Dropout(dropout_1))\n",
    "    \n",
    "\n",
    "    if num_lstm > 1:\n",
    "        model.add(LSTM(units=lstm_2, input_shape=(input_sh[1], input_sh[2]), return_sequences=True if num_lstm == 3 else False)) \n",
    "        model.add(Dropout(drop_2))\n",
    "    \n",
    "    if num_lstm > 2:\n",
    "            model.add(LSTM(units=lstm_3, input_shape=(input_sh[1], input_sh[2]), return_sequences= False)) \n",
    "            model.add(Dropout(drop_3))\n",
    "    \n",
    "    model.add(Dense(1, activation='sigmoid')) \n",
    "\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])   \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-24 20:52:41,746] A new study created in memory with name: no-name-89599ead-eb9d-4b6f-9520-894f282336b1\n",
      "[I 2024-05-24 20:52:57,173] Trial 0 finished with value: 0.7397663474082947 and parameters: {'num_lstm_layers': 1, 'lstm_units_1': 107, 'dropout_1': 0.013702874590588887, 'optimizer': 'SGD'}. Best is trial 0 with value: 0.7397663474082947.\n",
      "[I 2024-05-24 20:53:17,006] Trial 1 finished with value: 0.8619934320449829 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 119, 'dropout_1': 0.36537811871716036, 'optimizer': 'Adam', 'lstm_units_2': 76, 'dropout_2': 0.2518581599776218}. Best is trial 1 with value: 0.8619934320449829.\n",
      "[I 2024-05-24 20:53:28,677] Trial 2 finished with value: 0.7446148157119751 and parameters: {'num_lstm_layers': 1, 'lstm_units_1': 69, 'dropout_1': 0.18881614580152317, 'optimizer': 'Adam'}. Best is trial 1 with value: 0.8619934320449829.\n",
      "[I 2024-05-24 20:53:53,187] Trial 3 finished with value: 0.6949981689453125 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 77, 'dropout_1': 0.20122996411545035, 'optimizer': 'SGD', 'lstm_units_2': 27, 'dropout_2': 0.3887131866729754, 'lstm_units_3': 99, 'dropout_3': 0.45634552770592757}. Best is trial 1 with value: 0.8619934320449829.\n",
      "[I 2024-05-24 20:54:12,122] Trial 4 finished with value: 0.8366557121276855 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 57, 'dropout_1': 0.09939547146626404, 'optimizer': 'Adam', 'lstm_units_2': 110, 'dropout_2': 0.43305897820693007}. Best is trial 1 with value: 0.8619934320449829.\n",
      "[I 2024-05-24 20:54:32,537] Trial 5 finished with value: 0.8450383305549621 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 103, 'dropout_1': 0.08424602530511305, 'optimizer': 'RMSprop', 'lstm_units_2': 61, 'dropout_2': 0.4618055520180238}. Best is trial 1 with value: 0.8619934320449829.\n",
      "[I 2024-05-24 20:54:57,370] Trial 6 finished with value: 0.8305878043174744 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 83, 'dropout_1': 0.2563002134481844, 'optimizer': 'Adam', 'lstm_units_2': 98, 'dropout_2': 0.3114651757234616, 'lstm_units_3': 73, 'dropout_3': 0.3597336769426857}. Best is trial 1 with value: 0.8619934320449829.\n",
      "[I 2024-05-24 20:55:16,593] Trial 7 finished with value: 0.8293172597885132 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 55, 'dropout_1': 0.20978484141235731, 'optimizer': 'RMSprop', 'lstm_units_2': 50, 'dropout_2': 0.01785651469202104}. Best is trial 1 with value: 0.8619934320449829.\n",
      "[I 2024-05-24 20:55:34,092] Trial 8 finished with value: 0.6743629097938537 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 18, 'dropout_1': 0.037279131888717065, 'optimizer': 'SGD', 'lstm_units_2': 54, 'dropout_2': 0.09422100498400465}. Best is trial 1 with value: 0.8619934320449829.\n",
      "[I 2024-05-24 20:56:01,205] Trial 9 finished with value: 0.8825556755065918 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 124, 'dropout_1': 0.26049317871937405, 'optimizer': 'RMSprop', 'lstm_units_2': 117, 'dropout_2': 0.43475705326425884, 'lstm_units_3': 127, 'dropout_3': 0.032062268392180626}. Best is trial 9 with value: 0.8825556755065918.\n",
      "[I 2024-05-24 20:56:27,174] Trial 10 finished with value: 0.7663745880126953 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 30, 'dropout_1': 0.4788392534449135, 'optimizer': 'RMSprop', 'lstm_units_2': 121, 'dropout_2': 0.16004465016866457, 'lstm_units_3': 128, 'dropout_3': 0.062320558534863485}. Best is trial 9 with value: 0.8825556755065918.\n",
      "[I 2024-05-24 20:56:54,108] Trial 11 finished with value: 0.8838042974472046 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 126, 'dropout_1': 0.36807874859470574, 'optimizer': 'RMSprop', 'lstm_units_2': 89, 'dropout_2': 0.23461969111635889, 'lstm_units_3': 21, 'dropout_3': 0.011420368084475021}. Best is trial 11 with value: 0.8838042974472046.\n",
      "[I 2024-05-24 20:57:21,058] Trial 12 finished with value: 0.8559766292572022 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 127, 'dropout_1': 0.3474325271630733, 'optimizer': 'RMSprop', 'lstm_units_2': 88, 'dropout_2': 0.32563711507666326, 'lstm_units_3': 22, 'dropout_3': 0.0009016099232639699}. Best is trial 11 with value: 0.8838042974472046.\n",
      "[I 2024-05-24 20:57:47,678] Trial 13 finished with value: 0.8692661404609681 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 97, 'dropout_1': 0.35550376707870124, 'optimizer': 'RMSprop', 'lstm_units_2': 127, 'dropout_2': 0.1918600190649566, 'lstm_units_3': 17, 'dropout_3': 0.16816367662610596}. Best is trial 11 with value: 0.8838042974472046.\n",
      "[I 2024-05-24 20:58:14,602] Trial 14 finished with value: 0.8596349000930786 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 127, 'dropout_1': 0.28327966384835135, 'optimizer': 'RMSprop', 'lstm_units_2': 101, 'dropout_2': 0.3466208730465542, 'lstm_units_3': 51, 'dropout_3': 0.1400623173463053}. Best is trial 11 with value: 0.8838042974472046.\n",
      "[I 2024-05-24 20:58:41,403] Trial 15 finished with value: 0.8547718048095703 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 110, 'dropout_1': 0.46049438300877993, 'optimizer': 'RMSprop', 'lstm_units_2': 82, 'dropout_2': 0.2489908356359055, 'lstm_units_3': 124, 'dropout_3': 0.0018862769954691681}. Best is trial 11 with value: 0.8838042974472046.\n",
      "[I 2024-05-24 20:59:07,699] Trial 16 finished with value: 0.8657393217086792 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 93, 'dropout_1': 0.4042273849548121, 'optimizer': 'RMSprop', 'lstm_units_2': 112, 'dropout_2': 0.48691714445607465, 'lstm_units_3': 57, 'dropout_3': 0.226943557011491}. Best is trial 11 with value: 0.8838042974472046.\n",
      "[I 2024-05-24 20:59:20,544] Trial 17 finished with value: 0.7784884929656982 and parameters: {'num_lstm_layers': 1, 'lstm_units_1': 113, 'dropout_1': 0.30059890970728614, 'optimizer': 'RMSprop'}. Best is trial 11 with value: 0.8838042974472046.\n",
      "[I 2024-05-24 20:59:46,847] Trial 18 finished with value: 0.8559839248657226 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 88, 'dropout_1': 0.1574895156500056, 'optimizer': 'RMSprop', 'lstm_units_2': 93, 'dropout_2': 0.25338255617630256, 'lstm_units_3': 93, 'dropout_3': 0.09461064027915006}. Best is trial 11 with value: 0.8838042974472046.\n",
      "[I 2024-05-24 21:00:04,613] Trial 19 finished with value: 0.7990945458412171 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 120, 'dropout_1': 0.42114138002969714, 'optimizer': 'SGD', 'lstm_units_2': 68, 'dropout_2': 0.10124824464736404}. Best is trial 11 with value: 0.8838042974472046.\n",
      "[I 2024-05-24 21:00:24,092] Trial 20 finished with value: 0.8148229241371154 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 45, 'dropout_1': 0.30358387222840366, 'optimizer': 'RMSprop', 'lstm_units_2': 110, 'dropout_2': 0.40231705591101674}. Best is trial 11 with value: 0.8838042974472046.\n",
      "[I 2024-05-24 21:00:50,426] Trial 21 finished with value: 0.8620518445968628 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 96, 'dropout_1': 0.35722373802294305, 'optimizer': 'RMSprop', 'lstm_units_2': 127, 'dropout_2': 0.18335229263603978, 'lstm_units_3': 17, 'dropout_3': 0.1809470244789616}. Best is trial 11 with value: 0.8838042974472046.\n",
      "[I 2024-05-24 21:01:17,130] Trial 22 finished with value: 0.8281197428703309 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 102, 'dropout_1': 0.41278960394756153, 'optimizer': 'RMSprop', 'lstm_units_2': 127, 'dropout_2': 0.19484988338341086, 'lstm_units_3': 34, 'dropout_3': 0.2961767510902371}. Best is trial 11 with value: 0.8838042974472046.\n",
      "[I 2024-05-24 21:01:43,798] Trial 23 finished with value: 0.8729536294937134 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 118, 'dropout_1': 0.31807652024832794, 'optimizer': 'RMSprop', 'lstm_units_2': 116, 'dropout_2': 0.13309285133939308, 'lstm_units_3': 37, 'dropout_3': 0.1004203823556303}. Best is trial 11 with value: 0.8838042974472046.\n",
      "[I 2024-05-24 21:02:10,665] Trial 24 finished with value: 0.8741219401359558 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 119, 'dropout_1': 0.26321185875852177, 'optimizer': 'RMSprop', 'lstm_units_2': 104, 'dropout_2': 0.10603831543744041, 'lstm_units_3': 39, 'dropout_3': 0.06691148869496441}. Best is trial 11 with value: 0.8838042974472046.\n",
      "[I 2024-05-24 21:02:37,593] Trial 25 finished with value: 0.8910405158996582 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 123, 'dropout_1': 0.25143629516760596, 'optimizer': 'RMSprop', 'lstm_units_2': 103, 'dropout_2': 0.008885232071972143, 'lstm_units_3': 40, 'dropout_3': 0.04662800153175649}. Best is trial 25 with value: 0.8910405158996582.\n",
      "[I 2024-05-24 21:03:04,437] Trial 26 finished with value: 0.8644176721572876 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 128, 'dropout_1': 0.15061940300526988, 'optimizer': 'RMSprop', 'lstm_units_2': 88, 'dropout_2': 0.0012719030237719475, 'lstm_units_3': 70, 'dropout_3': 0.0003964431406743296}. Best is trial 25 with value: 0.8910405158996582.\n",
      "[I 2024-05-24 21:03:22,508] Trial 27 finished with value: 0.7954289793968201 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 111, 'dropout_1': 0.23358391580440782, 'optimizer': 'SGD', 'lstm_units_2': 77, 'dropout_2': 0.03791561149750704}. Best is trial 25 with value: 0.8910405158996582.\n",
      "[I 2024-05-24 21:03:47,299] Trial 28 finished with value: 0.8777436971664428 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 128, 'dropout_1': 0.23423650397664839, 'optimizer': 'Adam', 'lstm_units_2': 103, 'dropout_2': 0.2861309699310504, 'lstm_units_3': 109, 'dropout_3': 0.05508792490801526}. Best is trial 25 with value: 0.8910405158996582.\n",
      "[I 2024-05-24 21:03:59,982] Trial 29 finished with value: 0.7736838221549988 and parameters: {'num_lstm_layers': 1, 'lstm_units_1': 106, 'dropout_1': 0.15004179482537017, 'optimizer': 'RMSprop'}. Best is trial 25 with value: 0.8910405158996582.\n",
      "[I 2024-05-24 21:04:23,991] Trial 30 finished with value: 0.6828331470489502 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 113, 'dropout_1': 0.38692714520024973, 'optimizer': 'SGD', 'lstm_units_2': 93, 'dropout_2': 0.06732826520751833, 'lstm_units_3': 52, 'dropout_3': 0.12421355688275516}. Best is trial 25 with value: 0.8910405158996582.\n",
      "[I 2024-05-24 21:04:49,114] Trial 31 finished with value: 0.8596422076225281 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 122, 'dropout_1': 0.23499820844774472, 'optimizer': 'Adam', 'lstm_units_2': 103, 'dropout_2': 0.35309615469960354, 'lstm_units_3': 111, 'dropout_3': 0.049091134069398704}. Best is trial 25 with value: 0.8910405158996582.\n",
      "[I 2024-05-24 21:05:14,135] Trial 32 finished with value: 0.8668638229370117 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 127, 'dropout_1': 0.3206388281687558, 'optimizer': 'Adam', 'lstm_units_2': 106, 'dropout_2': 0.38935225365785125, 'lstm_units_3': 112, 'dropout_3': 0.04285933673799847}. Best is trial 25 with value: 0.8910405158996582.\n",
      "[I 2024-05-24 21:05:38,935] Trial 33 finished with value: 0.8499160170555115 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 116, 'dropout_1': 0.27445458695490377, 'optimizer': 'Adam', 'lstm_units_2': 97, 'dropout_2': 0.29014123711676254, 'lstm_units_3': 114, 'dropout_3': 0.04417775746715982}. Best is trial 25 with value: 0.8910405158996582.\n",
      "[I 2024-05-24 21:06:04,093] Trial 34 finished with value: 0.8789850473403931 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 121, 'dropout_1': 0.18055730812745008, 'optimizer': 'Adam', 'lstm_units_2': 118, 'dropout_2': 0.271345207788143, 'lstm_units_3': 89, 'dropout_3': 0.09566104599280875}. Best is trial 25 with value: 0.8910405158996582.\n",
      "[I 2024-05-24 21:06:22,748] Trial 35 finished with value: 0.8693026781082154 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 106, 'dropout_1': 0.19617335323135632, 'optimizer': 'Adam', 'lstm_units_2': 118, 'dropout_2': 0.22832977021629808}. Best is trial 25 with value: 0.8910405158996582.\n",
      "[I 2024-05-24 21:06:46,591] Trial 36 finished with value: 0.8608543276786804 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 69, 'dropout_1': 0.1122477097147288, 'optimizer': 'Adam', 'lstm_units_2': 119, 'dropout_2': 0.49924023281796615, 'lstm_units_3': 68, 'dropout_3': 0.10426208981615585}. Best is trial 25 with value: 0.8910405158996582.\n",
      "[I 2024-05-24 21:07:04,487] Trial 37 finished with value: 0.8487039089202881 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 77, 'dropout_1': 0.16747329559539817, 'optimizer': 'Adam', 'lstm_units_2': 16, 'dropout_2': 0.427465190134296}. Best is trial 25 with value: 0.8910405158996582.\n",
      "[I 2024-05-24 21:07:31,219] Trial 38 finished with value: 0.8862066388130188 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 122, 'dropout_1': 0.05235678848863978, 'optimizer': 'RMSprop', 'lstm_units_2': 81, 'dropout_2': 0.36563836746642675, 'lstm_units_3': 92, 'dropout_3': 0.20238801888451435}. Best is trial 25 with value: 0.8910405158996582.\n",
      "[I 2024-05-24 21:07:57,576] Trial 39 finished with value: 0.8753632664680481 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 102, 'dropout_1': 0.014818722625346031, 'optimizer': 'RMSprop', 'lstm_units_2': 72, 'dropout_2': 0.4455337457017718, 'lstm_units_3': 80, 'dropout_3': 0.2723040819162226}. Best is trial 25 with value: 0.8910405158996582.\n",
      "[I 2024-05-24 21:08:17,632] Trial 40 finished with value: 0.883840823173523 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 115, 'dropout_1': 0.06535931386127346, 'optimizer': 'RMSprop', 'lstm_units_2': 41, 'dropout_2': 0.36005767610586803}. Best is trial 25 with value: 0.8910405158996582.\n",
      "[I 2024-05-24 21:08:37,690] Trial 41 finished with value: 0.8826067924499512 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 122, 'dropout_1': 0.052383543857265694, 'optimizer': 'RMSprop', 'lstm_units_2': 30, 'dropout_2': 0.3638862274917587}. Best is trial 25 with value: 0.8910405158996582.\n",
      "[I 2024-05-24 21:08:57,593] Trial 42 finished with value: 0.8741000413894653 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 115, 'dropout_1': 0.08222930991291771, 'optimizer': 'RMSprop', 'lstm_units_2': 41, 'dropout_2': 0.36503719318187583}. Best is trial 25 with value: 0.8910405158996582.\n",
      "[I 2024-05-24 21:09:17,956] Trial 43 finished with value: 0.8559328198432923 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 122, 'dropout_1': 0.05848608588790738, 'optimizer': 'RMSprop', 'lstm_units_2': 36, 'dropout_2': 0.31264959901913636}. Best is trial 25 with value: 0.8910405158996582.\n",
      "[I 2024-05-24 21:09:37,731] Trial 44 finished with value: 0.8644249677658081 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 110, 'dropout_1': 0.005720089920775974, 'optimizer': 'RMSprop', 'lstm_units_2': 26, 'dropout_2': 0.40979790492734347}. Best is trial 25 with value: 0.8910405158996582.\n",
      "[I 2024-05-24 21:09:50,924] Trial 45 finished with value: 0.7760861635208129 and parameters: {'num_lstm_layers': 1, 'lstm_units_1': 115, 'dropout_1': 0.04930648917419245, 'optimizer': 'RMSprop'}. Best is trial 25 with value: 0.8910405158996582.\n",
      "[I 2024-05-24 21:10:09,869] Trial 46 finished with value: 0.8112376928329468 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 53, 'dropout_1': 0.1245869333146145, 'optimizer': 'RMSprop', 'lstm_units_2': 56, 'dropout_2': 0.36565941964212845}. Best is trial 25 with value: 0.8910405158996582.\n",
      "[I 2024-05-24 21:10:29,054] Trial 47 finished with value: 0.8547499179840088 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 123, 'dropout_1': 0.0782182645140611, 'optimizer': 'RMSprop', 'lstm_units_2': 38, 'dropout_2': 0.3299481220555899}. Best is trial 25 with value: 0.8910405158996582.\n",
      "[I 2024-05-24 21:10:47,935] Trial 48 finished with value: 0.8596933245658874 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 99, 'dropout_1': 0.030421291652744275, 'optimizer': 'RMSprop', 'lstm_units_2': 64, 'dropout_2': 0.4725701466749262}. Best is trial 25 with value: 0.8910405158996582.\n",
      "[I 2024-05-24 21:11:04,719] Trial 49 finished with value: 0.7602701663970948 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 90, 'dropout_1': 0.12248372633874544, 'optimizer': 'SGD', 'lstm_units_2': 80, 'dropout_2': 0.3793618512095475}. Best is trial 25 with value: 0.8910405158996582.\n",
      "[I 2024-05-24 21:11:23,798] Trial 50 finished with value: 0.8644760966300964 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 106, 'dropout_1': 0.437747252062879, 'optimizer': 'RMSprop', 'lstm_units_2': 29, 'dropout_2': 0.33816519329844646}. Best is trial 25 with value: 0.8910405158996582.\n",
      "[I 2024-05-24 21:11:49,458] Trial 51 finished with value: 0.8729025244712829 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 124, 'dropout_1': 0.21436630255947486, 'optimizer': 'RMSprop', 'lstm_units_2': 85, 'dropout_2': 0.4429479778409063, 'lstm_units_3': 26, 'dropout_3': 0.21758359720159207}. Best is trial 25 with value: 0.8910405158996582.\n",
      "[I 2024-05-24 21:12:15,130] Trial 52 finished with value: 0.8910624384880066 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 117, 'dropout_1': 0.06561508163158411, 'optimizer': 'RMSprop', 'lstm_units_2': 17, 'dropout_2': 0.41337262419782944, 'lstm_units_3': 30, 'dropout_3': 0.3692532736959073}. Best is trial 52 with value: 0.8910624384880066.\n",
      "[I 2024-05-24 21:12:40,742] Trial 53 finished with value: 0.8777217864990234 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 118, 'dropout_1': 0.06542822928122113, 'optimizer': 'RMSprop', 'lstm_units_2': 18, 'dropout_2': 0.4151524760862028, 'lstm_units_3': 31, 'dropout_3': 0.3672729248174434}. Best is trial 52 with value: 0.8910624384880066.\n",
      "[I 2024-05-24 21:12:59,782] Trial 54 finished with value: 0.8826213955879212 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 110, 'dropout_1': 0.02458563040684217, 'optimizer': 'RMSprop', 'lstm_units_2': 23, 'dropout_2': 0.37986513327026245}. Best is trial 52 with value: 0.8910624384880066.\n",
      "[I 2024-05-24 21:13:25,177] Trial 55 finished with value: 0.8450675487518311 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 108, 'dropout_1': 0.0274017110258786, 'optimizer': 'RMSprop', 'lstm_units_2': 23, 'dropout_2': 0.4589413487489101, 'lstm_units_3': 44, 'dropout_3': 0.44416683829024073}. Best is trial 52 with value: 0.8910624384880066.\n",
      "[I 2024-05-24 21:13:36,683] Trial 56 finished with value: 0.75786052942276 and parameters: {'num_lstm_layers': 1, 'lstm_units_1': 62, 'dropout_1': 0.004298966736837717, 'optimizer': 'RMSprop'}. Best is trial 52 with value: 0.8910624384880066.\n",
      "[I 2024-05-24 21:14:02,499] Trial 57 finished with value: 0.7469660401344299 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 16, 'dropout_1': 0.09563366159078288, 'optimizer': 'RMSprop', 'lstm_units_2': 46, 'dropout_2': 0.3956275916287887, 'lstm_units_3': 27, 'dropout_3': 0.32877867807958555}. Best is trial 52 with value: 0.8910624384880066.\n",
      "[I 2024-05-24 21:14:21,140] Trial 58 finished with value: 0.8124351978302002 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 31, 'dropout_1': 0.03395545770497917, 'optimizer': 'RMSprop', 'lstm_units_2': 22, 'dropout_2': 0.22310748711529207}. Best is trial 52 with value: 0.8910624384880066.\n",
      "[I 2024-05-24 21:14:46,791] Trial 59 finished with value: 0.843950343132019 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 112, 'dropout_1': 0.48914001106163013, 'optimizer': 'RMSprop', 'lstm_units_2': 72, 'dropout_2': 0.38374734025729274, 'lstm_units_3': 45, 'dropout_3': 0.48902248959285965}. Best is trial 52 with value: 0.8910624384880066.\n",
      "[I 2024-05-24 21:15:09,767] Trial 60 finished with value: 0.6791675686836243 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 118, 'dropout_1': 0.3765437204015618, 'optimizer': 'SGD', 'lstm_units_2': 33, 'dropout_2': 0.3060825864053526, 'lstm_units_3': 100, 'dropout_3': 0.4180545050750804}. Best is trial 52 with value: 0.8910624384880066.\n",
      "[I 2024-05-24 21:15:29,110] Trial 61 finished with value: 0.8680686354637146 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 124, 'dropout_1': 0.0517799024844057, 'optimizer': 'RMSprop', 'lstm_units_2': 28, 'dropout_2': 0.3706098688199386}. Best is trial 52 with value: 0.8910624384880066.\n",
      "[I 2024-05-24 21:15:48,711] Trial 62 finished with value: 0.8620664477348328 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 125, 'dropout_1': 0.0728483712040574, 'optimizer': 'RMSprop', 'lstm_units_2': 31, 'dropout_2': 0.34933416535970524}. Best is trial 52 with value: 0.8910624384880066.\n",
      "[I 2024-05-24 21:16:07,832] Trial 63 finished with value: 0.8668711066246033 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 115, 'dropout_1': 0.09848331255710836, 'optimizer': 'RMSprop', 'lstm_units_2': 19, 'dropout_2': 0.42090990421735536}. Best is trial 52 with value: 0.8910624384880066.\n",
      "[I 2024-05-24 21:16:27,136] Trial 64 finished with value: 0.8680613398551941 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 119, 'dropout_1': 0.04997964687551898, 'optimizer': 'RMSprop', 'lstm_units_2': 44, 'dropout_2': 0.15428846314350317}. Best is trial 52 with value: 0.8910624384880066.\n",
      "[I 2024-05-24 21:16:46,361] Trial 65 finished with value: 0.8705074906349182 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 128, 'dropout_1': 0.019619333488201593, 'optimizer': 'RMSprop', 'lstm_units_2': 23, 'dropout_2': 0.3179228388330573}. Best is trial 52 with value: 0.8910624384880066.\n",
      "[I 2024-05-24 21:17:12,085] Trial 66 finished with value: 0.8559474110603332 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 110, 'dropout_1': 0.1332746426802689, 'optimizer': 'RMSprop', 'lstm_units_2': 91, 'dropout_2': 0.3987851283500128, 'lstm_units_3': 62, 'dropout_3': 0.39197565600105866}. Best is trial 52 with value: 0.8910624384880066.\n",
      "[I 2024-05-24 21:17:31,250] Trial 67 finished with value: 0.8777510046958923 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 103, 'dropout_1': 0.09074198739625775, 'optimizer': 'RMSprop', 'lstm_units_2': 34, 'dropout_2': 0.3581976092611453}. Best is trial 52 with value: 0.8910624384880066.\n",
      "[I 2024-05-24 21:17:56,254] Trial 68 finished with value: 0.8402117609977722 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 83, 'dropout_1': 0.34127366832705475, 'optimizer': 'RMSprop', 'lstm_units_2': 53, 'dropout_2': 0.03121039977664579, 'lstm_units_3': 81, 'dropout_3': 0.3046439942845171}. Best is trial 52 with value: 0.8910624384880066.\n",
      "[I 2024-05-24 21:18:22,047] Trial 69 finished with value: 0.8632712602615357 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 120, 'dropout_1': 0.06752371294972788, 'optimizer': 'RMSprop', 'lstm_units_2': 96, 'dropout_2': 0.29905278868606433, 'lstm_units_3': 23, 'dropout_3': 0.20066238857934315}. Best is trial 52 with value: 0.8910624384880066.\n",
      "[I 2024-05-24 21:18:38,826] Trial 70 finished with value: 0.8196129918098449 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 98, 'dropout_1': 0.1111162525470843, 'optimizer': 'SGD', 'lstm_units_2': 84, 'dropout_2': 0.3370549038462878}. Best is trial 52 with value: 0.8910624384880066.\n",
      "[I 2024-05-24 21:19:04,714] Trial 71 finished with value: 0.8486819982528686 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 125, 'dropout_1': 0.252280073237239, 'optimizer': 'RMSprop', 'lstm_units_2': 112, 'dropout_2': 0.43443024182160345, 'lstm_units_3': 119, 'dropout_3': 0.026440537851009752}. Best is trial 52 with value: 0.8910624384880066.\n",
      "[I 2024-05-24 21:19:30,487] Trial 72 finished with value: 0.858357059955597 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 116, 'dropout_1': 0.2930152764923325, 'optimizer': 'RMSprop', 'lstm_units_2': 124, 'dropout_2': 0.08085050651076836, 'lstm_units_3': 102, 'dropout_3': 0.16071043896400186}. Best is trial 52 with value: 0.8910624384880066.\n",
      "[I 2024-05-24 21:19:56,103] Trial 73 finished with value: 0.8693026781082154 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 121, 'dropout_1': 0.04338880913629893, 'optimizer': 'RMSprop', 'lstm_units_2': 99, 'dropout_2': 0.4095675068955207, 'lstm_units_3': 16, 'dropout_3': 0.07130303201120705}. Best is trial 52 with value: 0.8910624384880066.\n",
      "[I 2024-05-24 21:20:21,906] Trial 74 finished with value: 0.8656882047653198 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 113, 'dropout_1': 0.26948640976148347, 'optimizer': 'RMSprop', 'lstm_units_2': 77, 'dropout_2': 0.4656904216716035, 'lstm_units_3': 30, 'dropout_3': 0.350218397299356}. Best is trial 52 with value: 0.8910624384880066.\n",
      "[I 2024-05-24 21:20:47,687] Trial 75 finished with value: 0.8741000533103943 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 128, 'dropout_1': 0.21284766859354923, 'optimizer': 'RMSprop', 'lstm_units_2': 113, 'dropout_2': 0.26720319531149705, 'lstm_units_3': 128, 'dropout_3': 0.13461140477050088}. Best is trial 52 with value: 0.8910624384880066.\n",
      "[I 2024-05-24 21:21:13,649] Trial 76 finished with value: 0.8317415118217468 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 125, 'dropout_1': 0.32711546563215155, 'optimizer': 'RMSprop', 'lstm_units_2': 108, 'dropout_2': 0.4541701326198795, 'lstm_units_3': 44, 'dropout_3': 0.00762536834880009}. Best is trial 52 with value: 0.8910624384880066.\n",
      "[I 2024-05-24 21:21:39,089] Trial 77 finished with value: 0.8366265058517456 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 108, 'dropout_1': 0.44812121676955835, 'optimizer': 'RMSprop', 'lstm_units_2': 26, 'dropout_2': 0.37742685089579997, 'lstm_units_3': 21, 'dropout_3': 0.24156180002389965}. Best is trial 52 with value: 0.8910624384880066.\n",
      "[I 2024-05-24 21:21:58,429] Trial 78 finished with value: 0.8873968720436096 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 117, 'dropout_1': 0.22809521294318588, 'optimizer': 'RMSprop', 'lstm_units_2': 89, 'dropout_2': 0.12057442472344877}. Best is trial 52 with value: 0.8910624384880066.\n",
      "[I 2024-05-24 21:22:18,082] Trial 79 finished with value: 0.8547864198684693 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 117, 'dropout_1': 0.03958323096961848, 'optimizer': 'RMSprop', 'lstm_units_2': 89, 'dropout_2': 0.13339295069407953}. Best is trial 52 with value: 0.8910624384880066.\n",
      "[I 2024-05-24 21:22:37,121] Trial 80 finished with value: 0.8571449398994446 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 103, 'dropout_1': 0.177536560469014, 'optimizer': 'RMSprop', 'lstm_units_2': 94, 'dropout_2': 0.06833308561385601}. Best is trial 52 with value: 0.8910624384880066.\n",
      "[I 2024-05-24 21:22:56,839] Trial 81 finished with value: 0.8584008693695069 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 122, 'dropout_1': 0.24373398857104356, 'optimizer': 'RMSprop', 'lstm_units_2': 81, 'dropout_2': 0.2175406679077516}. Best is trial 52 with value: 0.8910624384880066.\n",
      "[I 2024-05-24 21:23:16,071] Trial 82 finished with value: 0.8765169739723205 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 113, 'dropout_1': 0.28534251107452213, 'optimizer': 'RMSprop', 'lstm_units_2': 67, 'dropout_2': 0.17926857757862735}. Best is trial 52 with value: 0.8910624384880066.\n",
      "[I 2024-05-24 21:23:35,533] Trial 83 finished with value: 0.8910916328430176 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 120, 'dropout_1': 0.3955258546924265, 'optimizer': 'RMSprop', 'lstm_units_2': 86, 'dropout_2': 0.114195676174256}. Best is trial 83 with value: 0.8910916328430176.\n",
      "[I 2024-05-24 21:23:54,915] Trial 84 finished with value: 0.8861847281455993 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 120, 'dropout_1': 0.390361501186487, 'optimizer': 'RMSprop', 'lstm_units_2': 86, 'dropout_2': 0.04856108053278621}. Best is trial 83 with value: 0.8910916328430176.\n",
      "[I 2024-05-24 21:24:14,179] Trial 85 finished with value: 0.8680540204048157 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 119, 'dropout_1': 0.39637198965452797, 'optimizer': 'RMSprop', 'lstm_units_2': 84, 'dropout_2': 0.04521345848209729}. Best is trial 83 with value: 0.8910916328430176.\n",
      "[I 2024-05-24 21:24:33,487] Trial 86 finished with value: 0.853545081615448 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 114, 'dropout_1': 0.4210640551261713, 'optimizer': 'RMSprop', 'lstm_units_2': 87, 'dropout_2': 0.11942585725726446}. Best is trial 83 with value: 0.8910916328430176.\n",
      "[I 2024-05-24 21:24:50,389] Trial 87 finished with value: 0.8160350441932678 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 110, 'dropout_1': 0.38664862979502773, 'optimizer': 'SGD', 'lstm_units_2': 75, 'dropout_2': 0.02419527775423939}. Best is trial 83 with value: 0.8910916328430176.\n",
      "[I 2024-05-24 21:25:09,916] Trial 88 finished with value: 0.8692880630493164 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 117, 'dropout_1': 0.34795924428649583, 'optimizer': 'RMSprop', 'lstm_units_2': 90, 'dropout_2': 0.05792632114455487}. Best is trial 83 with value: 0.8910916328430176.\n",
      "[I 2024-05-24 21:25:29,309] Trial 89 finished with value: 0.8632347464561463 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 125, 'dropout_1': 0.367268249146682, 'optimizer': 'RMSprop', 'lstm_units_2': 79, 'dropout_2': 0.0874517505628942}. Best is trial 83 with value: 0.8910916328430176.\n",
      "[I 2024-05-24 21:25:48,517] Trial 90 finished with value: 0.8716903924942017 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 105, 'dropout_1': 0.4285925537542682, 'optimizer': 'RMSprop', 'lstm_units_2': 101, 'dropout_2': 0.003293052131784915}. Best is trial 83 with value: 0.8910916328430176.\n",
      "[I 2024-05-24 21:26:08,201] Trial 91 finished with value: 0.8729171276092529 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 121, 'dropout_1': 0.4656434979811387, 'optimizer': 'RMSprop', 'lstm_units_2': 93, 'dropout_2': 0.014711084373738609}. Best is trial 83 with value: 0.8910916328430176.\n",
      "[I 2024-05-24 21:26:27,310] Trial 92 finished with value: 0.8657101035118103 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 123, 'dropout_1': 0.019475564941363828, 'optimizer': 'RMSprop', 'lstm_units_2': 16, 'dropout_2': 0.045139999863921795}. Best is trial 83 with value: 0.8910916328430176.\n",
      "[I 2024-05-24 21:26:46,568] Trial 93 finished with value: 0.8765607953071595 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 120, 'dropout_1': 0.405370638584857, 'optimizer': 'RMSprop', 'lstm_units_2': 20, 'dropout_2': 0.1591901375258398}. Best is trial 83 with value: 0.8910916328430176.\n",
      "[I 2024-05-24 21:27:05,673] Trial 94 finished with value: 0.8607740044593811 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 116, 'dropout_1': 0.08407790556677294, 'optimizer': 'RMSprop', 'lstm_units_2': 82, 'dropout_2': 0.14212784912038268}. Best is trial 83 with value: 0.8910916328430176.\n",
      "[I 2024-05-24 21:27:25,309] Trial 95 finished with value: 0.8717342138290405 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 126, 'dropout_1': 0.3702523141892921, 'optimizer': 'RMSprop', 'lstm_units_2': 86, 'dropout_2': 0.10509464220563755}. Best is trial 83 with value: 0.8910916328430176.\n",
      "[I 2024-05-24 21:27:42,976] Trial 96 finished with value: 0.8584008812904358 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 112, 'dropout_1': 0.05857824247168459, 'optimizer': 'Adam', 'lstm_units_2': 74, 'dropout_2': 0.39040999281235017}. Best is trial 83 with value: 0.8910916328430176.\n",
      "[I 2024-05-24 21:27:55,824] Trial 97 finished with value: 0.7905878186225891 and parameters: {'num_lstm_layers': 1, 'lstm_units_1': 123, 'dropout_1': 0.3940272962888076, 'optimizer': 'RMSprop'}. Best is trial 83 with value: 0.8910916328430176.\n",
      "[I 2024-05-24 21:28:14,880] Trial 98 finished with value: 0.8644030570983887 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 128, 'dropout_1': 0.30708568115743895, 'optimizer': 'RMSprop', 'lstm_units_2': 39, 'dropout_2': 0.0570129895939344}. Best is trial 83 with value: 0.8910916328430176.\n",
      "[I 2024-05-24 21:28:34,524] Trial 99 finished with value: 0.8717414975166321 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 118, 'dropout_1': 0.10833546608201833, 'optimizer': 'RMSprop', 'lstm_units_2': 70, 'dropout_2': 0.24231259240730435}. Best is trial 83 with value: 0.8910916328430176.\n",
      "[I 2024-05-24 21:28:53,618] Trial 100 finished with value: 0.8547280073165894 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 108, 'dropout_1': 0.00044467735204072045, 'optimizer': 'RMSprop', 'lstm_units_2': 31, 'dropout_2': 0.18036855663740042}. Best is trial 83 with value: 0.8910916328430176.\n",
      "[I 2024-05-24 21:29:19,643] Trial 101 finished with value: 0.8837678074836731 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 122, 'dropout_1': 0.22263407498350862, 'optimizer': 'RMSprop', 'lstm_units_2': 105, 'dropout_2': 0.4258860012259777, 'lstm_units_3': 56, 'dropout_3': 0.2657019039407424}. Best is trial 83 with value: 0.8910916328430176.\n",
      "[I 2024-05-24 21:29:45,818] Trial 102 finished with value: 0.853501272201538 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 120, 'dropout_1': 0.22122400405223688, 'optimizer': 'RMSprop', 'lstm_units_2': 99, 'dropout_2': 0.4045838463853456, 'lstm_units_3': 57, 'dropout_3': 0.27790068269488244}. Best is trial 83 with value: 0.8910916328430176.\n",
      "[I 2024-05-24 21:30:11,749] Trial 103 finished with value: 0.860861623287201 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 115, 'dropout_1': 0.33372870946213773, 'optimizer': 'RMSprop', 'lstm_units_2': 105, 'dropout_2': 0.4247065290168666, 'lstm_units_3': 40, 'dropout_3': 0.3376846590021323}. Best is trial 83 with value: 0.8910916328430176.\n",
      "[I 2024-05-24 21:30:37,789] Trial 104 finished with value: 0.8729682326316833 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 122, 'dropout_1': 0.19593889125561575, 'optimizer': 'RMSprop', 'lstm_units_2': 25, 'dropout_2': 0.20047956231865827, 'lstm_units_3': 54, 'dropout_3': 0.3809594039334713}. Best is trial 83 with value: 0.8910916328430176.\n",
      "[I 2024-05-24 21:30:57,428] Trial 105 finished with value: 0.8752975583076477 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 126, 'dropout_1': 0.029135287818050346, 'optimizer': 'RMSprop', 'lstm_units_2': 78, 'dropout_2': 0.3616471330907578}. Best is trial 83 with value: 0.8910916328430176.\n",
      "[I 2024-05-24 21:31:20,502] Trial 106 finished with value: 0.7228331565856934 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 118, 'dropout_1': 0.06779236570599506, 'optimizer': 'SGD', 'lstm_units_2': 96, 'dropout_2': 0.34260530255366073, 'lstm_units_3': 65, 'dropout_3': 0.18508640969287676}. Best is trial 83 with value: 0.8910916328430176.\n",
      "[I 2024-05-24 21:31:39,521] Trial 107 finished with value: 0.8583716630935669 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 110, 'dropout_1': 0.3797915881855666, 'optimizer': 'RMSprop', 'lstm_units_2': 83, 'dropout_2': 0.3220168958312991}. Best is trial 83 with value: 0.8910916328430176.\n",
      "[I 2024-05-24 21:32:05,654] Trial 108 finished with value: 0.8826725125312805 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 123, 'dropout_1': 0.22267152426051892, 'optimizer': 'RMSprop', 'lstm_units_2': 91, 'dropout_2': 0.3801837147942022, 'lstm_units_3': 34, 'dropout_3': 0.2568090823467695}. Best is trial 83 with value: 0.8910916328430176.\n",
      "[I 2024-05-24 21:32:31,668] Trial 109 finished with value: 0.8378386259078979 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 124, 'dropout_1': 0.2252072619769161, 'optimizer': 'RMSprop', 'lstm_units_2': 92, 'dropout_2': 0.011680594607023732, 'lstm_units_3': 34, 'dropout_3': 0.27057875345843363}. Best is trial 83 with value: 0.8910916328430176.\n",
      "[I 2024-05-24 21:32:54,970] Trial 110 finished with value: 0.8463161706924438 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 94, 'dropout_1': 0.20314212227386877, 'optimizer': 'Adam', 'lstm_units_2': 88, 'dropout_2': 0.4426274997136191, 'lstm_units_3': 47, 'dropout_3': 0.23907374586268562}. Best is trial 83 with value: 0.8910916328430176.\n",
      "[I 2024-05-24 21:33:20,427] Trial 111 finished with value: 0.8027528405189515 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 42, 'dropout_1': 0.24173428736036667, 'optimizer': 'RMSprop', 'lstm_units_2': 102, 'dropout_2': 0.3737598265191093, 'lstm_units_3': 36, 'dropout_3': 0.30744383998382313}. Best is trial 83 with value: 0.8910916328430176.\n",
      "[I 2024-05-24 21:33:46,393] Trial 112 finished with value: 0.8729171276092529 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 121, 'dropout_1': 0.17775133140656113, 'optimizer': 'RMSprop', 'lstm_units_2': 107, 'dropout_2': 0.38609515152389334, 'lstm_units_3': 22, 'dropout_3': 0.21721954327854945}. Best is trial 83 with value: 0.8910916328430176.\n",
      "[I 2024-05-24 21:34:12,396] Trial 113 finished with value: 0.8534939765930176 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 115, 'dropout_1': 0.3565457668141921, 'optimizer': 'RMSprop', 'lstm_units_2': 94, 'dropout_2': 0.41823101411349584, 'lstm_units_3': 74, 'dropout_3': 0.16494760899731112}. Best is trial 83 with value: 0.8910916328430176.\n",
      "[I 2024-05-24 21:34:38,120] Trial 114 finished with value: 0.8620299339294434 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 126, 'dropout_1': 0.1411538111233539, 'optimizer': 'RMSprop', 'lstm_units_2': 21, 'dropout_2': 0.3947999029896085, 'lstm_units_3': 29, 'dropout_3': 0.019139573926293948}. Best is trial 83 with value: 0.8910916328430176.\n",
      "[I 2024-05-24 21:35:04,111] Trial 115 finished with value: 0.8523256659507752 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 123, 'dropout_1': 0.2569819042725359, 'optimizer': 'RMSprop', 'lstm_units_2': 59, 'dropout_2': 0.35583881354167796, 'lstm_units_3': 41, 'dropout_3': 0.07989860574588531}. Best is trial 83 with value: 0.8910916328430176.\n",
      "[I 2024-05-24 21:35:22,266] Trial 116 finished with value: 0.8753778576850891 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 76, 'dropout_1': 0.0549564268887429, 'optimizer': 'RMSprop', 'lstm_units_2': 86, 'dropout_2': 0.07391616541659318}. Best is trial 83 with value: 0.8910916328430176.\n",
      "[I 2024-05-24 21:35:47,593] Trial 117 finished with value: 0.8728806138038635 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 117, 'dropout_1': 0.042882051202158294, 'optimizer': 'RMSprop', 'lstm_units_2': 89, 'dropout_2': 0.3344665449493597, 'lstm_units_3': 50, 'dropout_3': 0.3251281509706368}. Best is trial 83 with value: 0.8910916328430176.\n",
      "[I 2024-05-24 21:36:05,760] Trial 118 finished with value: 0.8826505899429321 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 112, 'dropout_1': 0.022075580161132542, 'optimizer': 'RMSprop', 'lstm_units_2': 16, 'dropout_2': 0.11395278901250616}. Best is trial 83 with value: 0.8910916328430176.\n",
      "[I 2024-05-24 21:36:24,387] Trial 119 finished with value: 0.8475355982780457 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 112, 'dropout_1': 0.009356420036964136, 'optimizer': 'RMSprop', 'lstm_units_2': 17, 'dropout_2': 0.11142763414629846}. Best is trial 83 with value: 0.8910916328430176.\n",
      "[I 2024-05-24 21:36:49,873] Trial 120 finished with value: 0.8632347583770752 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 120, 'dropout_1': 0.18565388204312572, 'optimizer': 'RMSprop', 'lstm_units_2': 115, 'dropout_2': 0.09395104016493627, 'lstm_units_3': 60, 'dropout_3': 0.2632988683534203}. Best is trial 83 with value: 0.8910916328430176.\n",
      "[I 2024-05-24 21:37:08,921] Trial 121 finished with value: 0.8559766292572022 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 114, 'dropout_1': 0.021586025669605966, 'optimizer': 'RMSprop', 'lstm_units_2': 23, 'dropout_2': 0.12927470764879545}. Best is trial 83 with value: 0.8910916328430176.\n",
      "[I 2024-05-24 21:37:27,591] Trial 122 finished with value: 0.8704855680465698 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 128, 'dropout_1': 0.07519018546153294, 'optimizer': 'RMSprop', 'lstm_units_2': 20, 'dropout_2': 0.16931858951642906}. Best is trial 83 with value: 0.8910916328430176.\n",
      "[I 2024-05-24 21:37:46,518] Trial 123 finished with value: 0.8559328317642212 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 119, 'dropout_1': 0.412811157769211, 'optimizer': 'RMSprop', 'lstm_units_2': 24, 'dropout_2': 0.14610357105887362}. Best is trial 83 with value: 0.8910916328430176.\n",
      "[I 2024-05-24 21:38:05,764] Trial 124 finished with value: 0.8559182167053223 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 123, 'dropout_1': 0.16176387522584842, 'optimizer': 'RMSprop', 'lstm_units_2': 98, 'dropout_2': 0.48065743081168866}. Best is trial 83 with value: 0.8910916328430176.\n",
      "[I 2024-05-24 21:38:24,583] Trial 125 finished with value: 0.8644322872161865 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 117, 'dropout_1': 0.06153800287979077, 'optimizer': 'RMSprop', 'lstm_units_2': 28, 'dropout_2': 0.40361256495976405}. Best is trial 83 with value: 0.8910916328430176.\n",
      "[I 2024-05-24 21:38:43,683] Trial 126 finished with value: 0.8862577557563782 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 110, 'dropout_1': 0.2247987754507471, 'optimizer': 'RMSprop', 'lstm_units_2': 109, 'dropout_2': 0.11708979006362644}. Best is trial 83 with value: 0.8910916328430176.\n",
      "[I 2024-05-24 21:39:00,214] Trial 127 finished with value: 0.7906024336814881 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 100, 'dropout_1': 0.2265424504242575, 'optimizer': 'SGD', 'lstm_units_2': 104, 'dropout_2': 0.11183885679486363}. Best is trial 83 with value: 0.8910916328430176.\n",
      "[I 2024-05-24 21:39:18,831] Trial 128 finished with value: 0.8692953586578369 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 109, 'dropout_1': 0.2731842710070035, 'optimizer': 'RMSprop', 'lstm_units_2': 48, 'dropout_2': 0.1263406879503799}. Best is trial 83 with value: 0.8910916328430176.\n",
      "[I 2024-05-24 21:39:43,830] Trial 129 finished with value: 0.8717122912406922 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 105, 'dropout_1': 0.20280117018351762, 'optimizer': 'RMSprop', 'lstm_units_2': 81, 'dropout_2': 0.08564792284555219, 'lstm_units_3': 92, 'dropout_3': 0.28983126256762265}. Best is trial 83 with value: 0.8910916328430176.\n",
      "[I 2024-05-24 21:40:02,410] Trial 130 finished with value: 0.8680102348327636 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 111, 'dropout_1': 0.24265100585376098, 'optimizer': 'RMSprop', 'lstm_units_2': 112, 'dropout_2': 0.024650233355447446}. Best is trial 83 with value: 0.8910916328430176.\n",
      "[I 2024-05-24 21:40:21,253] Trial 131 finished with value: 0.8765899896621704 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 114, 'dropout_1': 0.03738360209041877, 'optimizer': 'RMSprop', 'lstm_units_2': 18, 'dropout_2': 0.09810841413552768}. Best is trial 83 with value: 0.8910916328430176.\n",
      "[I 2024-05-24 21:40:40,503] Trial 132 finished with value: 0.8656663060188293 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 121, 'dropout_1': 0.2158485937525401, 'optimizer': 'RMSprop', 'lstm_units_2': 109, 'dropout_2': 0.3675866317511489}. Best is trial 83 with value: 0.8910916328430176.\n",
      "[I 2024-05-24 21:40:59,592] Trial 133 finished with value: 0.8656152009963989 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 125, 'dropout_1': 0.23533476024044614, 'optimizer': 'RMSprop', 'lstm_units_2': 16, 'dropout_2': 0.3803759342462835}. Best is trial 83 with value: 0.8910916328430176.\n",
      "[I 2024-05-24 21:41:18,491] Trial 134 finished with value: 0.8584300875663757 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 116, 'dropout_1': 0.027234511353765083, 'optimizer': 'RMSprop', 'lstm_units_2': 91, 'dropout_2': 0.3461437864754149}. Best is trial 83 with value: 0.8910916328430176.\n",
      "[I 2024-05-24 21:41:37,587] Trial 135 finished with value: 0.8693318724632263 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 118, 'dropout_1': 0.26291126932297987, 'optimizer': 'RMSprop', 'lstm_units_2': 96, 'dropout_2': 0.432042997862091}. Best is trial 83 with value: 0.8910916328430176.\n",
      "[I 2024-05-24 21:41:56,078] Trial 136 finished with value: 0.8704709768295288 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 112, 'dropout_1': 0.0498377987375715, 'optimizer': 'RMSprop', 'lstm_units_2': 121, 'dropout_2': 0.13960987726348528}. Best is trial 83 with value: 0.8910916328430176.\n",
      "[I 2024-05-24 21:42:19,663] Trial 137 finished with value: 0.8644030690193176 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 107, 'dropout_1': 0.09170135601363258, 'optimizer': 'Adam', 'lstm_units_2': 101, 'dropout_2': 0.20529628272415168, 'lstm_units_3': 32, 'dropout_3': 0.19433584460338782}. Best is trial 83 with value: 0.8910916328430176.\n",
      "[I 2024-05-24 21:42:44,977] Trial 138 finished with value: 0.86081782579422 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 123, 'dropout_1': 0.49980426430916025, 'optimizer': 'RMSprop', 'lstm_units_2': 31, 'dropout_2': 0.04570132908273793, 'lstm_units_3': 25, 'dropout_3': 0.11968916973838126}. Best is trial 83 with value: 0.8910916328430176.\n",
      "[I 2024-05-24 21:43:03,192] Trial 139 finished with value: 0.8390142440795898 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 66, 'dropout_1': 0.009706116819635027, 'optimizer': 'RMSprop', 'lstm_units_2': 87, 'dropout_2': 0.122342677400075}. Best is trial 83 with value: 0.8910916328430176.\n",
      "[I 2024-05-24 21:43:21,364] Trial 140 finished with value: 0.777217960357666 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 24, 'dropout_1': 0.4474276191856878, 'optimizer': 'RMSprop', 'lstm_units_2': 83, 'dropout_2': 0.16770152715433656}. Best is trial 83 with value: 0.8910916328430176.\n",
      "[I 2024-05-24 21:43:47,265] Trial 141 finished with value: 0.8777583122253418 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 126, 'dropout_1': 0.24903461375792313, 'optimizer': 'RMSprop', 'lstm_units_2': 114, 'dropout_2': 0.44834592225373116, 'lstm_units_3': 105, 'dropout_3': 0.032351366940760684}. Best is trial 83 with value: 0.8910916328430176.\n",
      "[I 2024-05-24 21:44:12,496] Trial 142 finished with value: 0.86081782579422 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 120, 'dropout_1': 0.2099490017750169, 'optimizer': 'RMSprop', 'lstm_units_2': 122, 'dropout_2': 0.4292804035103176, 'lstm_units_3': 19, 'dropout_3': 0.05994478002518079}. Best is trial 83 with value: 0.8910916328430176.\n",
      "[I 2024-05-24 21:44:38,291] Trial 143 finished with value: 0.8620372414588928 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 124, 'dropout_1': 0.2890549563399227, 'optimizer': 'RMSprop', 'lstm_units_2': 110, 'dropout_2': 0.4174866122195531, 'lstm_units_3': 123, 'dropout_3': 0.25333961746750305}. Best is trial 83 with value: 0.8910916328430176.\n",
      "[I 2024-05-24 21:45:03,552] Trial 144 finished with value: 0.8789631247520446 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 128, 'dropout_1': 0.2311296808029788, 'optimizer': 'RMSprop', 'lstm_units_2': 85, 'dropout_2': 0.4079592742926903, 'lstm_units_3': 36, 'dropout_3': 0.07884024314209964}. Best is trial 83 with value: 0.8910916328430176.\n",
      "[I 2024-05-24 21:45:28,239] Trial 145 finished with value: 0.8523840785026551 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 86, 'dropout_1': 0.30881222547960885, 'optimizer': 'RMSprop', 'lstm_units_2': 43, 'dropout_2': 0.3926382838399006, 'lstm_units_3': 82, 'dropout_3': 0.4140963284629358}. Best is trial 83 with value: 0.8910916328430176.\n",
      "[I 2024-05-24 21:45:53,572] Trial 146 finished with value: 0.8752902507781982 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 121, 'dropout_1': 0.40198267102702046, 'optimizer': 'RMSprop', 'lstm_units_2': 119, 'dropout_2': 0.2702456564514131, 'lstm_units_3': 74, 'dropout_3': 0.03770489144401386}. Best is trial 83 with value: 0.8910916328430176.\n",
      "[I 2024-05-24 21:46:12,647] Trial 147 finished with value: 0.8668638229370117 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 118, 'dropout_1': 0.19327422004121322, 'optimizer': 'RMSprop', 'lstm_units_2': 19, 'dropout_2': 0.28690273283073386}. Best is trial 83 with value: 0.8910916328430176.\n",
      "[I 2024-05-24 21:46:31,416] Trial 148 finished with value: 0.8632347583770752 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 114, 'dropout_1': 0.035679558479777704, 'optimizer': 'RMSprop', 'lstm_units_2': 116, 'dropout_2': 0.062456678365199136}. Best is trial 83 with value: 0.8910916328430176.\n",
      "[I 2024-05-24 21:46:54,173] Trial 149 finished with value: 0.7337422370910645 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 116, 'dropout_1': 0.3808746713546063, 'optimizer': 'SGD', 'lstm_units_2': 90, 'dropout_2': 0.44053592403782127, 'lstm_units_3': 48, 'dropout_3': 0.14774184633617293}. Best is trial 83 with value: 0.8910916328430176.\n",
      "[I 2024-05-24 21:47:13,412] Trial 150 finished with value: 0.86078861951828 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 122, 'dropout_1': 0.07698965944590719, 'optimizer': 'RMSprop', 'lstm_units_2': 107, 'dropout_2': 0.4672699648941917}. Best is trial 83 with value: 0.8910916328430176.\n",
      "[I 2024-05-24 21:47:37,502] Trial 151 finished with value: 0.8680540204048157 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 119, 'dropout_1': 0.2638664767494404, 'optimizer': 'Adam', 'lstm_units_2': 125, 'dropout_2': 0.3527469168875018, 'lstm_units_3': 90, 'dropout_3': 0.0929583614727062}. Best is trial 83 with value: 0.8910916328430176.\n",
      "[I 2024-05-24 21:47:49,608] Trial 152 finished with value: 0.8075574994087219 and parameters: {'num_lstm_layers': 1, 'lstm_units_1': 125, 'dropout_1': 0.17564250284510097, 'optimizer': 'Adam'}. Best is trial 83 with value: 0.8910916328430176.\n",
      "[I 2024-05-24 21:48:13,282] Trial 153 finished with value: 0.8487404108047485 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 122, 'dropout_1': 0.2575949920384703, 'optimizer': 'Adam', 'lstm_units_2': 35, 'dropout_2': 0.2357876755037876, 'lstm_units_3': 89, 'dropout_3': 0.010755918001985764}. Best is trial 83 with value: 0.8910916328430176.\n",
      "[I 2024-05-24 21:48:37,185] Trial 154 finished with value: 0.8705586075782776 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 126, 'dropout_1': 0.23527032785615312, 'optimizer': 'Adam', 'lstm_units_2': 21, 'dropout_2': 0.2562595631354386, 'lstm_units_3': 118, 'dropout_3': 0.05663884014223608}. Best is trial 83 with value: 0.8910916328430176.\n",
      "[I 2024-05-24 21:49:01,103] Trial 155 finished with value: 0.8475429058074951 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 120, 'dropout_1': 0.22124344981397043, 'optimizer': 'Adam', 'lstm_units_2': 111, 'dropout_2': 0.37353396132184546, 'lstm_units_3': 95, 'dropout_3': 0.11492991443125847}. Best is trial 83 with value: 0.8910916328430176.\n",
      "[I 2024-05-24 21:49:26,366] Trial 156 finished with value: 0.8669368386268616 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 116, 'dropout_1': 0.27551716802228443, 'optimizer': 'RMSprop', 'lstm_units_2': 117, 'dropout_2': 0.11646643330636614, 'lstm_units_3': 105, 'dropout_3': 0.02626548297662373}. Best is trial 83 with value: 0.8910916328430176.\n",
      "[I 2024-05-24 21:49:51,669] Trial 157 finished with value: 0.8522307515144348 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 113, 'dropout_1': 0.04743249719054571, 'optimizer': 'RMSprop', 'lstm_units_2': 79, 'dropout_2': 0.30437644004922404, 'lstm_units_3': 78, 'dropout_3': 0.0909296080843151}. Best is trial 83 with value: 0.8910916328430176.\n",
      "[I 2024-05-24 21:50:17,118] Trial 158 finished with value: 0.864497983455658 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 123, 'dropout_1': 0.018123180649988958, 'optimizer': 'RMSprop', 'lstm_units_2': 26, 'dropout_2': 0.3252651161548416, 'lstm_units_3': 86, 'dropout_3': 0.47603600670291546}. Best is trial 83 with value: 0.8910916328430176.\n",
      "[I 2024-05-24 21:50:42,421] Trial 159 finished with value: 0.8837824106216431 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 119, 'dropout_1': 0.2080108082623622, 'optimizer': 'RMSprop', 'lstm_units_2': 92, 'dropout_2': 0.3633956142596344, 'lstm_units_3': 26, 'dropout_3': 0.0008829635640419942}. Best is trial 83 with value: 0.8910916328430176.\n",
      "[I 2024-05-24 21:51:07,406] Trial 160 finished with value: 0.8716976881027222 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 110, 'dropout_1': 0.21058068441936484, 'optimizer': 'RMSprop', 'lstm_units_2': 94, 'dropout_2': 0.0356762033166161, 'lstm_units_3': 41, 'dropout_3': 0.015548377846972827}. Best is trial 83 with value: 0.8910916328430176.\n",
      "[I 2024-05-24 21:51:32,670] Trial 161 finished with value: 0.8717634320259094 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 119, 'dropout_1': 0.20437206241878705, 'optimizer': 'RMSprop', 'lstm_units_2': 89, 'dropout_2': 0.3631499887663722, 'lstm_units_3': 29, 'dropout_3': 0.04468357010576646}. Best is trial 83 with value: 0.8910916328430176.\n",
      "[I 2024-05-24 21:51:57,893] Trial 162 finished with value: 0.8970938324928284 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 124, 'dropout_1': 0.24829603755957766, 'optimizer': 'RMSprop', 'lstm_units_2': 119, 'dropout_2': 0.383528600256297, 'lstm_units_3': 24, 'dropout_3': 0.0017415629411977266}. Best is trial 162 with value: 0.8970938324928284.\n",
      "[I 2024-05-24 21:52:23,271] Trial 163 finished with value: 0.8619861245155335 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 124, 'dropout_1': 0.24068653260401765, 'optimizer': 'RMSprop', 'lstm_units_2': 128, 'dropout_2': 0.38554883621376024, 'lstm_units_3': 25, 'dropout_3': 0.003252935305221922}. Best is trial 162 with value: 0.8970938324928284.\n",
      "[I 2024-05-24 21:52:48,467] Trial 164 finished with value: 0.8438481330871582 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 128, 'dropout_1': 0.22709262565894128, 'optimizer': 'RMSprop', 'lstm_units_2': 92, 'dropout_2': 0.3732380446366426, 'lstm_units_3': 19, 'dropout_3': 0.02759213240155794}. Best is trial 162 with value: 0.8970938324928284.\n",
      "[I 2024-05-24 21:53:14,179] Trial 165 finished with value: 0.8692515611648559 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 117, 'dropout_1': 0.252965759699358, 'optimizer': 'RMSprop', 'lstm_units_2': 87, 'dropout_2': 0.39551541476041974, 'lstm_units_3': 33, 'dropout_3': 0.0021916593953204833}. Best is trial 162 with value: 0.8970938324928284.\n",
      "[I 2024-05-24 21:53:39,698] Trial 166 finished with value: 0.8704855799674988 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 121, 'dropout_1': 0.24942534706177946, 'optimizer': 'RMSprop', 'lstm_units_2': 85, 'dropout_2': 0.35545532708792504, 'lstm_units_3': 28, 'dropout_3': 0.2246199229074093}. Best is trial 162 with value: 0.8970938324928284.\n",
      "[I 2024-05-24 21:54:05,340] Trial 167 finished with value: 0.8559912323951722 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 124, 'dropout_1': 0.391881985182759, 'optimizer': 'RMSprop', 'lstm_units_2': 99, 'dropout_2': 0.4132655407240196, 'lstm_units_3': 23, 'dropout_3': 0.02263502653978083}. Best is trial 162 with value: 0.8970938324928284.\n",
      "[I 2024-05-24 21:54:30,763] Trial 168 finished with value: 0.8559547305107117 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 126, 'dropout_1': 0.2190620747843318, 'optimizer': 'RMSprop', 'lstm_units_2': 104, 'dropout_2': 0.3854020108869539, 'lstm_units_3': 38, 'dropout_3': 0.046910633330775824}. Best is trial 162 with value: 0.8970938324928284.\n",
      "[I 2024-05-24 21:54:49,689] Trial 169 finished with value: 0.872887909412384 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 114, 'dropout_1': 0.4118970340279621, 'optimizer': 'RMSprop', 'lstm_units_2': 96, 'dropout_2': 0.07830791217538345}. Best is trial 162 with value: 0.8970938324928284.\n",
      "[I 2024-05-24 21:55:02,207] Trial 170 finished with value: 0.7893756866455078 and parameters: {'num_lstm_layers': 1, 'lstm_units_1': 118, 'dropout_1': 0.36364301042781383, 'optimizer': 'RMSprop'}. Best is trial 162 with value: 0.8970938324928284.\n",
      "[I 2024-05-24 21:55:27,746] Trial 171 finished with value: 0.8922745585441589 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 122, 'dropout_1': 0.0608334946109374, 'optimizer': 'RMSprop', 'lstm_units_2': 118, 'dropout_2': 0.3668101620855798, 'lstm_units_3': 16, 'dropout_3': 0.0677113586579078}. Best is trial 162 with value: 0.8970938324928284.\n",
      "[I 2024-05-24 21:55:53,015] Trial 172 finished with value: 0.8656881928443909 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 122, 'dropout_1': 0.0695825767243983, 'optimizer': 'RMSprop', 'lstm_units_2': 120, 'dropout_2': 0.36564803916606975, 'lstm_units_3': 16, 'dropout_3': 0.061961105817245656}. Best is trial 162 with value: 0.8970938324928284.\n",
      "[I 2024-05-24 21:56:18,311] Trial 173 finished with value: 0.8692661643028259 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 119, 'dropout_1': 0.10395278082014442, 'optimizer': 'RMSprop', 'lstm_units_2': 122, 'dropout_2': 0.005832101519791666, 'lstm_units_3': 20, 'dropout_3': 0.013195631943003674}. Best is trial 162 with value: 0.8970938324928284.\n",
      "[I 2024-05-24 21:56:43,840] Trial 174 finished with value: 0.8753778576850891 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 121, 'dropout_1': 0.056686026332695774, 'optimizer': 'RMSprop', 'lstm_units_2': 125, 'dropout_2': 0.34386398698716686, 'lstm_units_3': 25, 'dropout_3': 0.03745599390201808}. Best is trial 162 with value: 0.8970938324928284.\n",
      "[I 2024-05-24 21:57:09,003] Trial 175 finished with value: 0.8656809091567993 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 126, 'dropout_1': 0.034950443374129725, 'optimizer': 'RMSprop', 'lstm_units_2': 18, 'dropout_2': 0.37620660639353737, 'lstm_units_3': 18, 'dropout_3': 0.20708875964720652}. Best is trial 162 with value: 0.8970938324928284.\n",
      "[I 2024-05-24 21:57:34,246] Trial 176 finished with value: 0.8813800692558289 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 116, 'dropout_1': 0.08310572002822927, 'optimizer': 'RMSprop', 'lstm_units_2': 114, 'dropout_2': 0.4029347425041201, 'lstm_units_3': 31, 'dropout_3': 0.07181810916610423}. Best is trial 162 with value: 0.8970938324928284.\n",
      "[I 2024-05-24 21:57:51,992] Trial 177 finished with value: 0.8402555584907532 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 48, 'dropout_1': 0.06224238966764776, 'optimizer': 'RMSprop', 'lstm_units_2': 116, 'dropout_2': 0.10229041311727124}. Best is trial 162 with value: 0.8970938324928284.\n",
      "[I 2024-05-24 21:58:17,642] Trial 178 finished with value: 0.8898576140403748 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 124, 'dropout_1': 0.043552915761269115, 'optimizer': 'RMSprop', 'lstm_units_2': 118, 'dropout_2': 0.3584512936733349, 'lstm_units_3': 27, 'dropout_3': 0.2413361693147798}. Best is trial 162 with value: 0.8970938324928284.\n",
      "[I 2024-05-24 21:58:43,240] Trial 179 finished with value: 0.8692515611648559 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 123, 'dropout_1': 0.048517620081100464, 'optimizer': 'RMSprop', 'lstm_units_2': 91, 'dropout_2': 0.33396118097835314, 'lstm_units_3': 24, 'dropout_3': 0.23988273709637165}. Best is trial 162 with value: 0.8970938324928284.\n",
      "[I 2024-05-24 21:59:02,317] Trial 180 finished with value: 0.8595472812652588 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 119, 'dropout_1': 0.044144664045071905, 'optimizer': 'RMSprop', 'lstm_units_2': 84, 'dropout_2': 0.3589189064770385}. Best is trial 162 with value: 0.8970938324928284.\n",
      "[I 2024-05-24 21:59:28,036] Trial 181 finished with value: 0.8728660106658935 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 124, 'dropout_1': 0.02735833865666662, 'optimizer': 'RMSprop', 'lstm_units_2': 120, 'dropout_2': 0.37999369065962957, 'lstm_units_3': 27, 'dropout_3': 0.2562169128945691}. Best is trial 162 with value: 0.8970938324928284.\n",
      "[I 2024-05-24 21:59:53,685] Trial 182 finished with value: 0.8729536294937134 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 128, 'dropout_1': 0.23098155001732792, 'optimizer': 'RMSprop', 'lstm_units_2': 118, 'dropout_2': 0.36745026891749927, 'lstm_units_3': 21, 'dropout_3': 0.29148054049158273}. Best is trial 162 with value: 0.8970938324928284.\n",
      "[I 2024-05-24 22:00:19,389] Trial 183 finished with value: 0.8680394411087036 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 121, 'dropout_1': 0.0609287511127616, 'optimizer': 'RMSprop', 'lstm_units_2': 113, 'dropout_2': 0.3528634208692208, 'lstm_units_3': 35, 'dropout_3': 0.3145265504355845}. Best is trial 162 with value: 0.8970938324928284.\n",
      "[I 2024-05-24 22:00:44,689] Trial 184 finished with value: 0.8813946604728699 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 125, 'dropout_1': 0.020522324014695458, 'optimizer': 'RMSprop', 'lstm_units_2': 16, 'dropout_2': 0.34608221469198075, 'lstm_units_3': 30, 'dropout_3': 0.27835460479187135}. Best is trial 162 with value: 0.8970938324928284.\n",
      "[I 2024-05-24 22:01:10,003] Trial 185 finished with value: 0.8631763577461242 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 116, 'dropout_1': 0.24051174090423988, 'optimizer': 'RMSprop', 'lstm_units_2': 88, 'dropout_2': 0.393522697184562, 'lstm_units_3': 54, 'dropout_3': 0.03143664939897762}. Best is trial 162 with value: 0.8970938324928284.\n",
      "[I 2024-05-24 22:01:28,446] Trial 186 finished with value: 0.8850456357002259 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 112, 'dropout_1': 0.07223167594486514, 'optimizer': 'RMSprop', 'lstm_units_2': 123, 'dropout_2': 0.36847217397433757}. Best is trial 162 with value: 0.8970938324928284.\n",
      "[I 2024-05-24 22:01:47,258] Trial 187 finished with value: 0.8572471618652344 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 111, 'dropout_1': 0.076920232510495, 'optimizer': 'RMSprop', 'lstm_units_2': 124, 'dropout_2': 0.3686372447419816}. Best is trial 162 with value: 0.8970938324928284.\n",
      "[I 2024-05-24 22:02:05,980] Trial 188 finished with value: 0.8499379277229309 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 107, 'dropout_1': 0.05618564444528978, 'optimizer': 'RMSprop', 'lstm_units_2': 106, 'dropout_2': 0.3826632350845899}. Best is trial 162 with value: 0.8970938324928284.\n",
      "[I 2024-05-24 22:02:23,210] Trial 189 finished with value: 0.8050821423530579 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 113, 'dropout_1': 0.0912412700612698, 'optimizer': 'SGD', 'lstm_units_2': 123, 'dropout_2': 0.09067144873514181}. Best is trial 162 with value: 0.8970938324928284.\n",
      "[I 2024-05-24 22:02:42,263] Trial 190 finished with value: 0.8656882047653198 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 117, 'dropout_1': 0.06633074694192714, 'optimizer': 'RMSprop', 'lstm_units_2': 109, 'dropout_2': 0.3512546813063242}. Best is trial 162 with value: 0.8970938324928284.\n",
      "[I 2024-05-24 22:03:01,503] Trial 191 finished with value: 0.8910551428794861 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 122, 'dropout_1': 0.04371758837679643, 'optimizer': 'RMSprop', 'lstm_units_2': 118, 'dropout_2': 0.4542875785447063}. Best is trial 162 with value: 0.8970938324928284.\n",
      "[I 2024-05-24 22:03:20,526] Trial 192 finished with value: 0.8862139463424683 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 120, 'dropout_1': 0.026007191747584174, 'optimizer': 'RMSprop', 'lstm_units_2': 118, 'dropout_2': 0.3607937688414451}. Best is trial 162 with value: 0.8970938324928284.\n",
      "[I 2024-05-24 22:03:39,671] Trial 193 finished with value: 0.8716830968856811 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 120, 'dropout_1': 0.03925817372179996, 'optimizer': 'RMSprop', 'lstm_units_2': 118, 'dropout_2': 0.3626448927525044}. Best is trial 162 with value: 0.8970938324928284.\n",
      "[I 2024-05-24 22:03:58,528] Trial 194 finished with value: 0.8910551428794861 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 115, 'dropout_1': 0.0019865838434008666, 'optimizer': 'RMSprop', 'lstm_units_2': 116, 'dropout_2': 0.49844560408085703}. Best is trial 162 with value: 0.8970938324928284.\n",
      "[I 2024-05-24 22:04:17,565] Trial 195 finished with value: 0.8729244232177734 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 118, 'dropout_1': 0.029782023136797772, 'optimizer': 'RMSprop', 'lstm_units_2': 120, 'dropout_2': 0.48896277574499974}. Best is trial 162 with value: 0.8970938324928284.\n",
      "[I 2024-05-24 22:04:36,739] Trial 196 finished with value: 0.8656663060188293 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 115, 'dropout_1': 0.00016286854565618694, 'optimizer': 'RMSprop', 'lstm_units_2': 115, 'dropout_2': 0.42447303877758996}. Best is trial 162 with value: 0.8970938324928284.\n",
      "[I 2024-05-24 22:04:56,409] Trial 197 finished with value: 0.8813873529434204 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 122, 'dropout_1': 0.0092365165922032, 'optimizer': 'RMSprop', 'lstm_units_2': 117, 'dropout_2': 0.4521634546733139}. Best is trial 162 with value: 0.8970938324928284.\n",
      "[I 2024-05-24 22:05:15,709] Trial 198 finished with value: 0.8862139463424683 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 120, 'dropout_1': 0.01687265488663041, 'optimizer': 'RMSprop', 'lstm_units_2': 120, 'dropout_2': 0.49819875782509665}. Best is trial 162 with value: 0.8970938324928284.\n",
      "[I 2024-05-24 22:05:35,194] Trial 199 finished with value: 0.8753121495246887 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 121, 'dropout_1': 0.01067239486600353, 'optimizer': 'RMSprop', 'lstm_units_2': 122, 'dropout_2': 0.46140625863192336}. Best is trial 162 with value: 0.8970938324928284.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'num_lstm_layers': 3, 'lstm_units_1': 124, 'dropout_1': 0.24829603755957766, 'optimizer': 'RMSprop', 'lstm_units_2': 119, 'dropout_2': 0.383528600256297, 'lstm_units_3': 24, 'dropout_3': 0.0017415629411977266}\n"
     ]
    }
   ],
   "source": [
    "X_train_val = load('../../BEST SET/X_train_fft.joblib')\n",
    "y_train_val = load('../../BEST SET/y_Train.joblib')\n",
    "X_test = load('../../BEST SET/X_test_fft.joblib')\n",
    "y_test = load('../../BEST SET/y_Test.joblib')\n",
    "half = X_train_val[::2]\n",
    "half_labels = y_train_val[::2]\n",
    "second_half = X_train_val[1::2]\n",
    "second_half_labels = y_train_val[1::2]\n",
    "X_test = np.append(X_test, second_half, axis=0)\n",
    "y_test = np.append(y_test, second_half_labels, axis=0)\n",
    "X_train_val = half\n",
    "y_train_val = half_labels\n",
    "\n",
    "input_sh = X_train_val.shape\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Optuna Objective Function\n",
    "def objective(trial):\n",
    "    lstm_2 = 0\n",
    "    drop_2 = 0\n",
    "    lstm_3 = 0\n",
    "    drop_3 = 0\n",
    "    num_lstm = trial.suggest_int('num_lstm_layers', 1, 3)\n",
    "    lstm_1 = trial.suggest_int('lstm_units_1', 16, 128)\n",
    "    dropout_1 = trial.suggest_float('dropout_1', 0.0, 0.5)\n",
    "    optimizer = trial.suggest_categorical('optimizer', ['SGD', 'RMSprop', 'Adam'])\n",
    "\n",
    "    if num_lstm > 1:\n",
    "        lstm_2 = trial.suggest_int(f'lstm_units_2', 16, 128)\n",
    "        drop_2 = trial.suggest_float(f'dropout_2', 0.0, 0.5)\n",
    "    if num_lstm > 2:\n",
    "        lstm_3 = trial.suggest_int(f'lstm_units_3', 16, 128)\n",
    "        drop_3 = trial.suggest_float(f'dropout_3', 0.0, 0.5)\n",
    "\n",
    "\n",
    "    # Assuming StratifiedKFold, customize if needed\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)  \n",
    "    scores = []\n",
    "    for train_index, test_index in skf.split(X_train_val, y_train_val):\n",
    "        X_tr, X_val = X_train_val[train_index], X_train_val[test_index]\n",
    "        y_tr, y_val = y_train_val[train_index], y_train_val[test_index]\n",
    "\n",
    "        model = create_model(optimizer, num_lstm, lstm_1, dropout_1, lstm_2, drop_2, lstm_3, drop_3)\n",
    "        model.fit(X_tr, y_tr,epochs=20, verbose = 0) \n",
    "        score = model.evaluate(X_val, y_val, verbose = 0)\n",
    "        scores.append(score[1])\n",
    "    return np.array(scores).mean()\n",
    "\n",
    "# Hyperparameter Optimization\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=200)\n",
    "\n",
    "best_params = study.best_params\n",
    "print(\"Best hyperparameters:\", best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-24 22:05:35,448] A new study created in memory with name: no-name-a84b27a0-b0a2-4aff-bb87-324e2fe85066\n",
      "[I 2024-05-24 22:05:48,474] Trial 0 finished with value: 0.7336358666419983 and parameters: {'num_lstm_layers': 1, 'lstm_units_1': 54, 'dropout_1': 0.10435671802174634, 'optimizer': 'SGD'}. Best is trial 0 with value: 0.7336358666419983.\n",
      "[I 2024-05-24 22:06:18,364] Trial 1 finished with value: 0.8700763940811157 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 98, 'dropout_1': 0.041309228892223326, 'optimizer': 'RMSprop', 'lstm_units_2': 78, 'dropout_2': 0.26019508326620544, 'lstm_units_3': 29, 'dropout_3': 0.08726921059782766}. Best is trial 1 with value: 0.8700763940811157.\n",
      "[I 2024-05-24 22:06:39,095] Trial 2 finished with value: 0.863608467578888 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 90, 'dropout_1': 0.05316720774925998, 'optimizer': 'Adam', 'lstm_units_2': 113, 'dropout_2': 0.12052146512450146}. Best is trial 1 with value: 0.8700763940811157.\n",
      "[I 2024-05-24 22:07:05,637] Trial 3 finished with value: 0.7215064644813538 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 118, 'dropout_1': 0.0596134684030698, 'optimizer': 'SGD', 'lstm_units_2': 56, 'dropout_2': 0.02641257037626532, 'lstm_units_3': 77, 'dropout_3': 0.0032751304591956965}. Best is trial 1 with value: 0.8700763940811157.\n",
      "[I 2024-05-24 22:07:24,919] Trial 4 finished with value: 0.7392680048942566 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 33, 'dropout_1': 0.39055075250526994, 'optimizer': 'SGD', 'lstm_units_2': 91, 'dropout_2': 0.42386276248086535}. Best is trial 1 with value: 0.8700763940811157.\n",
      "[I 2024-05-24 22:07:46,636] Trial 5 finished with value: 0.8047081112861634 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 37, 'dropout_1': 0.40105445986142885, 'optimizer': 'RMSprop', 'lstm_units_2': 45, 'dropout_2': 0.42154356989811725}. Best is trial 1 with value: 0.8700763940811157.\n",
      "[I 2024-05-24 22:08:13,322] Trial 6 finished with value: 0.8313079476356506 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 77, 'dropout_1': 0.24982818302720988, 'optimizer': 'Adam', 'lstm_units_2': 76, 'dropout_2': 0.4094085440090738, 'lstm_units_3': 102, 'dropout_3': 0.03636178852665206}. Best is trial 1 with value: 0.8700763940811157.\n",
      "[I 2024-05-24 22:08:26,011] Trial 7 finished with value: 0.7135627508163452 and parameters: {'num_lstm_layers': 1, 'lstm_units_1': 35, 'dropout_1': 0.07694958649322736, 'optimizer': 'Adam'}. Best is trial 1 with value: 0.8700763940811157.\n",
      "[I 2024-05-24 22:08:51,918] Trial 8 finished with value: 0.7667330503463745 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 39, 'dropout_1': 0.3423279377008889, 'optimizer': 'Adam', 'lstm_units_2': 80, 'dropout_2': 0.1149541300550021, 'lstm_units_3': 29, 'dropout_3': 0.019608439446520576}. Best is trial 1 with value: 0.8700763940811157.\n",
      "[I 2024-05-24 22:09:18,480] Trial 9 finished with value: 0.7893790006637573 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 46, 'dropout_1': 0.3371578106807521, 'optimizer': 'Adam', 'lstm_units_2': 111, 'dropout_2': 0.370756035644046, 'lstm_units_3': 61, 'dropout_3': 0.17031000354127712}. Best is trial 1 with value: 0.8700763940811157.\n",
      "[I 2024-05-24 22:09:48,206] Trial 10 finished with value: 0.8781441688537598 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 126, 'dropout_1': 0.20103962061813438, 'optimizer': 'RMSprop', 'lstm_units_2': 18, 'dropout_2': 0.2610155655574033, 'lstm_units_3': 16, 'dropout_3': 0.33235570854308866}. Best is trial 10 with value: 0.8781441688537598.\n",
      "[I 2024-05-24 22:10:18,114] Trial 11 finished with value: 0.884595787525177 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 127, 'dropout_1': 0.18557064354140615, 'optimizer': 'RMSprop', 'lstm_units_2': 16, 'dropout_2': 0.2579583208902796, 'lstm_units_3': 16, 'dropout_3': 0.43635523473955434}. Best is trial 11 with value: 0.884595787525177.\n",
      "[I 2024-05-24 22:10:48,055] Trial 12 finished with value: 0.8870184183120727 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 128, 'dropout_1': 0.18553438493102495, 'optimizer': 'RMSprop', 'lstm_units_2': 21, 'dropout_2': 0.2756453000380457, 'lstm_units_3': 25, 'dropout_3': 0.4537169500337092}. Best is trial 12 with value: 0.8870184183120727.\n",
      "[I 2024-05-24 22:11:10,453] Trial 13 finished with value: 0.8619955658912659 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 109, 'dropout_1': 0.15878670026764324, 'optimizer': 'RMSprop', 'lstm_units_2': 19, 'dropout_2': 0.2921278406698996}. Best is trial 12 with value: 0.8870184183120727.\n",
      "[I 2024-05-24 22:11:40,730] Trial 14 finished with value: 0.8781180620193482 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 128, 'dropout_1': 0.16825466578180895, 'optimizer': 'RMSprop', 'lstm_units_2': 38, 'dropout_2': 0.16716313296424992, 'lstm_units_3': 58, 'dropout_3': 0.49672565040814587}. Best is trial 12 with value: 0.8870184183120727.\n",
      "[I 2024-05-24 22:12:03,022] Trial 15 finished with value: 0.8765312790870666 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 104, 'dropout_1': 0.26456175549352634, 'optimizer': 'RMSprop', 'lstm_units_2': 35, 'dropout_2': 0.33532934535145126}. Best is trial 12 with value: 0.8870184183120727.\n",
      "[I 2024-05-24 22:12:32,228] Trial 16 finished with value: 0.8668342709541321 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 79, 'dropout_1': 0.2372399535582362, 'optimizer': 'RMSprop', 'lstm_units_2': 58, 'dropout_2': 0.19845771038034793, 'lstm_units_3': 42, 'dropout_3': 0.46546842323252413}. Best is trial 12 with value: 0.8870184183120727.\n",
      "[I 2024-05-24 22:12:46,241] Trial 17 finished with value: 0.7150907635688781 and parameters: {'num_lstm_layers': 1, 'lstm_units_1': 60, 'dropout_1': 0.46605693333389664, 'optimizer': 'RMSprop'}. Best is trial 12 with value: 0.8870184183120727.\n",
      "[I 2024-05-24 22:13:09,075] Trial 18 finished with value: 0.8344815135002136 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 116, 'dropout_1': 0.0015650099569521347, 'optimizer': 'RMSprop', 'lstm_units_2': 27, 'dropout_2': 0.32060574814999226}. Best is trial 12 with value: 0.8870184183120727.\n",
      "[I 2024-05-24 22:13:37,861] Trial 19 finished with value: 0.8829959392547607 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 92, 'dropout_1': 0.13751131082903095, 'optimizer': 'RMSprop', 'lstm_units_2': 53, 'dropout_2': 0.20587107740582372, 'lstm_units_3': 16, 'dropout_3': 0.36942575480707907}. Best is trial 12 with value: 0.8870184183120727.\n",
      "[I 2024-05-24 22:13:57,501] Trial 20 finished with value: 0.80463627576828 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 113, 'dropout_1': 0.319161056712818, 'optimizer': 'SGD', 'lstm_units_2': 16, 'dropout_2': 0.49731900421665065}. Best is trial 12 with value: 0.8870184183120727.\n",
      "[I 2024-05-24 22:14:26,808] Trial 21 finished with value: 0.7611140251159668 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 18, 'dropout_1': 0.12384393134760668, 'optimizer': 'RMSprop', 'lstm_units_2': 59, 'dropout_2': 0.19114405977254162, 'lstm_units_3': 18, 'dropout_3': 0.37748317807250276}. Best is trial 12 with value: 0.8870184183120727.\n",
      "[I 2024-05-24 22:14:55,684] Trial 22 finished with value: 0.8700763940811157 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 92, 'dropout_1': 0.17836774355900395, 'optimizer': 'RMSprop', 'lstm_units_2': 44, 'dropout_2': 0.2219056994519177, 'lstm_units_3': 40, 'dropout_3': 0.38729403850469496}. Best is trial 12 with value: 0.8870184183120727.\n",
      "[I 2024-05-24 22:15:25,638] Trial 23 finished with value: 0.8741217136383057 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 128, 'dropout_1': 0.12390464597894582, 'optimizer': 'RMSprop', 'lstm_units_2': 27, 'dropout_2': 0.14847736949332463, 'lstm_units_3': 17, 'dropout_3': 0.2816368495635299}. Best is trial 12 with value: 0.8870184183120727.\n",
      "[I 2024-05-24 22:15:54,637] Trial 24 finished with value: 0.8321470618247986 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 87, 'dropout_1': 0.2050653602203335, 'optimizer': 'RMSprop', 'lstm_units_2': 30, 'dropout_2': 0.0679266722833719, 'lstm_units_3': 37, 'dropout_3': 0.4263480295557124}. Best is trial 12 with value: 0.8870184183120727.\n",
      "[I 2024-05-24 22:16:23,695] Trial 25 finished with value: 0.817588472366333 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 66, 'dropout_1': 0.2960575636581733, 'optimizer': 'RMSprop', 'lstm_units_2': 49, 'dropout_2': 0.23761218261808856, 'lstm_units_3': 127, 'dropout_3': 0.32195695399619395}. Best is trial 12 with value: 0.8870184183120727.\n",
      "[I 2024-05-24 22:16:53,513] Trial 26 finished with value: 0.850721561908722 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 102, 'dropout_1': 0.1445495565471338, 'optimizer': 'RMSprop', 'lstm_units_2': 94, 'dropout_2': 0.29778626688091303, 'lstm_units_3': 53, 'dropout_3': 0.4320672334301355}. Best is trial 12 with value: 0.8870184183120727.\n",
      "[I 2024-05-24 22:17:16,513] Trial 27 finished with value: 0.8716697096824646 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 119, 'dropout_1': 0.2169012289991626, 'optimizer': 'RMSprop', 'lstm_units_2': 64, 'dropout_2': 0.2726547027983717}. Best is trial 12 with value: 0.8870184183120727.\n",
      "[I 2024-05-24 22:17:46,130] Trial 28 finished with value: 0.8870282173156738 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 108, 'dropout_1': 0.27679550150935817, 'optimizer': 'RMSprop', 'lstm_units_2': 37, 'dropout_2': 0.35067001769692985, 'lstm_units_3': 78, 'dropout_3': 0.2222720759819716}. Best is trial 28 with value: 0.8870282173156738.\n",
      "[I 2024-05-24 22:17:59,873] Trial 29 finished with value: 0.7691556811332703 and parameters: {'num_lstm_layers': 1, 'lstm_units_1': 109, 'dropout_1': 0.27491651471646106, 'optimizer': 'SGD'}. Best is trial 28 with value: 0.8870282173156738.\n",
      "[I 2024-05-24 22:18:26,551] Trial 30 finished with value: 0.7280462384223938 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 123, 'dropout_1': 0.28926420350602483, 'optimizer': 'SGD', 'lstm_units_2': 25, 'dropout_2': 0.38442330168765865, 'lstm_units_3': 84, 'dropout_3': 0.221829312718814}. Best is trial 28 with value: 0.8870282173156738.\n",
      "[I 2024-05-24 22:18:56,115] Trial 31 finished with value: 0.8765345335006713 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 97, 'dropout_1': 0.11990435384017567, 'optimizer': 'RMSprop', 'lstm_units_2': 37, 'dropout_2': 0.3519838512667248, 'lstm_units_3': 90, 'dropout_3': 0.15564689982638025}. Best is trial 28 with value: 0.8870282173156738.\n",
      "[I 2024-05-24 22:19:25,556] Trial 32 finished with value: 0.8765410661697388 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 105, 'dropout_1': 0.08879014985703715, 'optimizer': 'RMSprop', 'lstm_units_2': 35, 'dropout_2': 0.21776586744965615, 'lstm_units_3': 29, 'dropout_3': 0.3976703380643682}. Best is trial 28 with value: 0.8870282173156738.\n",
      "[I 2024-05-24 22:19:55,402] Trial 33 finished with value: 0.8587762713432312 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 113, 'dropout_1': 0.19389093690072662, 'optimizer': 'RMSprop', 'lstm_units_2': 47, 'dropout_2': 0.300506925651702, 'lstm_units_3': 70, 'dropout_3': 0.48578212414148847}. Best is trial 28 with value: 0.8870282173156738.\n",
      "[I 2024-05-24 22:20:24,832] Trial 34 finished with value: 0.8539538860321045 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 97, 'dropout_1': 0.23451120290155147, 'optimizer': 'RMSprop', 'lstm_units_2': 24, 'dropout_2': 0.2422085768821738, 'lstm_units_3': 98, 'dropout_3': 0.24614864273662218}. Best is trial 28 with value: 0.8870282173156738.\n",
      "[I 2024-05-24 22:20:54,940] Trial 35 finished with value: 0.8579632997512817 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 121, 'dropout_1': 0.02694156760512606, 'optimizer': 'RMSprop', 'lstm_units_2': 68, 'dropout_2': 0.27062292226510026, 'lstm_units_3': 25, 'dropout_3': 0.33936376908630655}. Best is trial 28 with value: 0.8870282173156738.\n",
      "[I 2024-05-24 22:21:16,953] Trial 36 finished with value: 0.8466599345207214 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 84, 'dropout_1': 0.1521944644542487, 'optimizer': 'RMSprop', 'lstm_units_2': 124, 'dropout_2': 0.48045539538696536}. Best is trial 28 with value: 0.8870282173156738.\n",
      "[I 2024-05-24 22:21:43,565] Trial 37 finished with value: 0.758724045753479 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 119, 'dropout_1': 0.3802709666377495, 'optimizer': 'SGD', 'lstm_units_2': 52, 'dropout_2': 0.13760937428962755, 'lstm_units_3': 47, 'dropout_3': 0.4412090653047016}. Best is trial 28 with value: 0.8870282173156738.\n",
      "[I 2024-05-24 22:22:03,903] Trial 38 finished with value: 0.8587697505950928 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 93, 'dropout_1': 0.08953985876441078, 'optimizer': 'Adam', 'lstm_units_2': 39, 'dropout_2': 0.09116988988940644}. Best is trial 28 with value: 0.8870282173156738.\n",
      "[I 2024-05-24 22:22:33,814] Trial 39 finished with value: 0.865254020690918 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 110, 'dropout_1': 0.22258213747092154, 'optimizer': 'RMSprop', 'lstm_units_2': 20, 'dropout_2': 0.16914094398569662, 'lstm_units_3': 26, 'dropout_3': 0.2850928009483936}. Best is trial 28 with value: 0.8870282173156738.\n",
      "[I 2024-05-24 22:23:01,025] Trial 40 finished with value: 0.8547342181205749 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 99, 'dropout_1': 0.060962419438283066, 'optimizer': 'Adam', 'lstm_units_2': 32, 'dropout_2': 0.4555744944019121, 'lstm_units_3': 72, 'dropout_3': 0.19578307625655705}. Best is trial 28 with value: 0.8870282173156738.\n",
      "[I 2024-05-24 22:23:31,378] Trial 41 finished with value: 0.8934928894042968 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 124, 'dropout_1': 0.18855309077565646, 'optimizer': 'RMSprop', 'lstm_units_2': 16, 'dropout_2': 0.25226131417814907, 'lstm_units_3': 17, 'dropout_3': 0.3485404898013382}. Best is trial 41 with value: 0.8934928894042968.\n",
      "[I 2024-05-24 22:24:01,256] Trial 42 finished with value: 0.8740564107894897 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 123, 'dropout_1': 0.19441606798996777, 'optimizer': 'RMSprop', 'lstm_units_2': 16, 'dropout_2': 0.31662793652294624, 'lstm_units_3': 16, 'dropout_3': 0.3578571095928097}. Best is trial 41 with value: 0.8934928894042968.\n",
      "[I 2024-05-24 22:24:31,181] Trial 43 finished with value: 0.8757378935813904 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 116, 'dropout_1': 0.25743790331635996, 'optimizer': 'RMSprop', 'lstm_units_2': 23, 'dropout_2': 0.20476001650225437, 'lstm_units_3': 34, 'dropout_3': 0.2941648668583645}. Best is trial 41 with value: 0.8934928894042968.\n",
      "[I 2024-05-24 22:25:01,276] Trial 44 finished with value: 0.8886150002479554 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 128, 'dropout_1': 0.1347949014901785, 'optimizer': 'RMSprop', 'lstm_units_2': 42, 'dropout_2': 0.24752300896745805, 'lstm_units_3': 24, 'dropout_3': 0.40660826439764125}. Best is trial 41 with value: 0.8934928894042968.\n",
      "[I 2024-05-24 22:25:31,304] Trial 45 finished with value: 0.8684341073036194 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 124, 'dropout_1': 0.16922606086567982, 'optimizer': 'RMSprop', 'lstm_units_2': 41, 'dropout_2': 0.3547095321497227, 'lstm_units_3': 25, 'dropout_3': 0.41167435400427693}. Best is trial 41 with value: 0.8934928894042968.\n",
      "[I 2024-05-24 22:25:59,124] Trial 46 finished with value: 0.8547048449516297 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 127, 'dropout_1': 0.18158111190899018, 'optimizer': 'Adam', 'lstm_units_2': 28, 'dropout_2': 0.24962598655800358, 'lstm_units_3': 33, 'dropout_3': 0.4606273239092639}. Best is trial 41 with value: 0.8934928894042968.\n",
      "[I 2024-05-24 22:26:28,716] Trial 47 finished with value: 0.8749281764030457 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 115, 'dropout_1': 0.2351880712666359, 'optimizer': 'RMSprop', 'lstm_units_2': 32, 'dropout_2': 0.2800809957280613, 'lstm_units_3': 23, 'dropout_3': 0.45685785269642454}. Best is trial 41 with value: 0.8934928894042968.\n",
      "[I 2024-05-24 22:26:51,741] Trial 48 finished with value: 0.884585976600647 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 128, 'dropout_1': 0.3110053346255566, 'optimizer': 'RMSprop', 'lstm_units_2': 23, 'dropout_2': 0.3783991303852087}. Best is trial 41 with value: 0.8934928894042968.\n",
      "[I 2024-05-24 22:27:18,312] Trial 49 finished with value: 0.7958077549934387 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 121, 'dropout_1': 0.10551139835947218, 'optimizer': 'SGD', 'lstm_units_2': 84, 'dropout_2': 0.40469986030673444, 'lstm_units_3': 80, 'dropout_3': 0.11359531087640395}. Best is trial 41 with value: 0.8934928894042968.\n",
      "[I 2024-05-24 22:27:47,817] Trial 50 finished with value: 0.8507248163223267 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 107, 'dropout_1': 0.3476895207543953, 'optimizer': 'RMSprop', 'lstm_units_2': 20, 'dropout_2': 0.33759311133507786, 'lstm_units_3': 49, 'dropout_3': 0.414029769498794}. Best is trial 41 with value: 0.8934928894042968.\n",
      "[I 2024-05-24 22:28:10,563] Trial 51 finished with value: 0.8878379225730896 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 128, 'dropout_1': 0.28704728204259927, 'optimizer': 'RMSprop', 'lstm_units_2': 22, 'dropout_2': 0.3997875683978471}. Best is trial 41 with value: 0.8934928894042968.\n",
      "[I 2024-05-24 22:28:33,696] Trial 52 finished with value: 0.882176423072815 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 123, 'dropout_1': 0.2511825614632509, 'optimizer': 'RMSprop', 'lstm_units_2': 21, 'dropout_2': 0.4325904180230321}. Best is trial 41 with value: 0.8934928894042968.\n",
      "[I 2024-05-24 22:28:55,632] Trial 53 finished with value: 0.884595787525177 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 112, 'dropout_1': 0.2705502184476367, 'optimizer': 'RMSprop', 'lstm_units_2': 32, 'dropout_2': 0.3164579904799055}. Best is trial 41 with value: 0.8934928894042968.\n",
      "[I 2024-05-24 22:29:11,269] Trial 54 finished with value: 0.8280984878540039 and parameters: {'num_lstm_layers': 1, 'lstm_units_1': 118, 'dropout_1': 0.217378188082127, 'optimizer': 'RMSprop'}. Best is trial 41 with value: 0.8934928894042968.\n",
      "[I 2024-05-24 22:29:33,845] Trial 55 finished with value: 0.8878509759902954 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 128, 'dropout_1': 0.3553153940200813, 'optimizer': 'RMSprop', 'lstm_units_2': 16, 'dropout_2': 0.2309935330997584}. Best is trial 41 with value: 0.8934928894042968.\n",
      "[I 2024-05-24 22:29:55,530] Trial 56 finished with value: 0.821643590927124 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 50, 'dropout_1': 0.37396959600484536, 'optimizer': 'RMSprop', 'lstm_units_2': 27, 'dropout_2': 0.23076636756034694}. Best is trial 41 with value: 0.8934928894042968.\n",
      "[I 2024-05-24 22:30:18,426] Trial 57 finished with value: 0.8676407337188721 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 124, 'dropout_1': 0.43373084244500826, 'optimizer': 'RMSprop', 'lstm_units_2': 16, 'dropout_2': 0.40086198188689715}. Best is trial 41 with value: 0.8934928894042968.\n",
      "[I 2024-05-24 22:30:39,749] Trial 58 finished with value: 0.8644312381744385 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 128, 'dropout_1': 0.31312408044242596, 'optimizer': 'Adam', 'lstm_units_2': 42, 'dropout_2': 0.2883054086135565}. Best is trial 41 with value: 0.8934928894042968.\n",
      "[I 2024-05-24 22:31:02,458] Trial 59 finished with value: 0.8741021156311035 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 117, 'dropout_1': 0.34422105103564926, 'optimizer': 'RMSprop', 'lstm_units_2': 36, 'dropout_2': 0.16859637604893057}. Best is trial 41 with value: 0.8934928894042968.\n",
      "[I 2024-05-24 22:31:17,773] Trial 60 finished with value: 0.794224226474762 and parameters: {'num_lstm_layers': 1, 'lstm_units_1': 120, 'dropout_1': 0.28884117386395475, 'optimizer': 'RMSprop'}. Best is trial 41 with value: 0.8934928894042968.\n",
      "[I 2024-05-24 22:31:40,829] Trial 61 finished with value: 0.8667820215225219 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 125, 'dropout_1': 0.1571726037827853, 'optimizer': 'RMSprop', 'lstm_units_2': 19, 'dropout_2': 0.2577847910130725}. Best is trial 41 with value: 0.8934928894042968.\n",
      "[I 2024-05-24 22:32:02,909] Trial 62 finished with value: 0.8805635213851929 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 114, 'dropout_1': 0.40335800913332, 'optimizer': 'RMSprop', 'lstm_units_2': 16, 'dropout_2': 0.18322541219881303}. Best is trial 41 with value: 0.8934928894042968.\n",
      "[I 2024-05-24 22:32:31,771] Trial 63 finished with value: 0.8329437136650085 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 72, 'dropout_1': 0.13468561290202427, 'optimizer': 'RMSprop', 'lstm_units_2': 23, 'dropout_2': 0.25748251880310385, 'lstm_units_3': 66, 'dropout_3': 0.4699925749007916}. Best is trial 41 with value: 0.8934928894042968.\n",
      "[I 2024-05-24 22:33:01,964] Trial 64 finished with value: 0.869224238395691 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 128, 'dropout_1': 0.36221460126537697, 'optimizer': 'RMSprop', 'lstm_units_2': 29, 'dropout_2': 0.22520341378899322, 'lstm_units_3': 108, 'dropout_3': 0.3136185063478264}. Best is trial 41 with value: 0.8934928894042968.\n",
      "[I 2024-05-24 22:33:23,412] Trial 65 finished with value: 0.7860846161842346 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 25, 'dropout_1': 0.18288468452680684, 'optimizer': 'RMSprop', 'lstm_units_2': 101, 'dropout_2': 0.022312613963012762}. Best is trial 41 with value: 0.8934928894042968.\n",
      "[I 2024-05-24 22:33:53,116] Trial 66 finished with value: 0.8660474061965943 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 121, 'dropout_1': 0.4949225604830726, 'optimizer': 'RMSprop', 'lstm_units_2': 26, 'dropout_2': 0.4382998386331107, 'lstm_units_3': 23, 'dropout_3': 0.2325973331763917}. Best is trial 41 with value: 0.8934928894042968.\n",
      "[I 2024-05-24 22:34:19,321] Trial 67 finished with value: 0.7627334594726562 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 110, 'dropout_1': 0.24279505547240568, 'optimizer': 'SGD', 'lstm_units_2': 20, 'dropout_2': 0.3327353797922015, 'lstm_units_3': 30, 'dropout_3': 0.35338695870323217}. Best is trial 41 with value: 0.8934928894042968.\n",
      "[I 2024-05-24 22:34:49,559] Trial 68 finished with value: 0.879744017124176 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 125, 'dropout_1': 0.32711543724502595, 'optimizer': 'RMSprop', 'lstm_units_2': 33, 'dropout_2': 0.3026514731760155, 'lstm_units_3': 41, 'dropout_3': 0.49859667115890133}. Best is trial 41 with value: 0.8934928894042968.\n",
      "[I 2024-05-24 22:35:12,149] Trial 69 finished with value: 0.8708828568458558 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 102, 'dropout_1': 0.2827876043815045, 'optimizer': 'RMSprop', 'lstm_units_2': 25, 'dropout_2': 0.2153009196981503}. Best is trial 41 with value: 0.8934928894042968.\n",
      "[I 2024-05-24 22:35:42,383] Trial 70 finished with value: 0.8829894304275513 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 118, 'dropout_1': 0.1967034732670656, 'optimizer': 'RMSprop', 'lstm_units_2': 62, 'dropout_2': 0.2712070564260461, 'lstm_units_3': 23, 'dropout_3': 0.2602310581474857}. Best is trial 41 with value: 0.8934928894042968.\n",
      "[I 2024-05-24 22:36:04,210] Trial 71 finished with value: 0.8603891849517822 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 112, 'dropout_1': 0.26997067917486645, 'optimizer': 'RMSprop', 'lstm_units_2': 32, 'dropout_2': 0.3635129014933335}. Best is trial 41 with value: 0.8934928894042968.\n",
      "[I 2024-05-24 22:36:27,422] Trial 72 finished with value: 0.8789114594459534 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 122, 'dropout_1': 0.3005892438786203, 'optimizer': 'RMSprop', 'lstm_units_2': 29, 'dropout_2': 0.31363018910847035}. Best is trial 41 with value: 0.8934928894042968.\n",
      "[I 2024-05-24 22:36:50,143] Trial 73 finished with value: 0.8612054228782654 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 113, 'dropout_1': 0.22501213768485284, 'optimizer': 'RMSprop', 'lstm_units_2': 44, 'dropout_2': 0.3369682858733438}. Best is trial 41 with value: 0.8934928894042968.\n",
      "[I 2024-05-24 22:37:12,567] Trial 74 finished with value: 0.8773377299308777 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 106, 'dropout_1': 0.1681335910746983, 'optimizer': 'RMSprop', 'lstm_units_2': 19, 'dropout_2': 0.24833333466404617}. Best is trial 41 with value: 0.8934928894042968.\n",
      "[I 2024-05-24 22:37:35,700] Trial 75 finished with value: 0.8684504389762878 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 125, 'dropout_1': 0.20684383920613236, 'optimizer': 'RMSprop', 'lstm_units_2': 37, 'dropout_2': 0.28528436518628664}. Best is trial 41 with value: 0.8934928894042968.\n",
      "[I 2024-05-24 22:38:05,572] Trial 76 finished with value: 0.8684732913970947 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 117, 'dropout_1': 0.2727501571802121, 'optimizer': 'RMSprop', 'lstm_units_2': 22, 'dropout_2': 0.23684174070678748, 'lstm_units_3': 62, 'dropout_3': 0.05917899704094012}. Best is trial 41 with value: 0.8934928894042968.\n",
      "[I 2024-05-24 22:38:25,314] Trial 77 finished with value: 0.8216403245925903 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 59, 'dropout_1': 0.4140593514220409, 'optimizer': 'Adam', 'lstm_units_2': 50, 'dropout_2': 0.38872597142951526}. Best is trial 41 with value: 0.8934928894042968.\n",
      "[I 2024-05-24 22:38:55,016] Trial 78 finished with value: 0.8894345045089722 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 128, 'dropout_1': 0.2580347928619677, 'optimizer': 'RMSprop', 'lstm_units_2': 16, 'dropout_2': 0.3061617426114109, 'lstm_units_3': 20, 'dropout_3': 0.38065185720203976}. Best is trial 41 with value: 0.8934928894042968.\n",
      "[I 2024-05-24 22:39:21,196] Trial 79 finished with value: 0.7392810463905335 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 128, 'dropout_1': 0.13924062399555956, 'optimizer': 'SGD', 'lstm_units_2': 16, 'dropout_2': 0.26920138666106097, 'lstm_units_3': 20, 'dropout_3': 0.40298147580387256}. Best is trial 41 with value: 0.8934928894042968.\n",
      "[I 2024-05-24 22:39:51,023] Trial 80 finished with value: 0.8821731686592102 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 120, 'dropout_1': 0.2504184194993897, 'optimizer': 'RMSprop', 'lstm_units_2': 75, 'dropout_2': 0.18871172840020023, 'lstm_units_3': 31, 'dropout_3': 0.37943559982245034}. Best is trial 41 with value: 0.8934928894042968.\n",
      "[I 2024-05-24 22:40:20,894] Trial 81 finished with value: 0.8765182137489319 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 125, 'dropout_1': 0.26095989561188715, 'optimizer': 'RMSprop', 'lstm_units_2': 24, 'dropout_2': 0.3092611947214043, 'lstm_units_3': 20, 'dropout_3': 0.4420764599261315}. Best is trial 41 with value: 0.8934928894042968.\n",
      "[I 2024-05-24 22:40:50,969] Trial 82 finished with value: 0.8789506316184997 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 122, 'dropout_1': 0.32679662940059795, 'optimizer': 'RMSprop', 'lstm_units_2': 30, 'dropout_2': 0.34531564329988196, 'lstm_units_3': 37, 'dropout_3': 0.3917603430286368}. Best is trial 41 with value: 0.8934928894042968.\n",
      "[I 2024-05-24 22:41:21,396] Trial 83 finished with value: 0.8733087301254272 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 125, 'dropout_1': 0.2998184890055357, 'optimizer': 'RMSprop', 'lstm_units_2': 19, 'dropout_2': 0.3256739485353057, 'lstm_units_3': 16, 'dropout_3': 0.4205891911518751}. Best is trial 41 with value: 0.8934928894042968.\n",
      "[I 2024-05-24 22:41:51,531] Trial 84 finished with value: 0.8684406518936157 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 115, 'dropout_1': 0.21119267450276025, 'optimizer': 'RMSprop', 'lstm_units_2': 34, 'dropout_2': 0.29657692868861457, 'lstm_units_3': 77, 'dropout_3': 0.18133893237802334}. Best is trial 41 with value: 0.8934928894042968.\n",
      "[I 2024-05-24 22:42:21,184] Trial 85 finished with value: 0.8547374963760376 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 111, 'dropout_1': 0.22697159624100272, 'optimizer': 'RMSprop', 'lstm_units_2': 26, 'dropout_2': 0.3615307552040977, 'lstm_units_3': 28, 'dropout_3': 0.35933437825175846}. Best is trial 41 with value: 0.8934928894042968.\n",
      "[I 2024-05-24 22:42:50,949] Trial 86 finished with value: 0.8369563817977905 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 41, 'dropout_1': 0.18119000667986213, 'optimizer': 'RMSprop', 'lstm_units_2': 21, 'dropout_2': 0.2808092505010248, 'lstm_units_3': 21, 'dropout_3': 0.4765317923974491}. Best is trial 41 with value: 0.8934928894042968.\n",
      "[I 2024-05-24 22:43:13,747] Trial 87 finished with value: 0.8756987094879151 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 128, 'dropout_1': 0.24017524528514625, 'optimizer': 'RMSprop', 'lstm_units_2': 40, 'dropout_2': 0.26151844444656513}. Best is trial 41 with value: 0.8934928894042968.\n",
      "[I 2024-05-24 22:43:43,671] Trial 88 finished with value: 0.8700535416603088 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 120, 'dropout_1': 0.11466337671219826, 'optimizer': 'RMSprop', 'lstm_units_2': 18, 'dropout_2': 0.24293998793626012, 'lstm_units_3': 45, 'dropout_3': 0.4462525765126475}. Best is trial 41 with value: 0.8934928894042968.\n",
      "[I 2024-05-24 22:44:10,818] Trial 89 finished with value: 0.8716501235961914 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 108, 'dropout_1': 0.2583316184461696, 'optimizer': 'Adam', 'lstm_units_2': 30, 'dropout_2': 0.21336629642268604, 'lstm_units_3': 36, 'dropout_3': 0.320325302940637}. Best is trial 41 with value: 0.8934928894042968.\n",
      "[I 2024-05-24 22:44:33,881] Trial 90 finished with value: 0.8773540616035461 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 123, 'dropout_1': 0.28150927276072096, 'optimizer': 'RMSprop', 'lstm_units_2': 23, 'dropout_2': 0.3213041139044362}. Best is trial 41 with value: 0.8934928894042968.\n",
      "[I 2024-05-24 22:44:56,848] Trial 91 finished with value: 0.8781637668609619 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 126, 'dropout_1': 0.3054361219330092, 'optimizer': 'RMSprop', 'lstm_units_2': 16, 'dropout_2': 0.3806092458733691}. Best is trial 41 with value: 0.8934928894042968.\n",
      "[I 2024-05-24 22:45:19,394] Trial 92 finished with value: 0.8821796894073486 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 128, 'dropout_1': 0.31064901501628617, 'optimizer': 'RMSprop', 'lstm_units_2': 22, 'dropout_2': 0.4205076513975406}. Best is trial 41 with value: 0.8934928894042968.\n",
      "[I 2024-05-24 22:45:41,716] Trial 93 finished with value: 0.8474467873573304 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 116, 'dropout_1': 0.31942303783353476, 'optimizer': 'RMSprop', 'lstm_units_2': 27, 'dropout_2': 0.3728651270337134}. Best is trial 41 with value: 0.8934928894042968.\n",
      "[I 2024-05-24 22:46:04,703] Trial 94 finished with value: 0.8740988612174988 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 122, 'dropout_1': 0.35628480537112167, 'optimizer': 'RMSprop', 'lstm_units_2': 25, 'dropout_2': 0.34714480106055345}. Best is trial 41 with value: 0.8934928894042968.\n",
      "[I 2024-05-24 22:46:27,355] Trial 95 finished with value: 0.8765214800834655 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 119, 'dropout_1': 0.33292861137260693, 'optimizer': 'RMSprop', 'lstm_units_2': 18, 'dropout_2': 0.39479283358797085}. Best is trial 41 with value: 0.8934928894042968.\n",
      "[I 2024-05-24 22:46:50,228] Trial 96 finished with value: 0.8797603487968445 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 126, 'dropout_1': 0.2900173439863275, 'optimizer': 'RMSprop', 'lstm_units_2': 22, 'dropout_2': 0.002813858678174075}. Best is trial 41 with value: 0.8934928894042968.\n",
      "[I 2024-05-24 22:47:19,522] Trial 97 finished with value: 0.8547048449516297 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 83, 'dropout_1': 0.16753955190695285, 'optimizer': 'RMSprop', 'lstm_units_2': 31, 'dropout_2': 0.2869118931561606, 'lstm_units_3': 87, 'dropout_3': 0.3768174238644655}. Best is trial 41 with value: 0.8934928894042968.\n",
      "[I 2024-05-24 22:47:39,785] Trial 98 finished with value: 0.7675590991973877 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 103, 'dropout_1': 0.26751589207075566, 'optimizer': 'SGD', 'lstm_units_2': 34, 'dropout_2': 0.26151144459335984}. Best is trial 41 with value: 0.8934928894042968.\n",
      "[I 2024-05-24 22:48:10,091] Trial 99 finished with value: 0.8781441807746887 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 123, 'dropout_1': 0.15047324714754473, 'optimizer': 'RMSprop', 'lstm_units_2': 28, 'dropout_2': 0.30605352684394355, 'lstm_units_3': 98, 'dropout_3': 0.26475454914996654}. Best is trial 41 with value: 0.8934928894042968.\n",
      "[I 2024-05-24 22:48:33,373] Trial 100 finished with value: 0.8853891730308533 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 126, 'dropout_1': 0.2798803458169967, 'optimizer': 'RMSprop', 'lstm_units_2': 47, 'dropout_2': 0.4152740242425332}. Best is trial 41 with value: 0.8934928894042968.\n",
      "[I 2024-05-24 22:48:56,776] Trial 101 finished with value: 0.8869792461395264 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 126, 'dropout_1': 0.24741268378096035, 'optimizer': 'RMSprop', 'lstm_units_2': 46, 'dropout_2': 0.4693934069800374}. Best is trial 41 with value: 0.8934928894042968.\n",
      "[I 2024-05-24 22:49:20,034] Trial 102 finished with value: 0.8813569188117981 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 126, 'dropout_1': 0.23174564848603213, 'optimizer': 'RMSprop', 'lstm_units_2': 56, 'dropout_2': 0.4777010791152187}. Best is trial 41 with value: 0.8934928894042968.\n",
      "[I 2024-05-24 22:49:42,895] Trial 103 finished with value: 0.8829861521720886 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 119, 'dropout_1': 0.24474290972478518, 'optimizer': 'RMSprop', 'lstm_units_2': 43, 'dropout_2': 0.44609937188038595}. Best is trial 41 with value: 0.8934928894042968.\n",
      "[I 2024-05-24 22:50:05,890] Trial 104 finished with value: 0.8635725498199462 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 122, 'dropout_1': 0.27727794780392107, 'optimizer': 'RMSprop', 'lstm_units_2': 49, 'dropout_2': 0.4999343118924376}. Best is trial 41 with value: 0.8934928894042968.\n",
      "[I 2024-05-24 22:50:29,257] Trial 105 finished with value: 0.8789669513702393 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 124, 'dropout_1': 0.2163435865064809, 'optimizer': 'RMSprop', 'lstm_units_2': 45, 'dropout_2': 0.41914165210606347}. Best is trial 41 with value: 0.8934928894042968.\n",
      "[I 2024-05-24 22:50:59,405] Trial 106 finished with value: 0.8821797013282776 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 115, 'dropout_1': 0.25086325370659684, 'optimizer': 'RMSprop', 'lstm_units_2': 53, 'dropout_2': 0.4565218619772342, 'lstm_units_3': 28, 'dropout_3': 0.1401693889106046}. Best is trial 41 with value: 0.8934928894042968.\n",
      "[I 2024-05-24 22:51:22,262] Trial 107 finished with value: 0.8595729470252991 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 126, 'dropout_1': 0.19066412788995013, 'optimizer': 'RMSprop', 'lstm_units_2': 40, 'dropout_2': 0.22642606423680378}. Best is trial 41 with value: 0.8934928894042968.\n",
      "[I 2024-05-24 22:51:37,924] Trial 108 finished with value: 0.8063242793083191 and parameters: {'num_lstm_layers': 1, 'lstm_units_1': 128, 'dropout_1': 0.12903249451681767, 'optimizer': 'RMSprop'}. Best is trial 41 with value: 0.8934928894042968.\n",
      "[I 2024-05-24 22:51:58,322] Trial 109 finished with value: 0.8539212584495545 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 99, 'dropout_1': 0.07479481839934121, 'optimizer': 'Adam', 'lstm_units_2': 47, 'dropout_2': 0.4745428083843395}. Best is trial 41 with value: 0.8934928894042968.\n",
      "[I 2024-05-24 22:52:28,283] Trial 110 finished with value: 0.8700796723365783 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 118, 'dropout_1': 0.1881116565827666, 'optimizer': 'RMSprop', 'lstm_units_2': 37, 'dropout_2': 0.46517941988288797, 'lstm_units_3': 119, 'dropout_3': 0.43064872912164137}. Best is trial 41 with value: 0.8934928894042968.\n",
      "[I 2024-05-24 22:52:51,238] Trial 111 finished with value: 0.8749085664749146 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 128, 'dropout_1': 0.29057221937697403, 'optimizer': 'RMSprop', 'lstm_units_2': 18, 'dropout_2': 0.37168607024144407}. Best is trial 41 with value: 0.8934928894042968.\n",
      "[I 2024-05-24 22:53:14,444] Trial 112 finished with value: 0.8628346562385559 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 121, 'dropout_1': 0.2665612481525961, 'optimizer': 'RMSprop', 'lstm_units_2': 20, 'dropout_2': 0.41084462169344255}. Best is trial 41 with value: 0.8934928894042968.\n",
      "[I 2024-05-24 22:53:37,875] Trial 113 finished with value: 0.881347143650055 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 125, 'dropout_1': 0.28156885532347414, 'optimizer': 'RMSprop', 'lstm_units_2': 47, 'dropout_2': 0.3795198951691461}. Best is trial 41 with value: 0.8934928894042968.\n",
      "[I 2024-05-24 22:54:00,625] Trial 114 finished with value: 0.8797636151313781 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 124, 'dropout_1': 0.31379420370646705, 'optimizer': 'RMSprop', 'lstm_units_2': 16, 'dropout_2': 0.236330990905265}. Best is trial 41 with value: 0.8934928894042968.\n",
      "[I 2024-05-24 22:54:23,172] Trial 115 finished with value: 0.8716827630996704 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 120, 'dropout_1': 0.20142601508239438, 'optimizer': 'RMSprop', 'lstm_units_2': 24, 'dropout_2': 0.3949639198500536}. Best is trial 41 with value: 0.8934928894042968.\n",
      "[I 2024-05-24 22:54:46,334] Trial 116 finished with value: 0.8837860703468323 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 128, 'dropout_1': 0.3016955135613599, 'optimizer': 'RMSprop', 'lstm_units_2': 38, 'dropout_2': 0.4316444099268024}. Best is trial 41 with value: 0.8934928894042968.\n",
      "[I 2024-05-24 22:55:13,180] Trial 117 finished with value: 0.7691360831260681 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 123, 'dropout_1': 0.38125037139678286, 'optimizer': 'SGD', 'lstm_units_2': 35, 'dropout_2': 0.35549017589882104, 'lstm_units_3': 55, 'dropout_3': 0.20883584834430413}. Best is trial 41 with value: 0.8934928894042968.\n",
      "[I 2024-05-24 22:55:43,232] Trial 118 finished with value: 0.8765149474143982 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 117, 'dropout_1': 0.25541580083922066, 'optimizer': 'RMSprop', 'lstm_units_2': 18, 'dropout_2': 0.2512364328948997, 'lstm_units_3': 19, 'dropout_3': 0.39710138992917215}. Best is trial 41 with value: 0.8934928894042968.\n",
      "[I 2024-05-24 22:56:06,220] Trial 119 finished with value: 0.8635954141616822 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 126, 'dropout_1': 0.33602662346861517, 'optimizer': 'RMSprop', 'lstm_units_2': 26, 'dropout_2': 0.2766175208521855}. Best is trial 41 with value: 0.8934928894042968.\n",
      "[I 2024-05-24 22:56:35,059] Trial 120 finished with value: 0.8370118856430053 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 74, 'dropout_1': 0.32358034234512967, 'optimizer': 'RMSprop', 'lstm_units_2': 20, 'dropout_2': 0.3262862406229109, 'lstm_units_3': 91, 'dropout_3': 0.4878873425117019}. Best is trial 41 with value: 0.8934928894042968.\n",
      "[I 2024-05-24 22:56:57,930] Trial 121 finished with value: 0.8829796314239502 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 128, 'dropout_1': 0.30646192986405285, 'optimizer': 'RMSprop', 'lstm_units_2': 38, 'dropout_2': 0.431738296443185}. Best is trial 41 with value: 0.8934928894042968.\n",
      "[I 2024-05-24 22:57:21,163] Trial 122 finished with value: 0.8805570006370544 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 124, 'dropout_1': 0.29177194694814457, 'optimizer': 'RMSprop', 'lstm_units_2': 42, 'dropout_2': 0.48643735407251926}. Best is trial 41 with value: 0.8934928894042968.\n",
      "[I 2024-05-24 22:57:43,998] Trial 123 finished with value: 0.8958828568458557 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 126, 'dropout_1': 0.27488981852980054, 'optimizer': 'RMSprop', 'lstm_units_2': 40, 'dropout_2': 0.44415068512105377}. Best is trial 123 with value: 0.8958828568458557.\n",
      "[I 2024-05-24 22:58:06,921] Trial 124 finished with value: 0.8466468572616577 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 121, 'dropout_1': 0.2721248816046376, 'optimizer': 'RMSprop', 'lstm_units_2': 46, 'dropout_2': 0.4498327200152892}. Best is trial 123 with value: 0.8958828568458557.\n",
      "[I 2024-05-24 22:58:29,937] Trial 125 finished with value: 0.8765214681625366 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 126, 'dropout_1': 0.26020853178309244, 'optimizer': 'RMSprop', 'lstm_units_2': 22, 'dropout_2': 0.4055868878059914}. Best is trial 123 with value: 0.8958828568458557.\n",
      "[I 2024-05-24 22:58:53,062] Trial 126 finished with value: 0.8821797013282776 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 113, 'dropout_1': 0.23619666608765733, 'optimizer': 'RMSprop', 'lstm_units_2': 50, 'dropout_2': 0.29581005520419795}. Best is trial 123 with value: 0.8958828568458557.\n",
      "[I 2024-05-24 22:59:16,391] Trial 127 finished with value: 0.8716827630996704 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 122, 'dropout_1': 0.28479337203686544, 'optimizer': 'RMSprop', 'lstm_units_2': 82, 'dropout_2': 0.41406165388241223}. Best is trial 123 with value: 0.8958828568458557.\n",
      "[I 2024-05-24 22:59:46,643] Trial 128 finished with value: 0.897525143623352 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 118, 'dropout_1': 0.11042220207278841, 'optimizer': 'RMSprop', 'lstm_units_2': 128, 'dropout_2': 0.4433608498504585, 'lstm_units_3': 16, 'dropout_3': 0.3350524806969235}. Best is trial 128 with value: 0.897525143623352.\n",
      "[I 2024-05-24 23:00:17,203] Trial 129 finished with value: 0.8773475170135498 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 118, 'dropout_1': 0.11264813882380248, 'optimizer': 'RMSprop', 'lstm_units_2': 90, 'dropout_2': 0.4641801711554465, 'lstm_units_3': 16, 'dropout_3': 0.30762147765285736}. Best is trial 128 with value: 0.897525143623352.\n",
      "[I 2024-05-24 23:00:44,556] Trial 130 finished with value: 0.8643953204154968 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 112, 'dropout_1': 0.1601916445296901, 'optimizer': 'Adam', 'lstm_units_2': 102, 'dropout_2': 0.443006002048865, 'lstm_units_3': 25, 'dropout_3': 0.3386669350822363}. Best is trial 128 with value: 0.897525143623352.\n",
      "[I 2024-05-24 23:01:15,216] Trial 131 finished with value: 0.8748889923095703 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 126, 'dropout_1': 0.0879774172781742, 'optimizer': 'RMSprop', 'lstm_units_2': 126, 'dropout_2': 0.4265335785726469, 'lstm_units_3': 32, 'dropout_3': 0.3644270827742848}. Best is trial 128 with value: 0.897525143623352.\n",
      "[I 2024-05-24 23:01:44,853] Trial 132 finished with value: 0.8773377180099488 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 124, 'dropout_1': 0.14717246471465806, 'optimizer': 'RMSprop', 'lstm_units_2': 16, 'dropout_2': 0.20466587459069818, 'lstm_units_3': 21, 'dropout_3': 0.3510137440305044}. Best is trial 128 with value: 0.897525143623352.\n",
      "[I 2024-05-24 23:02:15,175] Trial 133 finished with value: 0.8732956647872925 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 128, 'dropout_1': 0.26724978286745577, 'optimizer': 'RMSprop', 'lstm_units_2': 113, 'dropout_2': 0.25478421481945795, 'lstm_units_3': 72, 'dropout_3': 0.3326457226348396}. Best is trial 128 with value: 0.897525143623352.\n",
      "[I 2024-05-24 23:02:45,255] Trial 134 finished with value: 0.8652148365974426 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 120, 'dropout_1': 0.05167192024573769, 'optimizer': 'RMSprop', 'lstm_units_2': 118, 'dropout_2': 0.46338199119167833, 'lstm_units_3': 26, 'dropout_3': 0.4125913735651078}. Best is trial 128 with value: 0.897525143623352.\n",
      "[I 2024-05-24 23:03:15,441] Trial 135 finished with value: 0.8668146729469299 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 115, 'dropout_1': 0.353017587914056, 'optimizer': 'RMSprop', 'lstm_units_2': 66, 'dropout_2': 0.2665239919125198, 'lstm_units_3': 19, 'dropout_3': 0.4545000424806279}. Best is trial 128 with value: 0.897525143623352.\n",
      "[I 2024-05-24 23:03:38,634] Trial 136 finished with value: 0.8765182256698608 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 123, 'dropout_1': 0.09843495473152997, 'optimizer': 'RMSprop', 'lstm_units_2': 53, 'dropout_2': 0.39327026714656854}. Best is trial 128 with value: 0.897525143623352.\n",
      "[I 2024-05-24 23:04:01,764] Trial 137 finished with value: 0.8853924393653869 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 126, 'dropout_1': 0.36644049680252155, 'optimizer': 'RMSprop', 'lstm_units_2': 42, 'dropout_2': 0.4894362825213009}. Best is trial 128 with value: 0.897525143623352.\n",
      "[I 2024-05-24 23:04:31,967] Trial 138 finished with value: 0.8418048858642578 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 117, 'dropout_1': 0.41152762371679696, 'optimizer': 'RMSprop', 'lstm_units_2': 44, 'dropout_2': 0.47998315857703994, 'lstm_units_3': 34, 'dropout_3': 0.27598600163212755}. Best is trial 128 with value: 0.897525143623352.\n",
      "[I 2024-05-24 23:04:54,974] Trial 139 finished with value: 0.8894606351852417 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 121, 'dropout_1': 0.3688716892820221, 'optimizer': 'RMSprop', 'lstm_units_2': 42, 'dropout_2': 0.4688584495303434}. Best is trial 128 with value: 0.897525143623352.\n",
      "[I 2024-05-24 23:05:15,581] Trial 140 finished with value: 0.8006791114807129 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 121, 'dropout_1': 0.3922548829551535, 'optimizer': 'SGD', 'lstm_units_2': 41, 'dropout_2': 0.49352752502178276}. Best is trial 128 with value: 0.897525143623352.\n",
      "[I 2024-05-24 23:05:38,787] Trial 141 finished with value: 0.8725022792816162 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 125, 'dropout_1': 0.3779724928630927, 'optimizer': 'RMSprop', 'lstm_units_2': 39, 'dropout_2': 0.46786552723269653}. Best is trial 128 with value: 0.897525143623352.\n",
      "[I 2024-05-24 23:06:01,762] Trial 142 finished with value: 0.888650918006897 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 119, 'dropout_1': 0.35647526871164814, 'optimizer': 'RMSprop', 'lstm_units_2': 43, 'dropout_2': 0.4382170036448211}. Best is trial 128 with value: 0.897525143623352.\n",
      "[I 2024-05-24 23:06:24,988] Trial 143 finished with value: 0.8749020457267761 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 126, 'dropout_1': 0.3671370340865552, 'optimizer': 'RMSprop', 'lstm_units_2': 56, 'dropout_2': 0.4516961235180636}. Best is trial 128 with value: 0.897525143623352.\n",
      "[I 2024-05-24 23:06:48,386] Trial 144 finished with value: 0.8716664552688599 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 123, 'dropout_1': 0.3644522931448994, 'optimizer': 'RMSprop', 'lstm_units_2': 72, 'dropout_2': 0.4903425369715795}. Best is trial 128 with value: 0.897525143623352.\n",
      "[I 2024-05-24 23:07:11,517] Trial 145 finished with value: 0.8692699432373047 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 119, 'dropout_1': 0.3488742331429111, 'optimizer': 'RMSprop', 'lstm_units_2': 49, 'dropout_2': 0.45706212862255874}. Best is trial 128 with value: 0.897525143623352.\n",
      "[I 2024-05-24 23:07:41,907] Trial 146 finished with value: 0.8749346971511841 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 126, 'dropout_1': 0.3892326156105976, 'optimizer': 'RMSprop', 'lstm_units_2': 43, 'dropout_2': 0.4731026212260045, 'lstm_units_3': 40, 'dropout_3': 0.29577352127763484}. Best is trial 128 with value: 0.897525143623352.\n",
      "[I 2024-05-24 23:08:04,893] Trial 147 finished with value: 0.8837730050086975 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 121, 'dropout_1': 0.1720203885433051, 'optimizer': 'RMSprop', 'lstm_units_2': 46, 'dropout_2': 0.4388404140690128}. Best is trial 128 with value: 0.897525143623352.\n",
      "[I 2024-05-24 23:08:28,021] Trial 148 finished with value: 0.8829796195030213 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 128, 'dropout_1': 0.12436452977069933, 'optimizer': 'RMSprop', 'lstm_units_2': 52, 'dropout_2': 0.24344825688596042}. Best is trial 128 with value: 0.897525143623352.\n",
      "[I 2024-05-24 23:08:50,816] Trial 149 finished with value: 0.8846121191978454 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 124, 'dropout_1': 0.3968764853719544, 'optimizer': 'RMSprop', 'lstm_units_2': 41, 'dropout_2': 0.4375671198093046}. Best is trial 128 with value: 0.897525143623352.\n",
      "[I 2024-05-24 23:09:13,713] Trial 150 finished with value: 0.8829894185066223 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 123, 'dropout_1': 0.3956850766535971, 'optimizer': 'RMSprop', 'lstm_units_2': 41, 'dropout_2': 0.44181577288167373}. Best is trial 128 with value: 0.897525143623352.\n",
      "[I 2024-05-24 23:09:29,133] Trial 151 finished with value: 0.8159984350204468 and parameters: {'num_lstm_layers': 1, 'lstm_units_1': 124, 'dropout_1': 0.3731570561600478, 'optimizer': 'RMSprop'}. Best is trial 128 with value: 0.897525143623352.\n",
      "[I 2024-05-24 23:09:57,672] Trial 152 finished with value: 0.8361727952957153 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 66, 'dropout_1': 0.3999129344360417, 'optimizer': 'RMSprop', 'lstm_units_2': 36, 'dropout_2': 0.4308382294049279, 'lstm_units_3': 80, 'dropout_3': 0.23814884421752713}. Best is trial 128 with value: 0.897525143623352.\n",
      "[I 2024-05-24 23:10:20,479] Trial 153 finished with value: 0.8725185990333557 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 126, 'dropout_1': 0.4374142011946929, 'optimizer': 'RMSprop', 'lstm_units_2': 44, 'dropout_2': 0.4236632634590074}. Best is trial 128 with value: 0.897525143623352.\n",
      "[I 2024-05-24 23:10:50,220] Trial 154 finished with value: 0.8684569597244263 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 119, 'dropout_1': 0.35788170288739185, 'optimizer': 'RMSprop', 'lstm_units_2': 42, 'dropout_2': 0.4568514086423786, 'lstm_units_3': 23, 'dropout_3': 0.43199334754460494}. Best is trial 128 with value: 0.897525143623352.\n",
      "[I 2024-05-24 23:11:19,869] Trial 155 finished with value: 0.862811803817749 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 128, 'dropout_1': 0.4377858927824922, 'optimizer': 'RMSprop', 'lstm_units_2': 40, 'dropout_2': 0.4843390935782919, 'lstm_units_3': 28, 'dropout_3': 0.3740188536826876}. Best is trial 128 with value: 0.897525143623352.\n",
      "[I 2024-05-24 23:11:42,727] Trial 156 finished with value: 0.8934765458106995 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 122, 'dropout_1': 0.338142522113704, 'optimizer': 'RMSprop', 'lstm_units_2': 48, 'dropout_2': 0.22274720977628928}. Best is trial 128 with value: 0.897525143623352.\n",
      "[I 2024-05-24 23:12:05,423] Trial 157 finished with value: 0.8724990129470825 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 116, 'dropout_1': 0.34478147583861296, 'optimizer': 'RMSprop', 'lstm_units_2': 47, 'dropout_2': 0.2167475081509507}. Best is trial 128 with value: 0.897525143623352.\n",
      "[I 2024-05-24 23:12:28,521] Trial 158 finished with value: 0.8934830904006958 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 122, 'dropout_1': 0.33505892992416725, 'optimizer': 'RMSprop', 'lstm_units_2': 58, 'dropout_2': 0.2257183666037318}. Best is trial 128 with value: 0.897525143623352.\n",
      "[I 2024-05-24 23:12:51,638] Trial 159 finished with value: 0.8684471726417542 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 121, 'dropout_1': 0.3358619303503878, 'optimizer': 'RMSprop', 'lstm_units_2': 59, 'dropout_2': 0.23328173169938624}. Best is trial 128 with value: 0.897525143623352.\n",
      "[I 2024-05-24 23:13:12,912] Trial 160 finished with value: 0.8749183654785156 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 118, 'dropout_1': 0.3424758932288886, 'optimizer': 'Adam', 'lstm_units_2': 49, 'dropout_2': 0.19350614364019902}. Best is trial 128 with value: 0.897525143623352.\n",
      "[I 2024-05-24 23:13:35,829] Trial 161 finished with value: 0.8821731686592102 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 124, 'dropout_1': 0.420627650638628, 'optimizer': 'RMSprop', 'lstm_units_2': 45, 'dropout_2': 0.22456279062151302}. Best is trial 128 with value: 0.897525143623352.\n",
      "[I 2024-05-24 23:13:59,139] Trial 162 finished with value: 0.8854185700416565 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 122, 'dropout_1': 0.36706371481642286, 'optimizer': 'RMSprop', 'lstm_units_2': 55, 'dropout_2': 0.44691640195681365}. Best is trial 128 with value: 0.897525143623352.\n",
      "[I 2024-05-24 23:14:22,002] Trial 163 finished with value: 0.8942666888237 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 122, 'dropout_1': 0.36677781254965164, 'optimizer': 'RMSprop', 'lstm_units_2': 56, 'dropout_2': 0.20375045988088353}. Best is trial 128 with value: 0.897525143623352.\n",
      "[I 2024-05-24 23:14:45,019] Trial 164 finished with value: 0.8587860703468323 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 121, 'dropout_1': 0.3708733074676025, 'optimizer': 'RMSprop', 'lstm_units_2': 60, 'dropout_2': 0.18033040917864954}. Best is trial 128 with value: 0.897525143623352.\n",
      "[I 2024-05-24 23:15:08,199] Trial 165 finished with value: 0.8741380572319031 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 122, 'dropout_1': 0.3601669463235461, 'optimizer': 'RMSprop', 'lstm_units_2': 56, 'dropout_2': 0.20773194631959008}. Best is trial 128 with value: 0.897525143623352.\n",
      "[I 2024-05-24 23:15:29,308] Trial 166 finished with value: 0.7973880171775818 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 32, 'dropout_1': 0.38107187453853575, 'optimizer': 'RMSprop', 'lstm_units_2': 63, 'dropout_2': 0.22967877563406028}. Best is trial 128 with value: 0.897525143623352.\n",
      "[I 2024-05-24 23:15:51,968] Trial 167 finished with value: 0.87169908285141 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 119, 'dropout_1': 0.3558214064705865, 'optimizer': 'RMSprop', 'lstm_units_2': 52, 'dropout_2': 0.20048128775201046}. Best is trial 128 with value: 0.897525143623352.\n",
      "[I 2024-05-24 23:16:13,758] Trial 168 finished with value: 0.8692601561546326 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 95, 'dropout_1': 0.3229489565522625, 'optimizer': 'RMSprop', 'lstm_units_2': 54, 'dropout_2': 0.24287328904510694}. Best is trial 128 with value: 0.897525143623352.\n",
      "[I 2024-05-24 23:16:35,743] Trial 169 finished with value: 0.8426472544670105 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 89, 'dropout_1': 0.3683086718558894, 'optimizer': 'RMSprop', 'lstm_units_2': 58, 'dropout_2': 0.4685549448005637}. Best is trial 128 with value: 0.897525143623352.\n",
      "[I 2024-05-24 23:16:58,625] Trial 170 finished with value: 0.8619988203048706 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 115, 'dropout_1': 0.3465674883901822, 'optimizer': 'RMSprop', 'lstm_units_2': 55, 'dropout_2': 0.2241092397598308}. Best is trial 128 with value: 0.897525143623352.\n",
      "[I 2024-05-24 23:17:21,564] Trial 171 finished with value: 0.901537811756134 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 126, 'dropout_1': 0.3341380439081693, 'optimizer': 'RMSprop', 'lstm_units_2': 48, 'dropout_2': 0.41652198990743566}. Best is trial 171 with value: 0.901537811756134.\n",
      "[I 2024-05-24 23:17:44,587] Trial 172 finished with value: 0.8676439881324768 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 126, 'dropout_1': 0.387347498205954, 'optimizer': 'RMSprop', 'lstm_units_2': 50, 'dropout_2': 0.4499682258911274}. Best is trial 171 with value: 0.901537811756134.\n",
      "[I 2024-05-24 23:18:07,336] Trial 173 finished with value: 0.8724794268608094 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 123, 'dropout_1': 0.3349946508822718, 'optimizer': 'RMSprop', 'lstm_units_2': 48, 'dropout_2': 0.21321449039543594}. Best is trial 171 with value: 0.901537811756134.\n",
      "[I 2024-05-24 23:18:30,462] Trial 174 finished with value: 0.8635692834854126 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 128, 'dropout_1': 0.3522151551354355, 'optimizer': 'RMSprop', 'lstm_units_2': 45, 'dropout_2': 0.24932824546006727}. Best is trial 171 with value: 0.901537811756134.\n",
      "[I 2024-05-24 23:18:53,275] Trial 175 finished with value: 0.8660245656967163 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 124, 'dropout_1': 0.3166649150781328, 'optimizer': 'RMSprop', 'lstm_units_2': 61, 'dropout_2': 0.4767654674933725}. Best is trial 171 with value: 0.901537811756134.\n",
      "[I 2024-05-24 23:19:13,467] Trial 176 finished with value: 0.8378020167350769 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 120, 'dropout_1': 0.3293849224322427, 'optimizer': 'SGD', 'lstm_units_2': 51, 'dropout_2': 0.4482534967556204}. Best is trial 171 with value: 0.901537811756134.\n",
      "[I 2024-05-24 23:19:36,405] Trial 177 finished with value: 0.8660670042037963 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 126, 'dropout_1': 0.36375022319051065, 'optimizer': 'RMSprop', 'lstm_units_2': 58, 'dropout_2': 0.4042545828505582}. Best is trial 171 with value: 0.901537811756134.\n",
      "[I 2024-05-24 23:19:59,422] Trial 178 finished with value: 0.8789636850357055 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 122, 'dropout_1': 0.3360109531717864, 'optimizer': 'RMSprop', 'lstm_units_2': 38, 'dropout_2': 0.4606821608510891}. Best is trial 171 with value: 0.901537811756134.\n",
      "[I 2024-05-24 23:20:22,114] Trial 179 finished with value: 0.8692471027374268 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 117, 'dropout_1': 0.35911268332889734, 'optimizer': 'RMSprop', 'lstm_units_2': 18, 'dropout_2': 0.14795589190821568}. Best is trial 171 with value: 0.901537811756134.\n",
      "[I 2024-05-24 23:20:37,603] Trial 180 finished with value: 0.8030821442604065 and parameters: {'num_lstm_layers': 1, 'lstm_units_1': 128, 'dropout_1': 0.10196037950109137, 'optimizer': 'RMSprop'}. Best is trial 171 with value: 0.901537811756134.\n",
      "[I 2024-05-24 23:21:00,461] Trial 181 finished with value: 0.8716925621032715 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 125, 'dropout_1': 0.27722514059402187, 'optimizer': 'RMSprop', 'lstm_units_2': 43, 'dropout_2': 0.42136747976744265}. Best is trial 171 with value: 0.901537811756134.\n",
      "[I 2024-05-24 23:21:23,394] Trial 182 finished with value: 0.879744005203247 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 126, 'dropout_1': 0.37996155162106926, 'optimizer': 'RMSprop', 'lstm_units_2': 47, 'dropout_2': 0.4169910914699253}. Best is trial 171 with value: 0.901537811756134.\n",
      "[I 2024-05-24 23:21:46,475] Trial 183 finished with value: 0.874128258228302 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 123, 'dropout_1': 0.2496978483788803, 'optimizer': 'RMSprop', 'lstm_units_2': 45, 'dropout_2': 0.4342304048665459}. Best is trial 171 with value: 0.901537811756134.\n",
      "[I 2024-05-24 23:22:09,296] Trial 184 finished with value: 0.8813471317291259 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 126, 'dropout_1': 0.13612649846104274, 'optimizer': 'RMSprop', 'lstm_units_2': 48, 'dropout_2': 0.41263868497436523}. Best is trial 171 with value: 0.901537811756134.\n",
      "[I 2024-05-24 23:22:32,258] Trial 185 finished with value: 0.8797799468040466 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 121, 'dropout_1': 0.34721699543375667, 'optimizer': 'RMSprop', 'lstm_units_2': 69, 'dropout_2': 0.23995521345084922}. Best is trial 171 with value: 0.901537811756134.\n",
      "[I 2024-05-24 23:22:55,032] Trial 186 finished with value: 0.8724924921989441 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 128, 'dropout_1': 0.3062334476985527, 'optimizer': 'RMSprop', 'lstm_units_2': 43, 'dropout_2': 0.2651671906015163}. Best is trial 171 with value: 0.901537811756134.\n",
      "[I 2024-05-24 23:23:18,050] Trial 187 finished with value: 0.865211570262909 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 124, 'dropout_1': 0.2955801083895609, 'optimizer': 'RMSprop', 'lstm_units_2': 39, 'dropout_2': 0.49867382618482525}. Best is trial 171 with value: 0.901537811756134.\n",
      "[I 2024-05-24 23:23:41,070] Trial 188 finished with value: 0.8821862220764161 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 120, 'dropout_1': 0.3680635801776135, 'optimizer': 'RMSprop', 'lstm_units_2': 52, 'dropout_2': 0.4466771207763978}. Best is trial 171 with value: 0.901537811756134.\n",
      "[I 2024-05-24 23:24:04,057] Trial 189 finished with value: 0.8886117219924927 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 125, 'dropout_1': 0.2621820125799791, 'optimizer': 'RMSprop', 'lstm_units_2': 36, 'dropout_2': 0.4268945614417803}. Best is trial 171 with value: 0.901537811756134.\n",
      "[I 2024-05-24 23:24:34,295] Trial 190 finished with value: 0.8757411479949951 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 122, 'dropout_1': 0.26095364093742096, 'optimizer': 'RMSprop', 'lstm_units_2': 33, 'dropout_2': 0.42833731493739735, 'lstm_units_3': 45, 'dropout_3': 0.2159227554012395}. Best is trial 171 with value: 0.901537811756134.\n",
      "[I 2024-05-24 23:24:57,644] Trial 191 finished with value: 0.8805635333061218 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 126, 'dropout_1': 0.2832639924925045, 'optimizer': 'RMSprop', 'lstm_units_2': 35, 'dropout_2': 0.408644744103887}. Best is trial 171 with value: 0.901537811756134.\n",
      "[I 2024-05-24 23:25:20,505] Trial 192 finished with value: 0.8942764878273011 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 128, 'dropout_1': 0.2415915562300286, 'optimizer': 'RMSprop', 'lstm_units_2': 40, 'dropout_2': 0.43715807815714924}. Best is trial 171 with value: 0.901537811756134.\n",
      "[I 2024-05-24 23:25:43,142] Trial 193 finished with value: 0.8805570006370544 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 128, 'dropout_1': 0.22794581238674216, 'optimizer': 'RMSprop', 'lstm_units_2': 41, 'dropout_2': 0.44264049470968964}. Best is trial 171 with value: 0.901537811756134.\n",
      "[I 2024-05-24 23:26:05,957] Trial 194 finished with value: 0.8676766395568848 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 124, 'dropout_1': 0.23878430118823102, 'optimizer': 'RMSprop', 'lstm_units_2': 37, 'dropout_2': 0.46624330128980945}. Best is trial 171 with value: 0.901537811756134.\n",
      "[I 2024-05-24 23:26:28,841] Trial 195 finished with value: 0.8797505497932434 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 125, 'dropout_1': 0.25694516415641255, 'optimizer': 'RMSprop', 'lstm_units_2': 40, 'dropout_2': 0.4532518357333063}. Best is trial 171 with value: 0.901537811756134.\n",
      "[I 2024-05-24 23:26:51,283] Trial 196 finished with value: 0.8862152218818664 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 128, 'dropout_1': 0.25129239373094786, 'optimizer': 'RMSprop', 'lstm_units_2': 16, 'dropout_2': 0.4353567273509049}. Best is trial 171 with value: 0.901537811756134.\n",
      "[I 2024-05-24 23:27:18,614] Trial 197 finished with value: 0.8498824715614319 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 128, 'dropout_1': 0.24599229269295209, 'optimizer': 'Adam', 'lstm_units_2': 16, 'dropout_2': 0.43621261946342216, 'lstm_units_3': 38, 'dropout_3': 0.33990144184821336}. Best is trial 171 with value: 0.901537811756134.\n",
      "[I 2024-05-24 23:27:41,151] Trial 198 finished with value: 0.8781507134437561 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 119, 'dropout_1': 0.21272804485606833, 'optimizer': 'RMSprop', 'lstm_units_2': 19, 'dropout_2': 0.4308066425539686}. Best is trial 171 with value: 0.901537811756134.\n",
      "[I 2024-05-24 23:28:04,319] Trial 199 finished with value: 0.875679099559784 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 122, 'dropout_1': 0.26972407525776343, 'optimizer': 'RMSprop', 'lstm_units_2': 18, 'dropout_2': 0.2551829663102462}. Best is trial 171 with value: 0.901537811756134.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'num_lstm_layers': 2, 'lstm_units_1': 126, 'dropout_1': 0.3341380439081693, 'optimizer': 'RMSprop', 'lstm_units_2': 48, 'dropout_2': 0.41652198990743566}\n"
     ]
    }
   ],
   "source": [
    "X_train_val = load('../../BEST SET/X_train_fft.joblib')\n",
    "y_train_val = load('../../BEST SET/y_Train.joblib')\n",
    "X_test = load('../../BEST SET/X_test_fft.joblib')\n",
    "y_test = load('../../BEST SET/y_Test.joblib') \n",
    "\n",
    "quarter = X_train_val[::4]\n",
    "quarter_labels = y_train_val[::4]\n",
    "second_qtr = X_train_val[1::4]\n",
    "second_qtr_labels = y_train_val[1::4]\n",
    "third_qtr = X_train_val[2::4]\n",
    "third_qtr_labels = y_train_val[2::4]\n",
    "fourth_qtr = X_train_val[3::4]\n",
    "fourth_qtr_labels = y_train_val[3::4]\n",
    "\n",
    "X_train_val = np.append(quarter, second_qtr, axis=0)\n",
    "X_train_val = np.append(X_train_val, third_qtr, axis=0)\n",
    "y_train_val = np.append(quarter_labels, second_qtr_labels, axis=0)\n",
    "y_train_val = np.append(y_train_val, third_qtr_labels, axis=0)\n",
    "X_test = np.append(X_test, fourth_qtr, axis=0)\n",
    "y_test = np.append(y_test, fourth_qtr_labels, axis=0)\n",
    "\n",
    "\n",
    "input_sh = X_train_val.shape\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Optuna Objective Function\n",
    "def objective(trial):\n",
    "    lstm_2 = 0\n",
    "    drop_2 = 0\n",
    "    lstm_3 = 0\n",
    "    drop_3 = 0\n",
    "    num_lstm = trial.suggest_int('num_lstm_layers', 1, 3)\n",
    "    lstm_1 = trial.suggest_int('lstm_units_1', 16, 128)\n",
    "    dropout_1 = trial.suggest_float('dropout_1', 0.0, 0.5)\n",
    "    optimizer = trial.suggest_categorical('optimizer', ['SGD', 'RMSprop', 'Adam'])\n",
    "\n",
    "    if num_lstm > 1:\n",
    "        lstm_2 = trial.suggest_int(f'lstm_units_2', 16, 128)\n",
    "        drop_2 = trial.suggest_float(f'dropout_2', 0.0, 0.5)\n",
    "    if num_lstm > 2:\n",
    "        lstm_3 = trial.suggest_int(f'lstm_units_3', 16, 128)\n",
    "        drop_3 = trial.suggest_float(f'dropout_3', 0.0, 0.5)\n",
    "\n",
    "\n",
    "    # Assuming StratifiedKFold, customize if needed\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)  \n",
    "    scores = []\n",
    "    for train_index, test_index in skf.split(X_train_val, y_train_val):\n",
    "        X_tr, X_val = X_train_val[train_index], X_train_val[test_index]\n",
    "        y_tr, y_val = y_train_val[train_index], y_train_val[test_index]\n",
    "\n",
    "        model = create_model(optimizer, num_lstm, lstm_1, dropout_1, lstm_2, drop_2, lstm_3, drop_3)\n",
    "        model.fit(X_tr, y_tr,epochs=20, verbose = 0) \n",
    "        score = model.evaluate(X_val, y_val, verbose = 0)\n",
    "        scores.append(score[1])\n",
    "    return np.array(scores).mean()\n",
    "\n",
    "# Hyperparameter Optimization\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=200)\n",
    "\n",
    "best_params = study.best_params\n",
    "print(\"Best hyperparameters:\", best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-24 23:28:04,614] A new study created in memory with name: no-name-caab8c09-db7b-4399-8a66-36f7b367e22d\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 18 calls to <function Model.make_test_function.<locals>.test_function at 0x000002E396DF51F0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x000002E38F1DBF70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-24 23:28:13,769] Trial 0 finished with value: 0.745724356174469 and parameters: {'num_lstm_layers': 1, 'lstm_units_1': 126, 'dropout_1': 0.21172412254961664, 'optimizer': 'RMSprop'}. Best is trial 0 with value: 0.745724356174469.\n",
      "[I 2024-05-24 23:28:33,243] Trial 1 finished with value: 0.8377607822418213 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 77, 'dropout_1': 0.2753449725798874, 'optimizer': 'RMSprop', 'lstm_units_2': 51, 'dropout_2': 0.3669421029662463, 'lstm_units_3': 115, 'dropout_3': 0.4033343386525616}. Best is trial 1 with value: 0.8377607822418213.\n",
      "[I 2024-05-24 23:28:41,823] Trial 2 finished with value: 0.753158974647522 and parameters: {'num_lstm_layers': 1, 'lstm_units_1': 77, 'dropout_1': 0.15482883417819576, 'optimizer': 'RMSprop'}. Best is trial 1 with value: 0.8377607822418213.\n",
      "[I 2024-05-24 23:28:50,053] Trial 3 finished with value: 0.656273889541626 and parameters: {'num_lstm_layers': 1, 'lstm_units_1': 51, 'dropout_1': 0.463373054563063, 'optimizer': 'Adam'}. Best is trial 1 with value: 0.8377607822418213.\n",
      "[I 2024-05-24 23:29:03,465] Trial 4 finished with value: 0.8473699688911438 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 83, 'dropout_1': 0.16737009360344063, 'optimizer': 'Adam', 'lstm_units_2': 127, 'dropout_2': 0.1728553137067736}. Best is trial 4 with value: 0.8473699688911438.\n",
      "[I 2024-05-24 23:29:11,462] Trial 5 finished with value: 0.6247428774833679 and parameters: {'num_lstm_layers': 1, 'lstm_units_1': 17, 'dropout_1': 0.06494247148490029, 'optimizer': 'SGD'}. Best is trial 4 with value: 0.8473699688911438.\n",
      "[I 2024-05-24 23:29:20,189] Trial 6 finished with value: 0.7386423587799072 and parameters: {'num_lstm_layers': 1, 'lstm_units_1': 94, 'dropout_1': 0.4796322176219998, 'optimizer': 'RMSprop'}. Best is trial 4 with value: 0.8473699688911438.\n",
      "[I 2024-05-24 23:29:38,588] Trial 7 finished with value: 0.7164854407310486 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 66, 'dropout_1': 0.48156884492682184, 'optimizer': 'SGD', 'lstm_units_2': 110, 'dropout_2': 0.1262622980668614, 'lstm_units_3': 32, 'dropout_3': 0.2124681676880869}. Best is trial 4 with value: 0.8473699688911438.\n",
      "[I 2024-05-24 23:29:47,025] Trial 8 finished with value: 0.7385542154312134 and parameters: {'num_lstm_layers': 1, 'lstm_units_1': 91, 'dropout_1': 0.38559789706114245, 'optimizer': 'Adam'}. Best is trial 4 with value: 0.8473699688911438.\n",
      "[I 2024-05-24 23:29:55,051] Trial 9 finished with value: 0.6273876070976258 and parameters: {'num_lstm_layers': 1, 'lstm_units_1': 38, 'dropout_1': 0.09283980890838983, 'optimizer': 'Adam'}. Best is trial 4 with value: 0.8473699688911438.\n",
      "[I 2024-05-24 23:30:09,120] Trial 10 finished with value: 0.8644431352615356 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 121, 'dropout_1': 0.001665343037601158, 'optimizer': 'Adam', 'lstm_units_2': 126, 'dropout_2': 0.031094309597689662}. Best is trial 10 with value: 0.8644431352615356.\n",
      "[I 2024-05-24 23:30:23,072] Trial 11 finished with value: 0.859712016582489 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 124, 'dropout_1': 0.00596325103611913, 'optimizer': 'Adam', 'lstm_units_2': 128, 'dropout_2': 0.0013395794765079932}. Best is trial 10 with value: 0.8644431352615356.\n",
      "[I 2024-05-24 23:30:36,892] Trial 12 finished with value: 0.8669703245162964 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 123, 'dropout_1': 0.0006433787835695183, 'optimizer': 'Adam', 'lstm_units_2': 97, 'dropout_2': 0.00541168453169014}. Best is trial 12 with value: 0.8669703245162964.\n",
      "[I 2024-05-24 23:30:50,649] Trial 13 finished with value: 0.8717895984649658 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 109, 'dropout_1': 0.003416564145085417, 'optimizer': 'Adam', 'lstm_units_2': 86, 'dropout_2': 0.0002709159065135587}. Best is trial 13 with value: 0.8717895984649658.\n",
      "[I 2024-05-24 23:31:04,193] Trial 14 finished with value: 0.8621804356575012 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 108, 'dropout_1': 0.08521939635865182, 'optimizer': 'Adam', 'lstm_units_2': 82, 'dropout_2': 0.0014204227941579145}. Best is trial 13 with value: 0.8717895984649658.\n",
      "[I 2024-05-24 23:31:22,668] Trial 15 finished with value: 0.8983837842941285 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 108, 'dropout_1': 0.2861612195130288, 'optimizer': 'Adam', 'lstm_units_2': 87, 'dropout_2': 0.47827142849929627, 'lstm_units_3': 128, 'dropout_3': 0.02650198528422537}. Best is trial 15 with value: 0.8983837842941285.\n",
      "[I 2024-05-24 23:31:40,764] Trial 16 finished with value: 0.6852777004241943 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 104, 'dropout_1': 0.3073426183770852, 'optimizer': 'SGD', 'lstm_units_2': 63, 'dropout_2': 0.47415851075574406, 'lstm_units_3': 127, 'dropout_3': 0.027884189414788606}. Best is trial 15 with value: 0.8983837842941285.\n",
      "[I 2024-05-24 23:31:59,318] Trial 17 finished with value: 0.8232735753059387 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 106, 'dropout_1': 0.35739570797201803, 'optimizer': 'Adam', 'lstm_units_2': 17, 'dropout_2': 0.3178132452003536, 'lstm_units_3': 78, 'dropout_3': 0.010357933067018155}. Best is trial 15 with value: 0.8983837842941285.\n",
      "[I 2024-05-24 23:32:17,716] Trial 18 finished with value: 0.8330884456634522 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 63, 'dropout_1': 0.21205268344214784, 'optimizer': 'Adam', 'lstm_units_2': 84, 'dropout_2': 0.48989801849110404, 'lstm_units_3': 82, 'dropout_3': 0.1983216837348775}. Best is trial 15 with value: 0.8983837842941285.\n",
      "[I 2024-05-24 23:32:31,161] Trial 19 finished with value: 0.7289450526237488 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 112, 'dropout_1': 0.3232450634022996, 'optimizer': 'SGD', 'lstm_units_2': 51, 'dropout_2': 0.23545928284765016}. Best is trial 15 with value: 0.8983837842941285.\n",
      "[I 2024-05-24 23:32:49,827] Trial 20 finished with value: 0.849838376045227 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 94, 'dropout_1': 0.40576482582754975, 'optimizer': 'Adam', 'lstm_units_2': 98, 'dropout_2': 0.35954097407304275, 'lstm_units_3': 17, 'dropout_3': 0.4857388362322612}. Best is trial 15 with value: 0.8983837842941285.\n",
      "[I 2024-05-24 23:33:03,799] Trial 21 finished with value: 0.8596532464027404 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 115, 'dropout_1': 0.051187904045855354, 'optimizer': 'Adam', 'lstm_units_2': 97, 'dropout_2': 0.09599054093804119}. Best is trial 15 with value: 0.8983837842941285.\n",
      "[I 2024-05-24 23:33:17,185] Trial 22 finished with value: 0.8258889198303223 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 102, 'dropout_1': 0.14309031161583077, 'optimizer': 'Adam', 'lstm_units_2': 78, 'dropout_2': 0.06841634412460788}. Best is trial 15 with value: 0.8983837842941285.\n",
      "[I 2024-05-24 23:33:30,974] Trial 23 finished with value: 0.8596238613128662 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 127, 'dropout_1': 0.11420447143764151, 'optimizer': 'Adam', 'lstm_units_2': 99, 'dropout_2': 0.17279836975608842}. Best is trial 15 with value: 0.8983837842941285.\n",
      "[I 2024-05-24 23:33:44,719] Trial 24 finished with value: 0.8838377952575683 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 116, 'dropout_1': 0.03763812045470649, 'optimizer': 'Adam', 'lstm_units_2': 63, 'dropout_2': 0.41203059638622797}. Best is trial 15 with value: 0.8983837842941285.\n",
      "[I 2024-05-24 23:34:03,079] Trial 25 finished with value: 0.8354393124580384 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 88, 'dropout_1': 0.22239104831009554, 'optimizer': 'Adam', 'lstm_units_2': 61, 'dropout_2': 0.4215678882762044, 'lstm_units_3': 98, 'dropout_3': 0.1233191967411002}. Best is trial 15 with value: 0.8983837842941285.\n",
      "[I 2024-05-24 23:34:16,815] Trial 26 finished with value: 0.84272700548172 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 99, 'dropout_1': 0.043254305112094625, 'optimizer': 'Adam', 'lstm_units_2': 66, 'dropout_2': 0.42209366694766404}. Best is trial 15 with value: 0.8983837842941285.\n",
      "[I 2024-05-24 23:34:30,479] Trial 27 finished with value: 0.8595650911331176 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 115, 'dropout_1': 0.2684363742931947, 'optimizer': 'Adam', 'lstm_units_2': 34, 'dropout_2': 0.28299825688563834}. Best is trial 15 with value: 0.8983837842941285.\n",
      "[I 2024-05-24 23:34:50,609] Trial 28 finished with value: 0.8571848154067994 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 117, 'dropout_1': 0.12263812766729427, 'optimizer': 'RMSprop', 'lstm_units_2': 87, 'dropout_2': 0.4304139664261428, 'lstm_units_3': 50, 'dropout_3': 0.33270103756562547}. Best is trial 15 with value: 0.8983837842941285.\n",
      "[I 2024-05-24 23:35:04,005] Trial 29 finished with value: 0.7529532790184021 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 97, 'dropout_1': 0.17667270076869737, 'optimizer': 'SGD', 'lstm_units_2': 73, 'dropout_2': 0.3701637132125503}. Best is trial 15 with value: 0.8983837842941285.\n",
      "[I 2024-05-24 23:35:24,040] Trial 30 finished with value: 0.8837790250778198 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 128, 'dropout_1': 0.23183184399788984, 'optimizer': 'RMSprop', 'lstm_units_2': 111, 'dropout_2': 0.4557444701135919, 'lstm_units_3': 58, 'dropout_3': 0.10949189862029507}. Best is trial 15 with value: 0.8983837842941285.\n",
      "[I 2024-05-24 23:35:44,035] Trial 31 finished with value: 0.8693505644798278 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 128, 'dropout_1': 0.24312730925024584, 'optimizer': 'RMSprop', 'lstm_units_2': 113, 'dropout_2': 0.4515934157660383, 'lstm_units_3': 57, 'dropout_3': 0.09966586318828108}. Best is trial 15 with value: 0.8983837842941285.\n",
      "[I 2024-05-24 23:36:04,113] Trial 32 finished with value: 0.8598001718521118 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 111, 'dropout_1': 0.3047017745832161, 'optimizer': 'RMSprop', 'lstm_units_2': 112, 'dropout_2': 0.3970786989446588, 'lstm_units_3': 55, 'dropout_3': 0.11251730283938952}. Best is trial 15 with value: 0.8983837842941285.\n",
      "[I 2024-05-24 23:36:24,103] Trial 33 finished with value: 0.8597707748413086 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 119, 'dropout_1': 0.041805233297710825, 'optimizer': 'RMSprop', 'lstm_units_2': 87, 'dropout_2': 0.4933998369005757, 'lstm_units_3': 95, 'dropout_3': 0.06769164769637268}. Best is trial 15 with value: 0.8983837842941285.\n",
      "[I 2024-05-24 23:36:38,410] Trial 34 finished with value: 0.8500440716743469 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 81, 'dropout_1': 0.1929709994872545, 'optimizer': 'RMSprop', 'lstm_units_2': 52, 'dropout_2': 0.3118032414079003}. Best is trial 15 with value: 0.8983837842941285.\n",
      "[I 2024-05-24 23:36:58,313] Trial 35 finished with value: 0.8596238613128662 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 107, 'dropout_1': 0.34170013272527167, 'optimizer': 'RMSprop', 'lstm_units_2': 108, 'dropout_2': 0.4551728978884188, 'lstm_units_3': 39, 'dropout_3': 0.2698382277613597}. Best is trial 15 with value: 0.8983837842941285.\n",
      "[I 2024-05-24 23:37:12,917] Trial 36 finished with value: 0.8378783583641052 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 128, 'dropout_1': 0.4435622723581815, 'optimizer': 'RMSprop', 'lstm_units_2': 72, 'dropout_2': 0.23438400219033784}. Best is trial 15 with value: 0.8983837842941285.\n",
      "[I 2024-05-24 23:37:31,738] Trial 37 finished with value: 0.8669115424156189 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 87, 'dropout_1': 0.26783249128819286, 'optimizer': 'Adam', 'lstm_units_2': 90, 'dropout_2': 0.35513337006877554, 'lstm_units_3': 67, 'dropout_3': 0.151634106946351}. Best is trial 15 with value: 0.8983837842941285.\n",
      "[I 2024-05-24 23:37:46,137] Trial 38 finished with value: 0.803996467590332 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 75, 'dropout_1': 0.08357994561439078, 'optimizer': 'RMSprop', 'lstm_units_2': 118, 'dropout_2': 0.39495845717657935}. Best is trial 15 with value: 0.8983837842941285.\n",
      "[I 2024-05-24 23:37:54,061] Trial 39 finished with value: 0.5860417246818542 and parameters: {'num_lstm_layers': 1, 'lstm_units_1': 22, 'dropout_1': 0.24018636245010028, 'optimizer': 'Adam'}. Best is trial 15 with value: 0.8983837842941285.\n",
      "[I 2024-05-24 23:38:12,094] Trial 40 finished with value: 0.7315016150474548 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 54, 'dropout_1': 0.02955653712430897, 'optimizer': 'SGD', 'lstm_units_2': 40, 'dropout_2': 0.19696395083163482, 'lstm_units_3': 97, 'dropout_3': 0.0037494322378830366}. Best is trial 15 with value: 0.8983837842941285.\n",
      "[I 2024-05-24 23:38:32,119] Trial 41 finished with value: 0.8811930656433106 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 128, 'dropout_1': 0.23764792333953907, 'optimizer': 'RMSprop', 'lstm_units_2': 118, 'dropout_2': 0.45350271785980883, 'lstm_units_3': 59, 'dropout_3': 0.07839295910094557}. Best is trial 15 with value: 0.8983837842941285.\n",
      "[I 2024-05-24 23:38:52,142] Trial 42 finished with value: 0.8548633575439453 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 121, 'dropout_1': 0.2891056356125066, 'optimizer': 'RMSprop', 'lstm_units_2': 104, 'dropout_2': 0.4553008295285996, 'lstm_units_3': 67, 'dropout_3': 0.06209245219484351}. Best is trial 15 with value: 0.8983837842941285.\n",
      "[I 2024-05-24 23:39:12,317] Trial 43 finished with value: 0.891125476360321 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 114, 'dropout_1': 0.1894849843666157, 'optimizer': 'RMSprop', 'lstm_units_2': 121, 'dropout_2': 0.3852972125891204, 'lstm_units_3': 43, 'dropout_3': 0.17122827174743377}. Best is trial 15 with value: 0.8983837842941285.\n",
      "[I 2024-05-24 23:39:32,034] Trial 44 finished with value: 0.8402585864067078 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 120, 'dropout_1': 0.20455322110254215, 'optimizer': 'RMSprop', 'lstm_units_2': 119, 'dropout_2': 0.3864753916941531, 'lstm_units_3': 44, 'dropout_3': 0.17183830217885485}. Best is trial 15 with value: 0.8983837842941285.\n",
      "[I 2024-05-24 23:39:51,678] Trial 45 finished with value: 0.8498971581459045 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 116, 'dropout_1': 0.17214583255219965, 'optimizer': 'RMSprop', 'lstm_units_2': 121, 'dropout_2': 0.4318088352956201, 'lstm_units_3': 27, 'dropout_3': 0.063947657266788}. Best is trial 15 with value: 0.8983837842941285.\n",
      "[I 2024-05-24 23:40:11,655] Trial 46 finished with value: 0.8669409394264221 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 123, 'dropout_1': 0.14813744738340562, 'optimizer': 'RMSprop', 'lstm_units_2': 128, 'dropout_2': 0.4706263254011063, 'lstm_units_3': 61, 'dropout_3': 0.25638008521595107}. Best is trial 15 with value: 0.8983837842941285.\n",
      "[I 2024-05-24 23:40:31,168] Trial 47 finished with value: 0.8574199199676513 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 112, 'dropout_1': 0.2530232802868798, 'optimizer': 'RMSprop', 'lstm_units_2': 110, 'dropout_2': 0.4926363403092221, 'lstm_units_3': 45, 'dropout_3': 0.14507455290378607}. Best is trial 15 with value: 0.8983837842941285.\n",
      "[I 2024-05-24 23:40:50,968] Trial 48 finished with value: 0.8691742658615113 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 102, 'dropout_1': 0.2259738467386057, 'optimizer': 'RMSprop', 'lstm_units_2': 120, 'dropout_2': 0.343299260499001, 'lstm_units_3': 78, 'dropout_3': 0.08063745923725019}. Best is trial 15 with value: 0.8983837842941285.\n",
      "[I 2024-05-24 23:41:11,444] Trial 49 finished with value: 0.8765207171440125 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 125, 'dropout_1': 0.289405595708232, 'optimizer': 'RMSprop', 'lstm_units_2': 115, 'dropout_2': 0.4124630849712363, 'lstm_units_3': 37, 'dropout_3': 0.04596358468749578}. Best is trial 15 with value: 0.8983837842941285.\n",
      "[I 2024-05-24 23:41:31,172] Trial 50 finished with value: 0.8017631411552429 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 38, 'dropout_1': 0.3705672855132307, 'optimizer': 'RMSprop', 'lstm_units_2': 104, 'dropout_2': 0.4518515392053802, 'lstm_units_3': 60, 'dropout_3': 0.2147872242421271}. Best is trial 15 with value: 0.8983837842941285.\n",
      "[I 2024-05-24 23:41:51,287] Trial 51 finished with value: 0.8596238613128662 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 123, 'dropout_1': 0.2899986187184824, 'optimizer': 'RMSprop', 'lstm_units_2': 116, 'dropout_2': 0.4088887440243876, 'lstm_units_3': 29, 'dropout_3': 0.048730766744787886}. Best is trial 15 with value: 0.8983837842941285.\n",
      "[I 2024-05-24 23:42:11,171] Trial 52 finished with value: 0.8595650792121887 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 128, 'dropout_1': 0.3268256722043479, 'optimizer': 'RMSprop', 'lstm_units_2': 124, 'dropout_2': 0.38071314287387814, 'lstm_units_3': 38, 'dropout_3': 0.10418698895644077}. Best is trial 15 with value: 0.8983837842941285.\n",
      "[I 2024-05-24 23:42:31,385] Trial 53 finished with value: 0.8668821573257446 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 118, 'dropout_1': 0.1962915162808752, 'optimizer': 'RMSprop', 'lstm_units_2': 93, 'dropout_2': 0.47277734403330857, 'lstm_units_3': 52, 'dropout_3': 0.043285385400762544}. Best is trial 15 with value: 0.8983837842941285.\n",
      "[I 2024-05-24 23:42:51,526] Trial 54 finished with value: 0.8523361802101135 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 113, 'dropout_1': 0.2549601508174304, 'optimizer': 'RMSprop', 'lstm_units_2': 105, 'dropout_2': 0.4357988020067317, 'lstm_units_3': 20, 'dropout_3': 0.13483247843649743}. Best is trial 15 with value: 0.8983837842941285.\n",
      "[I 2024-05-24 23:43:10,252] Trial 55 finished with value: 0.7435791850090027 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 124, 'dropout_1': 0.28557324462776223, 'optimizer': 'SGD', 'lstm_units_2': 126, 'dropout_2': 0.33311209858982976, 'lstm_units_3': 46, 'dropout_3': 0.08020306569348928}. Best is trial 15 with value: 0.8983837842941285.\n",
      "[I 2024-05-24 23:43:30,070] Trial 56 finished with value: 0.8669409275054931 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 108, 'dropout_1': 0.3129781158019503, 'optimizer': 'RMSprop', 'lstm_units_2': 114, 'dropout_2': 0.40873717893285844, 'lstm_units_3': 118, 'dropout_3': 0.02446086888759358}. Best is trial 15 with value: 0.8983837842941285.\n",
      "[I 2024-05-24 23:43:49,970] Trial 57 finished with value: 0.8887158393859863 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 119, 'dropout_1': 0.23220901610762942, 'optimizer': 'RMSprop', 'lstm_units_2': 55, 'dropout_2': 0.47882295834571587, 'lstm_units_3': 37, 'dropout_3': 0.17949277684195156}. Best is trial 15 with value: 0.8983837842941285.\n",
      "[I 2024-05-24 23:43:58,604] Trial 58 finished with value: 0.7386717557907104 and parameters: {'num_lstm_layers': 1, 'lstm_units_1': 98, 'dropout_1': 0.2258846133410777, 'optimizer': 'Adam'}. Best is trial 15 with value: 0.8983837842941285.\n",
      "[I 2024-05-24 23:44:18,295] Trial 59 finished with value: 0.8328827500343323 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 119, 'dropout_1': 0.18149054862102876, 'optimizer': 'RMSprop', 'lstm_units_2': 43, 'dropout_2': 0.47475407143724946, 'lstm_units_3': 68, 'dropout_3': 0.18084902373997086}. Best is trial 15 with value: 0.8983837842941285.\n",
      "[I 2024-05-24 23:44:31,895] Trial 60 finished with value: 0.8766382694244385 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 104, 'dropout_1': 0.15344433576818378, 'optimizer': 'Adam', 'lstm_units_2': 57, 'dropout_2': 0.4949942753403151}. Best is trial 15 with value: 0.8983837842941285.\n",
      "[I 2024-05-24 23:44:45,306] Trial 61 finished with value: 0.8644137501716613 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 104, 'dropout_1': 0.1522707020530289, 'optimizer': 'Adam', 'lstm_units_2': 54, 'dropout_2': 0.4786137384816182}. Best is trial 15 with value: 0.8983837842941285.\n",
      "[I 2024-05-24 23:44:59,375] Trial 62 finished with value: 0.8741992354393006 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 110, 'dropout_1': 0.13142092140522638, 'optimizer': 'Adam', 'lstm_units_2': 58, 'dropout_2': 0.44520334674032525}. Best is trial 15 with value: 0.8983837842941285.\n",
      "[I 2024-05-24 23:45:13,302] Trial 63 finished with value: 0.8645312905311584 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 115, 'dropout_1': 0.23093534958189407, 'optimizer': 'Adam', 'lstm_units_2': 69, 'dropout_2': 0.4967973380800519}. Best is trial 15 with value: 0.8983837842941285.\n",
      "[I 2024-05-24 23:45:26,779] Trial 64 finished with value: 0.8571554660797119 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 93, 'dropout_1': 0.2144564444376053, 'optimizer': 'Adam', 'lstm_units_2': 46, 'dropout_2': 0.4639718109747425}. Best is trial 15 with value: 0.8983837842941285.\n",
      "[I 2024-05-24 23:45:40,490] Trial 65 finished with value: 0.8477226018905639 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 106, 'dropout_1': 0.2633436128911181, 'optimizer': 'Adam', 'lstm_units_2': 57, 'dropout_2': 0.4976316097137071}. Best is trial 15 with value: 0.8983837842941285.\n",
      "[I 2024-05-24 23:45:58,844] Trial 66 finished with value: 0.6903026819229126 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 121, 'dropout_1': 0.19859021244970157, 'optimizer': 'SGD', 'lstm_units_2': 64, 'dropout_2': 0.4371637411977197, 'lstm_units_3': 33, 'dropout_3': 0.22149308301262477}. Best is trial 15 with value: 0.8983837842941285.\n",
      "[I 2024-05-24 23:46:17,707] Trial 67 finished with value: 0.8596532344818115 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 101, 'dropout_1': 0.09606582444968698, 'optimizer': 'Adam', 'lstm_units_2': 76, 'dropout_2': 0.48184121363688126, 'lstm_units_3': 87, 'dropout_3': 0.29726366598351206}. Best is trial 15 with value: 0.8983837842941285.\n",
      "[I 2024-05-24 23:46:31,645] Trial 68 finished with value: 0.8765501141548157 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 116, 'dropout_1': 0.16210239113605374, 'optimizer': 'Adam', 'lstm_units_2': 47, 'dropout_2': 0.4621931582735314}. Best is trial 15 with value: 0.8983837842941285.\n",
      "[I 2024-05-24 23:46:51,460] Trial 69 finished with value: 0.8281516194343567 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 65, 'dropout_1': 0.10550730483034731, 'optimizer': 'RMSprop', 'lstm_units_2': 66, 'dropout_2': 0.4410606630216483, 'lstm_units_3': 112, 'dropout_3': 0.1619841817836999}. Best is trial 15 with value: 0.8983837842941285.\n",
      "[I 2024-05-24 23:47:05,348] Trial 70 finished with value: 0.8499265313148499 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 124, 'dropout_1': 0.18332029605223846, 'optimizer': 'Adam', 'lstm_units_2': 80, 'dropout_2': 0.286931409651748}. Best is trial 15 with value: 0.8983837842941285.\n",
      "[I 2024-05-24 23:47:19,174] Trial 71 finished with value: 0.874140465259552 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 113, 'dropout_1': 0.16415365248446118, 'optimizer': 'Adam', 'lstm_units_2': 35, 'dropout_2': 0.46286439712942606}. Best is trial 15 with value: 0.8983837842941285.\n",
      "[I 2024-05-24 23:47:32,952] Trial 72 finished with value: 0.8352336287498474 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 117, 'dropout_1': 0.1346174206844156, 'optimizer': 'Adam', 'lstm_units_2': 48, 'dropout_2': 0.4291144690109288}. Best is trial 15 with value: 0.8983837842941285.\n",
      "[I 2024-05-24 23:47:47,044] Trial 73 finished with value: 0.8837202548980713 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 108, 'dropout_1': 0.24205107794654815, 'optimizer': 'Adam', 'lstm_units_2': 60, 'dropout_2': 0.48173056744197695}. Best is trial 15 with value: 0.8983837842941285.\n",
      "[I 2024-05-24 23:48:00,776] Trial 74 finished with value: 0.84272700548172 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 108, 'dropout_1': 0.23735395751643232, 'optimizer': 'Adam', 'lstm_units_2': 61, 'dropout_2': 0.4834901047165012}. Best is trial 15 with value: 0.8983837842941285.\n",
      "[I 2024-05-24 23:48:14,320] Trial 75 finished with value: 0.8669115543365479 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 96, 'dropout_1': 0.21154963076955147, 'optimizer': 'Adam', 'lstm_units_2': 55, 'dropout_2': 0.4970899701193826}. Best is trial 15 with value: 0.8983837842941285.\n",
      "[I 2024-05-24 23:48:28,819] Trial 76 finished with value: 0.8742286324501037 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 105, 'dropout_1': 0.06940238338389888, 'optimizer': 'RMSprop', 'lstm_units_2': 70, 'dropout_2': 0.4187500862743541}. Best is trial 15 with value: 0.8983837842941285.\n",
      "[I 2024-05-24 23:48:47,608] Trial 77 finished with value: 0.8427857756614685 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 110, 'dropout_1': 0.27901473121856823, 'optimizer': 'Adam', 'lstm_units_2': 58, 'dropout_2': 0.4525056396118235, 'lstm_units_3': 72, 'dropout_3': 0.23048099166096483}. Best is trial 15 with value: 0.8983837842941285.\n",
      "[I 2024-05-24 23:48:55,920] Trial 78 finished with value: 0.7726711750030517 and parameters: {'num_lstm_layers': 1, 'lstm_units_1': 126, 'dropout_1': 0.2543653555308021, 'optimizer': 'SGD'}. Best is trial 15 with value: 0.8983837842941285.\n",
      "[I 2024-05-24 23:49:10,526] Trial 79 finished with value: 0.845166027545929 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 87, 'dropout_1': 0.24469173279265652, 'optimizer': 'RMSprop', 'lstm_units_2': 61, 'dropout_2': 0.40569708348813605}. Best is trial 15 with value: 0.8983837842941285.\n",
      "[I 2024-05-24 23:49:29,412] Trial 80 finished with value: 0.8644431471824646 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 121, 'dropout_1': 0.27179706106475604, 'optimizer': 'Adam', 'lstm_units_2': 50, 'dropout_2': 0.38113718351333614, 'lstm_units_3': 23, 'dropout_3': 0.1824057847153495}. Best is trial 15 with value: 0.8983837842941285.\n",
      "[I 2024-05-24 23:49:43,279] Trial 81 finished with value: 0.8596532464027404 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 114, 'dropout_1': 0.18452484478345815, 'optimizer': 'Adam', 'lstm_units_2': 39, 'dropout_2': 0.46003408018253394}. Best is trial 15 with value: 0.8983837842941285.\n",
      "[I 2024-05-24 23:49:57,063] Trial 82 finished with value: 0.8765501141548157 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 117, 'dropout_1': 0.1655863787076084, 'optimizer': 'Adam', 'lstm_units_2': 29, 'dropout_2': 0.47014188427955744}. Best is trial 15 with value: 0.8983837842941285.\n",
      "[I 2024-05-24 23:50:10,747] Trial 83 finished with value: 0.8693211793899536 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 111, 'dropout_1': 0.20985025315676892, 'optimizer': 'Adam', 'lstm_units_2': 52, 'dropout_2': 0.48113180227381586}. Best is trial 15 with value: 0.8983837842941285.\n",
      "[I 2024-05-24 23:50:24,735] Trial 84 finished with value: 0.8643843650817871 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 101, 'dropout_1': 0.304546612531984, 'optimizer': 'Adam', 'lstm_units_2': 45, 'dropout_2': 0.4448608696404153}. Best is trial 15 with value: 0.8983837842941285.\n",
      "[I 2024-05-24 23:50:38,963] Trial 85 finished with value: 0.8427269935607911 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 70, 'dropout_1': 0.19174176984878014, 'optimizer': 'RMSprop', 'lstm_units_2': 122, 'dropout_2': 0.46414023448576275}. Best is trial 15 with value: 0.8983837842941285.\n",
      "[I 2024-05-24 23:50:58,118] Trial 86 finished with value: 0.8717308163642883 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 126, 'dropout_1': 0.49870895752158195, 'optimizer': 'Adam', 'lstm_units_2': 67, 'dropout_2': 0.4840535921778418, 'lstm_units_3': 126, 'dropout_3': 0.12531576033804065}. Best is trial 15 with value: 0.8983837842941285.\n",
      "[I 2024-05-24 23:51:12,736] Trial 87 finished with value: 0.862151050567627 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 121, 'dropout_1': 0.025097497414974523, 'optimizer': 'RMSprop', 'lstm_units_2': 75, 'dropout_2': 0.14156165706576485}. Best is trial 15 with value: 0.8983837842941285.\n",
      "[I 2024-05-24 23:51:31,546] Trial 88 finished with value: 0.8547458291053772 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 115, 'dropout_1': 0.21604050557708807, 'optimizer': 'Adam', 'lstm_units_2': 60, 'dropout_2': 0.4238969973685339, 'lstm_units_3': 49, 'dropout_3': 0.09676380132813833}. Best is trial 15 with value: 0.8983837842941285.\n",
      "[I 2024-05-24 23:51:45,921] Trial 89 finished with value: 0.8716720581054688 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 119, 'dropout_1': 0.23595910186126215, 'optimizer': 'RMSprop', 'lstm_units_2': 108, 'dropout_2': 0.44687011024323764}. Best is trial 15 with value: 0.8983837842941285.\n",
      "[I 2024-05-24 23:52:04,631] Trial 90 finished with value: 0.8572435975074768 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 104, 'dropout_1': 0.16460272616754393, 'optimizer': 'Adam', 'lstm_units_2': 55, 'dropout_2': 0.4012961288737473, 'lstm_units_3': 62, 'dropout_3': 0.38015679292179877}. Best is trial 15 with value: 0.8983837842941285.\n",
      "[I 2024-05-24 23:52:18,676] Trial 91 finished with value: 0.8692036390304565 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 117, 'dropout_1': 0.1618745235131992, 'optimizer': 'Adam', 'lstm_units_2': 28, 'dropout_2': 0.47001124431232416}. Best is trial 15 with value: 0.8983837842941285.\n",
      "[I 2024-05-24 23:52:32,519] Trial 92 finished with value: 0.8573023676872253 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 123, 'dropout_1': 0.1416622029886087, 'optimizer': 'Adam', 'lstm_units_2': 64, 'dropout_2': 0.4662422220735815}. Best is trial 15 with value: 0.8983837842941285.\n",
      "[I 2024-05-24 23:52:46,330] Trial 93 finished with value: 0.8789303541183472 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 112, 'dropout_1': 0.2627298732482548, 'optimizer': 'Adam', 'lstm_units_2': 117, 'dropout_2': 0.42685119381195985}. Best is trial 15 with value: 0.8983837842941285.\n",
      "[I 2024-05-24 23:53:00,106] Trial 94 finished with value: 0.8693799614906311 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 112, 'dropout_1': 0.260766512565595, 'optimizer': 'Adam', 'lstm_units_2': 101, 'dropout_2': 0.3706202438175661}. Best is trial 15 with value: 0.8983837842941285.\n",
      "[I 2024-05-24 23:53:14,787] Trial 95 finished with value: 0.8282103896141052 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 108, 'dropout_1': 0.2462110662988125, 'optimizer': 'RMSprop', 'lstm_units_2': 125, 'dropout_2': 0.43295156570561333}. Best is trial 15 with value: 0.8983837842941285.\n",
      "[I 2024-05-24 23:53:28,571] Trial 96 finished with value: 0.8524537086486816 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 128, 'dropout_1': 0.29771259764505903, 'optimizer': 'Adam', 'lstm_units_2': 49, 'dropout_2': 0.39146477059979506}. Best is trial 15 with value: 0.8983837842941285.\n",
      "[I 2024-05-24 23:53:49,016] Trial 97 finished with value: 0.8183955311775207 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 110, 'dropout_1': 0.319894736314352, 'optimizer': 'RMSprop', 'lstm_units_2': 118, 'dropout_2': 0.42199526204189564, 'lstm_units_3': 41, 'dropout_3': 0.4786738145241164}. Best is trial 15 with value: 0.8983837842941285.\n",
      "[I 2024-05-24 23:54:02,823] Trial 98 finished with value: 0.7409638643264771 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 114, 'dropout_1': 0.27921333710384394, 'optimizer': 'SGD', 'lstm_units_2': 111, 'dropout_2': 0.49843695361958534}. Best is trial 15 with value: 0.8983837842941285.\n",
      "[I 2024-05-24 23:54:22,112] Trial 99 finished with value: 0.8570672988891601 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 122, 'dropout_1': 0.22274695610231313, 'optimizer': 'Adam', 'lstm_units_2': 122, 'dropout_2': 0.06274327274768926, 'lstm_units_3': 53, 'dropout_3': 0.02294285682384716}. Best is trial 15 with value: 0.8983837842941285.\n",
      "[I 2024-05-24 23:54:41,750] Trial 100 finished with value: 0.852277398109436 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 55, 'dropout_1': 0.3463538213068562, 'optimizer': 'RMSprop', 'lstm_units_2': 92, 'dropout_2': 0.4503147630866956, 'lstm_units_3': 34, 'dropout_3': 0.19365348165516205}. Best is trial 15 with value: 0.8983837842941285.\n",
      "[I 2024-05-24 23:54:55,698] Trial 101 finished with value: 0.8765207171440125 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 117, 'dropout_1': 0.11716012435658202, 'optimizer': 'Adam', 'lstm_units_2': 116, 'dropout_2': 0.4836426834978327}. Best is trial 15 with value: 0.8983837842941285.\n",
      "[I 2024-05-24 23:55:09,689] Trial 102 finished with value: 0.8645019173622132 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 125, 'dropout_1': 0.19983290702896922, 'optimizer': 'Adam', 'lstm_units_2': 27, 'dropout_2': 0.4714121868829386}. Best is trial 15 with value: 0.8983837842941285.\n",
      "[I 2024-05-24 23:55:23,572] Trial 103 finished with value: 0.842844557762146 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 119, 'dropout_1': 0.17250862295877034, 'optimizer': 'Adam', 'lstm_units_2': 128, 'dropout_2': 0.4879974439730106}. Best is trial 15 with value: 0.8983837842941285.\n",
      "[I 2024-05-24 23:55:37,184] Trial 104 finished with value: 0.876579487323761 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 106, 'dropout_1': 0.22746766946718783, 'optimizer': 'Adam', 'lstm_units_2': 27, 'dropout_2': 0.461418651032955}. Best is trial 15 with value: 0.8983837842941285.\n",
      "[I 2024-05-24 23:55:50,727] Trial 105 finished with value: 0.8766676425933838 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 100, 'dropout_1': 0.23469262582781522, 'optimizer': 'Adam', 'lstm_units_2': 108, 'dropout_2': 0.4406149488412927}. Best is trial 15 with value: 0.8983837842941285.\n",
      "[I 2024-05-24 23:56:04,482] Trial 106 finished with value: 0.8450191020965576 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 100, 'dropout_1': 0.23603758643763664, 'optimizer': 'Adam', 'lstm_units_2': 107, 'dropout_2': 0.4153620607484359}. Best is trial 15 with value: 0.8983837842941285.\n",
      "[I 2024-05-24 23:56:18,342] Trial 107 finished with value: 0.8693505644798278 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 103, 'dropout_1': 0.2618746896986951, 'optimizer': 'Adam', 'lstm_units_2': 21, 'dropout_2': 0.43722914165834803}. Best is trial 15 with value: 0.8983837842941285.\n",
      "[I 2024-05-24 23:56:37,971] Trial 108 finished with value: 0.8862768292427063 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 96, 'dropout_1': 0.2225111949683776, 'optimizer': 'RMSprop', 'lstm_units_2': 100, 'dropout_2': 0.4496310978146966, 'lstm_units_3': 88, 'dropout_3': 0.09161413181190653}. Best is trial 15 with value: 0.8983837842941285.\n",
      "[I 2024-05-24 23:56:57,825] Trial 109 finished with value: 0.7847781419754029 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 99, 'dropout_1': 0.2204444133393722, 'optimizer': 'RMSprop', 'lstm_units_2': 102, 'dropout_2': 0.21177590943206537, 'lstm_units_3': 84, 'dropout_3': 0.09257817642310265}. Best is trial 15 with value: 0.8983837842941285.\n",
      "[I 2024-05-24 23:57:17,631] Trial 110 finished with value: 0.8355274558067322 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 83, 'dropout_1': 0.2515834040072357, 'optimizer': 'RMSprop', 'lstm_units_2': 111, 'dropout_2': 0.45203005981327377, 'lstm_units_3': 110, 'dropout_3': 0.14046664876103304}. Best is trial 15 with value: 0.8983837842941285.\n",
      "[I 2024-05-24 23:57:37,383] Trial 111 finished with value: 0.8620041131973266 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 96, 'dropout_1': 0.20682370872915895, 'optimizer': 'RMSprop', 'lstm_units_2': 95, 'dropout_2': 0.4288153751995911, 'lstm_units_3': 105, 'dropout_3': 0.000573755234989419}. Best is trial 15 with value: 0.8983837842941285.\n",
      "[I 2024-05-24 23:57:57,241] Trial 112 finished with value: 0.8573611378669739 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 92, 'dropout_1': 0.2333708468101611, 'optimizer': 'RMSprop', 'lstm_units_2': 113, 'dropout_2': 0.44118183307031356, 'lstm_units_3': 75, 'dropout_3': 0.1189294737943366}. Best is trial 15 with value: 0.8983837842941285.\n",
      "[I 2024-05-24 23:58:17,374] Trial 113 finished with value: 0.8620628833770752 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 107, 'dropout_1': 0.2701668457783585, 'optimizer': 'RMSprop', 'lstm_units_2': 118, 'dropout_2': 0.45823533739881306, 'lstm_units_3': 119, 'dropout_3': 0.0746594105742327}. Best is trial 15 with value: 0.8983837842941285.\n",
      "[I 2024-05-24 23:58:37,254] Trial 114 finished with value: 0.8161915898323059 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 104, 'dropout_1': 0.24628950098362204, 'optimizer': 'RMSprop', 'lstm_units_2': 85, 'dropout_2': 0.4133840546913266, 'lstm_units_3': 59, 'dropout_3': 0.15990736278823367}. Best is trial 15 with value: 0.8983837842941285.\n",
      "[I 2024-05-24 23:58:51,293] Trial 115 finished with value: 0.8644137382507324 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 106, 'dropout_1': 0.1884803231102316, 'optimizer': 'Adam', 'lstm_units_2': 100, 'dropout_2': 0.4783885143369359}. Best is trial 15 with value: 0.8983837842941285.\n",
      "[I 2024-05-24 23:59:10,267] Trial 116 finished with value: 0.8331472277641296 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 95, 'dropout_1': 0.22873045141691953, 'optimizer': 'Adam', 'lstm_units_2': 89, 'dropout_2': 0.49243783021958093, 'lstm_units_3': 91, 'dropout_3': 0.10887737018088722}. Best is trial 15 with value: 0.8983837842941285.\n",
      "[I 2024-05-24 23:59:24,812] Trial 117 finished with value: 0.8620628833770752 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 89, 'dropout_1': 0.2782696725263465, 'optimizer': 'RMSprop', 'lstm_units_2': 81, 'dropout_2': 0.3981739866433768}. Best is trial 15 with value: 0.8983837842941285.\n",
      "[I 2024-05-24 23:59:43,527] Trial 118 finished with value: 0.7241257667541504 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 109, 'dropout_1': 0.2040779425837786, 'optimizer': 'SGD', 'lstm_units_2': 107, 'dropout_2': 0.45753374694313936, 'lstm_units_3': 105, 'dropout_3': 0.0353055604519395}. Best is trial 15 with value: 0.8983837842941285.\n",
      "[I 2024-05-24 23:59:57,411] Trial 119 finished with value: 0.8545694947242737 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 98, 'dropout_1': 0.21892213797500482, 'optimizer': 'Adam', 'lstm_units_2': 116, 'dropout_2': 0.44237016322550343}. Best is trial 15 with value: 0.8983837842941285.\n",
      "[I 2024-05-25 00:00:17,528] Trial 120 finished with value: 0.8621216535568237 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 111, 'dropout_1': 0.25988393474573723, 'optimizer': 'RMSprop', 'lstm_units_2': 124, 'dropout_2': 0.4273873494741467, 'lstm_units_3': 65, 'dropout_3': 0.2022073500660629}. Best is trial 15 with value: 0.8983837842941285.\n",
      "[I 2024-05-25 00:00:31,725] Trial 121 finished with value: 0.869262421131134 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 115, 'dropout_1': 0.24456434219136097, 'optimizer': 'Adam', 'lstm_units_2': 16, 'dropout_2': 0.4649385589410275}. Best is trial 15 with value: 0.8983837842941285.\n",
      "[I 2024-05-25 00:00:45,556] Trial 122 finished with value: 0.8789891242980957 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 112, 'dropout_1': 0.2268355836615245, 'optimizer': 'Adam', 'lstm_units_2': 53, 'dropout_2': 0.4756702681436038}. Best is trial 15 with value: 0.8983837842941285.\n",
      "[I 2024-05-25 00:00:59,361] Trial 123 finished with value: 0.8644431352615356 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 102, 'dropout_1': 0.22766960276610124, 'optimizer': 'Adam', 'lstm_units_2': 54, 'dropout_2': 0.4894168365481042}. Best is trial 15 with value: 0.8983837842941285.\n",
      "[I 2024-05-25 00:01:13,238] Trial 124 finished with value: 0.8596826314926147 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 113, 'dropout_1': 0.4167418050792166, 'optimizer': 'Adam', 'lstm_units_2': 58, 'dropout_2': 0.4799021671672966}. Best is trial 15 with value: 0.8983837842941285.\n",
      "[I 2024-05-25 00:01:26,811] Trial 125 finished with value: 0.871701443195343 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 106, 'dropout_1': 0.29486061077593706, 'optimizer': 'Adam', 'lstm_units_2': 63, 'dropout_2': 0.4714926399766468}. Best is trial 15 with value: 0.8983837842941285.\n",
      "[I 2024-05-25 00:01:40,220] Trial 126 finished with value: 0.8621216535568237 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 108, 'dropout_1': 0.2114087206475657, 'optimizer': 'Adam', 'lstm_units_2': 70, 'dropout_2': 0.45135130915538574}. Best is trial 15 with value: 0.8983837842941285.\n",
      "[I 2024-05-25 00:01:53,874] Trial 127 finished with value: 0.8620628714561462 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 112, 'dropout_1': 0.06877936247875927, 'optimizer': 'Adam', 'lstm_units_2': 105, 'dropout_2': 0.44014515450813685}. Best is trial 15 with value: 0.8983837842941285.\n",
      "[I 2024-05-25 00:02:13,855] Trial 128 finished with value: 0.8596238613128662 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 119, 'dropout_1': 0.2539675908385223, 'optimizer': 'RMSprop', 'lstm_units_2': 97, 'dropout_2': 0.49050273793014787, 'lstm_units_3': 47, 'dropout_3': 0.05883100941380759}. Best is trial 15 with value: 0.8983837842941285.\n",
      "[I 2024-05-25 00:02:27,851] Trial 129 finished with value: 0.876579487323761 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 110, 'dropout_1': 0.2394902487376295, 'optimizer': 'Adam', 'lstm_units_2': 120, 'dropout_2': 0.4758715953562534}. Best is trial 15 with value: 0.8983837842941285.\n",
      "[I 2024-05-25 00:02:42,216] Trial 130 finished with value: 0.7870996236801148 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 24, 'dropout_1': 0.19754930963170902, 'optimizer': 'RMSprop', 'lstm_units_2': 43, 'dropout_2': 0.45837266231391693}. Best is trial 15 with value: 0.8983837842941285.\n",
      "[I 2024-05-25 00:02:56,198] Trial 131 finished with value: 0.8959447503089905 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 110, 'dropout_1': 0.23855156532011454, 'optimizer': 'Adam', 'lstm_units_2': 114, 'dropout_2': 0.479911138122465}. Best is trial 15 with value: 0.8983837842941285.\n",
      "[I 2024-05-25 00:03:09,937] Trial 132 finished with value: 0.8765795111656189 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 104, 'dropout_1': 0.22692221166572416, 'optimizer': 'Adam', 'lstm_units_2': 113, 'dropout_2': 0.49987987956244284}. Best is trial 15 with value: 0.8983837842941285.\n",
      "[I 2024-05-25 00:03:23,817] Trial 133 finished with value: 0.8452835798263549 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 103, 'dropout_1': 0.2635488492001607, 'optimizer': 'Adam', 'lstm_units_2': 110, 'dropout_2': 0.4960541745415716}. Best is trial 15 with value: 0.8983837842941285.\n",
      "[I 2024-05-25 00:03:37,529] Trial 134 finished with value: 0.8620041251182556 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 113, 'dropout_1': 0.2186972817062811, 'optimizer': 'Adam', 'lstm_units_2': 114, 'dropout_2': 0.4845712684845956}. Best is trial 15 with value: 0.8983837842941285.\n",
      "[I 2024-05-25 00:03:51,372] Trial 135 finished with value: 0.8595650911331176 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 101, 'dropout_1': 0.24874376498445447, 'optimizer': 'Adam', 'lstm_units_2': 118, 'dropout_2': 0.49783054548470124}. Best is trial 15 with value: 0.8983837842941285.\n",
      "[I 2024-05-25 00:04:10,325] Trial 136 finished with value: 0.8719365239143372 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 126, 'dropout_1': 0.2717431331394116, 'optimizer': 'Adam', 'lstm_units_2': 109, 'dropout_2': 0.4748799495670066, 'lstm_units_3': 41, 'dropout_3': 0.09003412993939777}. Best is trial 15 with value: 0.8983837842941285.\n",
      "[I 2024-05-25 00:04:24,190] Trial 137 finished with value: 0.8572436094284057 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 105, 'dropout_1': 0.23876222168782266, 'optimizer': 'Adam', 'lstm_units_2': 53, 'dropout_2': 0.44673851742143744}. Best is trial 15 with value: 0.8983837842941285.\n",
      "[I 2024-05-25 00:04:38,965] Trial 138 finished with value: 0.8596532464027404 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 109, 'dropout_1': 0.053574843867920226, 'optimizer': 'RMSprop', 'lstm_units_2': 113, 'dropout_2': 0.46937837105252433}. Best is trial 15 with value: 0.8983837842941285.\n",
      "[I 2024-05-25 00:04:53,270] Trial 139 finished with value: 0.8500734567642212 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 115, 'dropout_1': 0.20771630053310963, 'optimizer': 'Adam', 'lstm_units_2': 123, 'dropout_2': 0.29669850553484683}. Best is trial 15 with value: 0.8983837842941285.\n",
      "[I 2024-05-25 00:05:11,858] Trial 140 finished with value: 0.7072289228439331 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 124, 'dropout_1': 0.28136133055202645, 'optimizer': 'SGD', 'lstm_units_2': 103, 'dropout_2': 0.43330848247964804, 'lstm_units_3': 57, 'dropout_3': 0.24548292387789133}. Best is trial 15 with value: 0.8983837842941285.\n",
      "[I 2024-05-25 00:05:26,210] Trial 141 finished with value: 0.8621216535568237 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 108, 'dropout_1': 0.22601115562075721, 'optimizer': 'Adam', 'lstm_units_2': 56, 'dropout_2': 0.46017485557478743}. Best is trial 15 with value: 0.8983837842941285.\n",
      "[I 2024-05-25 00:05:40,166] Trial 142 finished with value: 0.8668527841567993 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 105, 'dropout_1': 0.018239294459708955, 'optimizer': 'Adam', 'lstm_units_2': 116, 'dropout_2': 0.4998863975974015}. Best is trial 15 with value: 0.8983837842941285.\n",
      "[I 2024-05-25 00:05:53,958] Trial 143 finished with value: 0.8523067831993103 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 100, 'dropout_1': 0.23179099625603622, 'optimizer': 'Adam', 'lstm_units_2': 60, 'dropout_2': 0.48466658819527947}. Best is trial 15 with value: 0.8983837842941285.\n",
      "[I 2024-05-25 00:06:07,845] Trial 144 finished with value: 0.8645606875419617 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 111, 'dropout_1': 0.2541775074824154, 'optimizer': 'Adam', 'lstm_units_2': 67, 'dropout_2': 0.4227155572132091}. Best is trial 15 with value: 0.8983837842941285.\n",
      "[I 2024-05-25 00:06:21,711] Trial 145 finished with value: 0.857273006439209 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 107, 'dropout_1': 0.21652891111975156, 'optimizer': 'Adam', 'lstm_units_2': 119, 'dropout_2': 0.46573366306950925}. Best is trial 15 with value: 0.8983837842941285.\n",
      "[I 2024-05-25 00:06:35,791] Trial 146 finished with value: 0.8523067831993103 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 120, 'dropout_1': 0.24220208245154218, 'optimizer': 'Adam', 'lstm_units_2': 51, 'dropout_2': 0.4521784381998451}. Best is trial 15 with value: 0.8983837842941285.\n",
      "[I 2024-05-25 00:06:50,399] Trial 147 finished with value: 0.8331472158432007 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 103, 'dropout_1': 0.1781002884672922, 'optimizer': 'RMSprop', 'lstm_units_2': 112, 'dropout_2': 0.4795653193264433}. Best is trial 15 with value: 0.8983837842941285.\n",
      "[I 2024-05-25 00:07:09,336] Trial 148 finished with value: 0.8426976203918457 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 117, 'dropout_1': 0.2257040832012289, 'optimizer': 'Adam', 'lstm_units_2': 106, 'dropout_2': 0.4886378006693669, 'lstm_units_3': 79, 'dropout_3': 0.12931434458551141}. Best is trial 15 with value: 0.8983837842941285.\n",
      "[I 2024-05-25 00:07:24,167] Trial 149 finished with value: 0.8255950570106506 and parameters: {'num_lstm_layers': 2, 'lstm_units_1': 113, 'dropout_1': 0.20206400068539734, 'optimizer': 'RMSprop', 'lstm_units_2': 121, 'dropout_2': 0.40754549728332295}. Best is trial 15 with value: 0.8983837842941285.\n",
      "[I 2024-05-25 00:07:43,033] Trial 150 finished with value: 0.8813693761825562 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 128, 'dropout_1': 0.2357143473217924, 'optimizer': 'Adam', 'lstm_units_2': 115, 'dropout_2': 0.4690270461605044, 'lstm_units_3': 54, 'dropout_3': 0.14696454271963302}. Best is trial 15 with value: 0.8983837842941285.\n",
      "[I 2024-05-25 00:08:02,324] Trial 151 finished with value: 0.8572436213493347 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 126, 'dropout_1': 0.23470103056325897, 'optimizer': 'Adam', 'lstm_units_2': 115, 'dropout_2': 0.46480980597266736, 'lstm_units_3': 54, 'dropout_3': 0.1454442860422698}. Best is trial 15 with value: 0.8983837842941285.\n",
      "[I 2024-05-25 00:08:11,096] Trial 152 finished with value: 0.7290038347244263 and parameters: {'num_lstm_layers': 1, 'lstm_units_1': 123, 'dropout_1': 0.2492184571493275, 'optimizer': 'Adam'}. Best is trial 15 with value: 0.8983837842941285.\n",
      "[I 2024-05-25 00:08:30,024] Trial 153 finished with value: 0.8644137620925904 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 122, 'dropout_1': 0.26480992623774624, 'optimizer': 'Adam', 'lstm_units_2': 78, 'dropout_2': 0.4732302410306421, 'lstm_units_3': 50, 'dropout_3': 0.16946317896290242}. Best is trial 15 with value: 0.8983837842941285.\n",
      "[I 2024-05-25 00:08:48,785] Trial 154 finished with value: 0.8886864662170411 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 128, 'dropout_1': 0.2182371802103726, 'optimizer': 'Adam', 'lstm_units_2': 116, 'dropout_2': 0.4470244521100194, 'lstm_units_3': 64, 'dropout_3': 0.11442770996172716}. Best is trial 15 with value: 0.8983837842941285.\n",
      "[I 2024-05-25 00:09:07,686] Trial 155 finished with value: 0.8570966720581055 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 128, 'dropout_1': 0.19076638688693057, 'optimizer': 'Adam', 'lstm_units_2': 111, 'dropout_2': 0.43720950359579014, 'lstm_units_3': 64, 'dropout_3': 0.11231112936510312}. Best is trial 15 with value: 0.8983837842941285.\n",
      "[I 2024-05-25 00:09:26,735] Trial 156 finished with value: 0.8620334982872009 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 128, 'dropout_1': 0.2122468385877679, 'optimizer': 'Adam', 'lstm_units_2': 116, 'dropout_2': 0.24668568440167774, 'lstm_units_3': 71, 'dropout_3': 0.0683131853853605}. Best is trial 15 with value: 0.8983837842941285.\n",
      "[I 2024-05-25 00:09:46,943] Trial 157 finished with value: 0.8549515128135681 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 125, 'dropout_1': 0.24167188436470255, 'optimizer': 'RMSprop', 'lstm_units_2': 109, 'dropout_2': 0.44865607936428903, 'lstm_units_3': 58, 'dropout_3': 0.15558246877275833}. Best is trial 15 with value: 0.8983837842941285.\n",
      "[I 2024-05-25 00:10:06,092] Trial 158 finished with value: 0.874022924900055 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 128, 'dropout_1': 0.22134463205388444, 'optimizer': 'Adam', 'lstm_units_2': 114, 'dropout_2': 0.42336932170177033, 'lstm_units_3': 42, 'dropout_3': 0.01790497563214634}. Best is trial 15 with value: 0.8983837842941285.\n",
      "[I 2024-05-25 00:10:25,114] Trial 159 finished with value: 0.8716132760047912 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 118, 'dropout_1': 0.258831834537481, 'optimizer': 'Adam', 'lstm_units_2': 126, 'dropout_2': 0.47827377171587326, 'lstm_units_3': 29, 'dropout_3': 0.08173169732867952}. Best is trial 15 with value: 0.8983837842941285.\n",
      "[I 2024-05-25 00:10:45,190] Trial 160 finished with value: 0.8667940020561218 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 76, 'dropout_1': 0.2320546815483189, 'optimizer': 'RMSprop', 'lstm_units_2': 118, 'dropout_2': 0.4543921772045394, 'lstm_units_3': 68, 'dropout_3': 0.056209736498598104}. Best is trial 15 with value: 0.8983837842941285.\n",
      "[I 2024-05-25 00:11:04,253] Trial 161 finished with value: 0.8643843650817871 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 110, 'dropout_1': 0.23005157513926097, 'optimizer': 'Adam', 'lstm_units_2': 21, 'dropout_2': 0.45920070104066785, 'lstm_units_3': 35, 'dropout_3': 0.13104852609671408}. Best is trial 15 with value: 0.8983837842941285.\n",
      "[I 2024-05-25 00:11:23,385] Trial 162 finished with value: 0.9031442880630494 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 106, 'dropout_1': 0.21929710484714846, 'optimizer': 'Adam', 'lstm_units_2': 63, 'dropout_2': 0.441277199407825, 'lstm_units_3': 51, 'dropout_3': 0.1841939196159818}. Best is trial 162 with value: 0.9031442880630494.\n",
      "[I 2024-05-25 00:11:42,692] Trial 163 finished with value: 0.8838084101676941 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 121, 'dropout_1': 0.20630754990855693, 'optimizer': 'Adam', 'lstm_units_2': 58, 'dropout_2': 0.017466741744087577, 'lstm_units_3': 55, 'dropout_3': 0.18833237751943582}. Best is trial 162 with value: 0.9031442880630494.\n",
      "[I 2024-05-25 00:12:01,862] Trial 164 finished with value: 0.8524537086486816 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 122, 'dropout_1': 0.1966611085500284, 'optimizer': 'Adam', 'lstm_units_2': 62, 'dropout_2': 0.1674173818866571, 'lstm_units_3': 55, 'dropout_3': 0.18332710844538236}. Best is trial 162 with value: 0.9031442880630494.\n",
      "[I 2024-05-25 00:12:20,851] Trial 165 finished with value: 0.8789009690284729 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 120, 'dropout_1': 0.18247377964636474, 'optimizer': 'Adam', 'lstm_units_2': 59, 'dropout_2': 0.010268175900586117, 'lstm_units_3': 50, 'dropout_3': 0.19869770057796823}. Best is trial 162 with value: 0.9031442880630494.\n",
      "[I 2024-05-25 00:12:39,929] Trial 166 finished with value: 0.8403173685073853 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 120, 'dropout_1': 0.21411180270402788, 'optimizer': 'Adam', 'lstm_units_2': 65, 'dropout_2': 0.4323544766430465, 'lstm_units_3': 50, 'dropout_3': 0.2010963170709908}. Best is trial 162 with value: 0.9031442880630494.\n",
      "[I 2024-05-25 00:12:59,042] Trial 167 finished with value: 0.8304731130599976 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 125, 'dropout_1': 0.187577513540925, 'optimizer': 'Adam', 'lstm_units_2': 60, 'dropout_2': 0.004841144887389312, 'lstm_units_3': 46, 'dropout_3': 0.1758669099058737}. Best is trial 162 with value: 0.9031442880630494.\n",
      "[I 2024-05-25 00:13:19,075] Trial 168 finished with value: 0.8717602133750916 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 116, 'dropout_1': 0.2068524591270969, 'optimizer': 'RMSprop', 'lstm_units_2': 56, 'dropout_2': 0.03089763503065376, 'lstm_units_3': 62, 'dropout_3': 0.22098358585043232}. Best is trial 162 with value: 0.9031442880630494.\n",
      "[I 2024-05-25 00:13:37,756] Trial 169 finished with value: 0.8549221277236938 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 122, 'dropout_1': 0.24644300964939117, 'optimizer': 'Adam', 'lstm_units_2': 58, 'dropout_2': 0.04477176577124977, 'lstm_units_3': 52, 'dropout_3': 0.19154221927606777}. Best is trial 162 with value: 0.9031442880630494.\n",
      "[I 2024-05-25 00:13:56,298] Trial 170 finished with value: 0.6977372884750366 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 119, 'dropout_1': 0.2182920143445381, 'optimizer': 'SGD', 'lstm_units_2': 63, 'dropout_2': 0.02116047413182067, 'lstm_units_3': 58, 'dropout_3': 0.15222505418034107}. Best is trial 162 with value: 0.9031442880630494.\n",
      "[I 2024-05-25 00:14:15,382] Trial 171 finished with value: 0.8860711097717285 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 114, 'dropout_1': 0.17955921165492483, 'optimizer': 'Adam', 'lstm_units_2': 59, 'dropout_2': 0.4123188080171716, 'lstm_units_3': 44, 'dropout_3': 0.2081088953514296}. Best is trial 162 with value: 0.9031442880630494.\n",
      "[I 2024-05-25 00:14:34,471] Trial 172 finished with value: 0.8693211793899536 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 115, 'dropout_1': 0.1835679643512995, 'optimizer': 'Adam', 'lstm_units_2': 60, 'dropout_2': 0.09682398566037914, 'lstm_units_3': 45, 'dropout_3': 0.240041748828187}. Best is trial 162 with value: 0.9031442880630494.\n",
      "[I 2024-05-25 00:14:53,596] Trial 173 finished with value: 0.8766676425933838 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 124, 'dropout_1': 0.1991191027639767, 'optimizer': 'Adam', 'lstm_units_2': 53, 'dropout_2': 0.38113051813029536, 'lstm_units_3': 49, 'dropout_3': 0.2112171805578211}. Best is trial 162 with value: 0.9031442880630494.\n",
      "[I 2024-05-25 00:15:12,605] Trial 174 finished with value: 0.8618571758270264 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 113, 'dropout_1': 0.17480151076966247, 'optimizer': 'Adam', 'lstm_units_2': 56, 'dropout_2': 0.011706673881550167, 'lstm_units_3': 55, 'dropout_3': 0.170088628190148}. Best is trial 162 with value: 0.9031442880630494.\n",
      "[I 2024-05-25 00:15:31,788] Trial 175 finished with value: 0.8693505764007569 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 117, 'dropout_1': 0.23864744616088834, 'optimizer': 'Adam', 'lstm_units_2': 59, 'dropout_2': 0.4120799894252715, 'lstm_units_3': 52, 'dropout_3': 0.190469597196258}. Best is trial 162 with value: 0.9031442880630494.\n",
      "[I 2024-05-25 00:15:50,777] Trial 176 finished with value: 0.9177490472793579 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 126, 'dropout_1': 0.25314584250529726, 'optimizer': 'Adam', 'lstm_units_2': 68, 'dropout_2': 0.3938336292925794, 'lstm_units_3': 39, 'dropout_3': 0.2584061977691302}. Best is trial 176 with value: 0.9177490472793579.\n",
      "[I 2024-05-25 00:16:10,902] Trial 177 finished with value: 0.8523361802101135 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 126, 'dropout_1': 0.2704204362229027, 'optimizer': 'RMSprop', 'lstm_units_2': 69, 'dropout_2': 0.38832136238397785, 'lstm_units_3': 37, 'dropout_3': 0.26421811446306437}. Best is trial 176 with value: 0.9177490472793579.\n",
      "[I 2024-05-25 00:16:29,800] Trial 178 finished with value: 0.8645019054412841 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 128, 'dropout_1': 0.2564166929186498, 'optimizer': 'Adam', 'lstm_units_2': 65, 'dropout_2': 0.35729869454783386, 'lstm_units_3': 43, 'dropout_3': 0.30933924054179995}. Best is trial 176 with value: 0.9177490472793579.\n",
      "[I 2024-05-25 00:16:48,973] Trial 179 finished with value: 0.8619159698486328 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 121, 'dropout_1': 0.2066535990676631, 'optimizer': 'Adam', 'lstm_units_2': 62, 'dropout_2': 0.3706367304393545, 'lstm_units_3': 62, 'dropout_3': 0.20941911673076719}. Best is trial 176 with value: 0.9177490472793579.\n",
      "[I 2024-05-25 00:17:09,411] Trial 180 finished with value: 0.8620628952980042 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 124, 'dropout_1': 0.2496337279775293, 'optimizer': 'RMSprop', 'lstm_units_2': 68, 'dropout_2': 0.4182610200622833, 'lstm_units_3': 39, 'dropout_3': 0.22779996834223393}. Best is trial 176 with value: 0.9177490472793579.\n",
      "[I 2024-05-25 00:17:28,706] Trial 181 finished with value: 0.862151026725769 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 126, 'dropout_1': 0.22103232579269227, 'optimizer': 'Adam', 'lstm_units_2': 72, 'dropout_2': 0.4418367988938473, 'lstm_units_3': 48, 'dropout_3': 0.10227812120889579}. Best is trial 176 with value: 0.9177490472793579.\n",
      "[I 2024-05-25 00:17:48,269] Trial 182 finished with value: 0.8741110801696778 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 119, 'dropout_1': 0.28722424446083106, 'optimizer': 'Adam', 'lstm_units_2': 121, 'dropout_2': 0.4014060721684878, 'lstm_units_3': 56, 'dropout_3': 0.13929981362895585}. Best is trial 176 with value: 0.9177490472793579.\n",
      "[I 2024-05-25 00:18:07,245] Trial 183 finished with value: 0.8693211793899536 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 112, 'dropout_1': 0.2385013727765974, 'optimizer': 'Adam', 'lstm_units_2': 64, 'dropout_2': 0.39644790821918885, 'lstm_units_3': 44, 'dropout_3': 0.28533213947250674}. Best is trial 176 with value: 0.9177490472793579.\n",
      "[I 2024-05-25 00:18:26,451] Trial 184 finished with value: 0.8550102949142456 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 115, 'dropout_1': 0.08098945199598323, 'optimizer': 'Adam', 'lstm_units_2': 57, 'dropout_2': 0.42636368630478055, 'lstm_units_3': 65, 'dropout_3': 0.16400116351336438}. Best is trial 176 with value: 0.9177490472793579.\n",
      "[I 2024-05-25 00:18:45,698] Trial 185 finished with value: 0.8426682233810425 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 123, 'dropout_1': 0.21289417219911166, 'optimizer': 'Adam', 'lstm_units_2': 50, 'dropout_2': 0.32568796117264165, 'lstm_units_3': 40, 'dropout_3': 0.1163453358355811}. Best is trial 176 with value: 0.9177490472793579.\n",
      "[I 2024-05-25 00:19:04,891] Trial 186 finished with value: 0.8377607822418213 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 110, 'dropout_1': 0.23298901396539512, 'optimizer': 'Adam', 'lstm_units_2': 118, 'dropout_2': 0.3435042090109366, 'lstm_units_3': 60, 'dropout_3': 0.18274979079000614}. Best is trial 176 with value: 0.9177490472793579.\n",
      "[I 2024-05-25 00:19:23,728] Trial 187 finished with value: 0.8717895984649658 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 128, 'dropout_1': 0.19306880493052878, 'optimizer': 'Adam', 'lstm_units_2': 54, 'dropout_2': 0.4448096295245256, 'lstm_units_3': 128, 'dropout_3': 0.20259445881747462}. Best is trial 176 with value: 0.9177490472793579.\n",
      "[I 2024-05-25 00:19:43,602] Trial 188 finished with value: 0.7891860127449035 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 44, 'dropout_1': 0.2673679309242731, 'optimizer': 'RMSprop', 'lstm_units_2': 61, 'dropout_2': 0.04459723350361498, 'lstm_units_3': 30, 'dropout_3': 0.25299905614979545}. Best is trial 176 with value: 0.9177490472793579.\n",
      "[I 2024-05-25 00:20:02,741] Trial 189 finished with value: 0.8645900607109069 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 121, 'dropout_1': 0.2517524296726599, 'optimizer': 'Adam', 'lstm_units_2': 74, 'dropout_2': 0.40821103433685174, 'lstm_units_3': 53, 'dropout_3': 0.15697556754457054}. Best is trial 176 with value: 0.9177490472793579.\n",
      "[I 2024-05-25 00:20:21,896] Trial 190 finished with value: 0.8717895984649658 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 117, 'dropout_1': 0.22362933038537328, 'optimizer': 'Adam', 'lstm_units_2': 115, 'dropout_2': 0.4410454403077264, 'lstm_units_3': 37, 'dropout_3': 0.23217672005116285}. Best is trial 176 with value: 0.9177490472793579.\n",
      "[I 2024-05-25 00:20:41,080] Trial 191 finished with value: 0.8838671803474426 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 124, 'dropout_1': 0.20222497982411852, 'optimizer': 'Adam', 'lstm_units_2': 52, 'dropout_2': 0.36531610478449383, 'lstm_units_3': 49, 'dropout_3': 0.2150970901263875}. Best is trial 176 with value: 0.9177490472793579.\n",
      "[I 2024-05-25 00:21:00,204] Trial 192 finished with value: 0.8741698503494263 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 126, 'dropout_1': 0.20061861015673338, 'optimizer': 'Adam', 'lstm_units_2': 48, 'dropout_2': 0.3686359462384596, 'lstm_units_3': 47, 'dropout_3': 0.21299371539786108}. Best is trial 176 with value: 0.9177490472793579.\n",
      "[I 2024-05-25 00:21:19,216] Trial 193 finished with value: 0.8766088843345642 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 124, 'dropout_1': 0.21404162542520477, 'optimizer': 'Adam', 'lstm_units_2': 52, 'dropout_2': 0.2621468267729676, 'lstm_units_3': 52, 'dropout_3': 0.16995097912416762}. Best is trial 176 with value: 0.9177490472793579.\n",
      "[I 2024-05-25 00:21:38,302] Trial 194 finished with value: 0.8594181656837463 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 114, 'dropout_1': 0.2410956955749771, 'optimizer': 'Adam', 'lstm_units_2': 55, 'dropout_2': 0.018839265679675764, 'lstm_units_3': 124, 'dropout_3': 0.09241272601025964}. Best is trial 176 with value: 0.9177490472793579.\n",
      "[I 2024-05-25 00:21:47,332] Trial 195 finished with value: 0.7871583938598633 and parameters: {'num_lstm_layers': 1, 'lstm_units_1': 121, 'dropout_1': 0.17682422146703036, 'optimizer': 'Adam'}. Best is trial 176 with value: 0.9177490472793579.\n",
      "[I 2024-05-25 00:22:06,389] Trial 196 finished with value: 0.8813987851142884 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 126, 'dropout_1': 0.22662856876383505, 'optimizer': 'Adam', 'lstm_units_2': 59, 'dropout_2': 0.38648806369812017, 'lstm_units_3': 44, 'dropout_3': 0.12275348363664348}. Best is trial 176 with value: 0.9177490472793579.\n",
      "[I 2024-05-25 00:22:26,640] Trial 197 finished with value: 0.8353217601776123 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 126, 'dropout_1': 0.20700642733226382, 'optimizer': 'RMSprop', 'lstm_units_2': 58, 'dropout_2': 0.3805018921685453, 'lstm_units_3': 45, 'dropout_3': 0.14371611999827344}. Best is trial 176 with value: 0.9177490472793579.\n",
      "[I 2024-05-25 00:22:45,857] Trial 198 finished with value: 0.8596532464027404 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 123, 'dropout_1': 0.30132556879784506, 'optimizer': 'Adam', 'lstm_units_2': 66, 'dropout_2': 0.3970542239075994, 'lstm_units_3': 43, 'dropout_3': 0.12932644875589425}. Best is trial 176 with value: 0.9177490472793579.\n",
      "[I 2024-05-25 00:23:05,116] Trial 199 finished with value: 0.874140465259552 and parameters: {'num_lstm_layers': 3, 'lstm_units_1': 119, 'dropout_1': 0.19019573002590734, 'optimizer': 'Adam', 'lstm_units_2': 62, 'dropout_2': 0.3785676540232999, 'lstm_units_3': 50, 'dropout_3': 0.12102092453752746}. Best is trial 176 with value: 0.9177490472793579.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'num_lstm_layers': 3, 'lstm_units_1': 126, 'dropout_1': 0.25314584250529726, 'optimizer': 'Adam', 'lstm_units_2': 68, 'dropout_2': 0.3938336292925794, 'lstm_units_3': 39, 'dropout_3': 0.2584061977691302}\n"
     ]
    }
   ],
   "source": [
    "X_train_val = load('../../BEST SET/X_train_fft.joblib')\n",
    "y_train_val = load('../../BEST SET/y_Train.joblib')\n",
    "X_test = load('../../BEST SET/X_test_fft.joblib')\n",
    "y_test = load('../../BEST SET/y_Test.joblib') \n",
    "\n",
    "quarter = X_train_val[::4]\n",
    "quarter_labels = y_train_val[::4]\n",
    "second_qtr = X_train_val[1::4]\n",
    "second_qtr_labels = y_train_val[1::4]\n",
    "third_qtr = X_train_val[2::4]\n",
    "third_qtr_labels = y_train_val[2::4]\n",
    "fourth_qtr = X_train_val[3::4]\n",
    "fourth_qtr_labels = y_train_val[3::4]\n",
    "\n",
    "\n",
    "X_test = np.append(X_test,second_qtr, axis=0)\n",
    "X_test = np.append(X_test, third_qtr, axis=0)\n",
    "X_test = np.append(X_test, fourth_qtr, axis=0)\n",
    "y_test = np.append(y_test,second_qtr_labels, axis=0)\n",
    "y_test = np.append(y_test, third_qtr_labels, axis=0)\n",
    "y_test = np.append(y_test, fourth_qtr_labels, axis=0)\n",
    "\n",
    "X_train_val = quarter\n",
    "y_train_val = quarter_labels\n",
    "\n",
    "\n",
    "input_sh = X_train_val.shape\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Optuna Objective Function\n",
    "def objective(trial):\n",
    "    lstm_2 = 0\n",
    "    drop_2 = 0\n",
    "    lstm_3 = 0\n",
    "    drop_3 = 0\n",
    "    num_lstm = trial.suggest_int('num_lstm_layers', 1, 3)\n",
    "    lstm_1 = trial.suggest_int('lstm_units_1', 16, 128)\n",
    "    dropout_1 = trial.suggest_float('dropout_1', 0.0, 0.5)\n",
    "    optimizer = trial.suggest_categorical('optimizer', ['SGD', 'RMSprop', 'Adam'])\n",
    "\n",
    "    if num_lstm > 1:\n",
    "        lstm_2 = trial.suggest_int(f'lstm_units_2', 16, 128)\n",
    "        drop_2 = trial.suggest_float(f'dropout_2', 0.0, 0.5)\n",
    "    if num_lstm > 2:\n",
    "        lstm_3 = trial.suggest_int(f'lstm_units_3', 16, 128)\n",
    "        drop_3 = trial.suggest_float(f'dropout_3', 0.0, 0.5)\n",
    "\n",
    "\n",
    "    # Assuming StratifiedKFold, customize if needed\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)  \n",
    "    scores = []\n",
    "    for train_index, test_index in skf.split(X_train_val, y_train_val):\n",
    "        X_tr, X_val = X_train_val[train_index], X_train_val[test_index]\n",
    "        y_tr, y_val = y_train_val[train_index], y_train_val[test_index]\n",
    "\n",
    "        model = create_model(optimizer, num_lstm, lstm_1, dropout_1, lstm_2, drop_2, lstm_3, drop_3)\n",
    "        model.fit(X_tr, y_tr,epochs=20, verbose = 0) \n",
    "        score = model.evaluate(X_val, y_val, verbose = 0)\n",
    "        scores.append(score[1])\n",
    "    return np.array(scores).mean()\n",
    "\n",
    "# Hyperparameter Optimization\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=200)\n",
    "\n",
    "best_params = study.best_params\n",
    "print(\"Best hyperparameters:\", best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (435191884.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[20], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    ``` (function) def create_model(\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "''' optimizer: Any,\n",
    "    num_lstm: Any,\n",
    "    lstm_1: Any,\n",
    "    dropout_1: Any,\n",
    "    lstm_2: int = 0,\n",
    "    drop_2: int = 0,\n",
    "    lstm_3: int = 0,\n",
    "    drop_3: int = 0 '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "25/30 [========================>.....] - ETA: 0s - loss: 0.6251 - accuracy: 0.7075\n",
      "Epoch 1: val_accuracy improved from -inf to 0.79032, saving model to test.h5\n",
      "30/30 [==============================] - 3s 26ms/step - loss: 0.6162 - accuracy: 0.7094 - val_loss: 0.5061 - val_accuracy: 0.7903\n",
      "Epoch 2/500\n",
      "25/30 [========================>.....] - ETA: 0s - loss: 0.4594 - accuracy: 0.8025\n",
      "Epoch 2: val_accuracy improved from 0.79032 to 0.86129, saving model to test.h5\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 0.4469 - accuracy: 0.8073 - val_loss: 0.3187 - val_accuracy: 0.8613\n",
      "Epoch 3/500\n",
      "25/30 [========================>.....] - ETA: 0s - loss: 0.3634 - accuracy: 0.8462\n",
      "Epoch 3: val_accuracy improved from 0.86129 to 0.87419, saving model to test.h5\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 0.3721 - accuracy: 0.8428 - val_loss: 0.3176 - val_accuracy: 0.8742\n",
      "Epoch 4/500\n",
      "26/30 [=========================>....] - ETA: 0s - loss: 0.3623 - accuracy: 0.8389\n",
      "Epoch 4: val_accuracy improved from 0.87419 to 0.88065, saving model to test.h5\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 0.3517 - accuracy: 0.8450 - val_loss: 0.3081 - val_accuracy: 0.8806\n",
      "Epoch 5/500\n",
      "28/30 [===========================>..] - ETA: 0s - loss: 0.3535 - accuracy: 0.8348\n",
      "Epoch 5: val_accuracy did not improve from 0.88065\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.3549 - accuracy: 0.8321 - val_loss: 0.3096 - val_accuracy: 0.8806\n",
      "Epoch 6/500\n",
      "27/30 [==========================>...] - ETA: 0s - loss: 0.3558 - accuracy: 0.8438\n",
      "Epoch 6: val_accuracy did not improve from 0.88065\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.3490 - accuracy: 0.8493 - val_loss: 0.2955 - val_accuracy: 0.8806\n",
      "Epoch 7/500\n",
      "26/30 [=========================>....] - ETA: 0s - loss: 0.3228 - accuracy: 0.8618\n",
      "Epoch 7: val_accuracy did not improve from 0.88065\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.3228 - accuracy: 0.8622 - val_loss: 0.2860 - val_accuracy: 0.8806\n",
      "Epoch 8/500\n",
      "27/30 [==========================>...] - ETA: 0s - loss: 0.3023 - accuracy: 0.8681\n",
      "Epoch 8: val_accuracy did not improve from 0.88065\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.2985 - accuracy: 0.8687 - val_loss: 0.3115 - val_accuracy: 0.8613\n",
      "Epoch 9/500\n",
      "28/30 [===========================>..] - ETA: 0s - loss: 0.3061 - accuracy: 0.8705\n",
      "Epoch 9: val_accuracy did not improve from 0.88065\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.3074 - accuracy: 0.8687 - val_loss: 0.2986 - val_accuracy: 0.8774\n",
      "Epoch 10/500\n",
      "27/30 [==========================>...] - ETA: 0s - loss: 0.2910 - accuracy: 0.8727\n",
      "Epoch 10: val_accuracy did not improve from 0.88065\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.2969 - accuracy: 0.8708 - val_loss: 0.3012 - val_accuracy: 0.8677\n",
      "Epoch 11/500\n",
      "26/30 [=========================>....] - ETA: 0s - loss: 0.2928 - accuracy: 0.8702\n",
      "Epoch 11: val_accuracy did not improve from 0.88065\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.2957 - accuracy: 0.8698 - val_loss: 0.3033 - val_accuracy: 0.8742\n",
      "Epoch 12/500\n",
      "26/30 [=========================>....] - ETA: 0s - loss: 0.3102 - accuracy: 0.8654\n",
      "Epoch 12: val_accuracy improved from 0.88065 to 0.88710, saving model to test.h5\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 0.3191 - accuracy: 0.8547 - val_loss: 0.2957 - val_accuracy: 0.8871\n",
      "Epoch 13/500\n",
      "26/30 [=========================>....] - ETA: 0s - loss: 0.2949 - accuracy: 0.8654\n",
      "Epoch 13: val_accuracy improved from 0.88710 to 0.90000, saving model to test.h5\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 0.2923 - accuracy: 0.8654 - val_loss: 0.2840 - val_accuracy: 0.9000\n",
      "Epoch 14/500\n",
      "27/30 [==========================>...] - ETA: 0s - loss: 0.3025 - accuracy: 0.8681\n",
      "Epoch 14: val_accuracy did not improve from 0.90000\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.2963 - accuracy: 0.8719 - val_loss: 0.2801 - val_accuracy: 0.9000\n",
      "Epoch 15/500\n",
      "27/30 [==========================>...] - ETA: 0s - loss: 0.2718 - accuracy: 0.8877\n",
      "Epoch 15: val_accuracy did not improve from 0.90000\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.2856 - accuracy: 0.8827 - val_loss: 0.2792 - val_accuracy: 0.8968\n",
      "Epoch 16/500\n",
      "25/30 [========================>.....] - ETA: 0s - loss: 0.2614 - accuracy: 0.8888\n",
      "Epoch 16: val_accuracy did not improve from 0.90000\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.2689 - accuracy: 0.8837 - val_loss: 0.3002 - val_accuracy: 0.8710\n",
      "Epoch 17/500\n",
      "25/30 [========================>.....] - ETA: 0s - loss: 0.2733 - accuracy: 0.8838\n",
      "Epoch 17: val_accuracy did not improve from 0.90000\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.2763 - accuracy: 0.8816 - val_loss: 0.2902 - val_accuracy: 0.8903\n",
      "Epoch 18/500\n",
      "26/30 [=========================>....] - ETA: 0s - loss: 0.2774 - accuracy: 0.8714\n",
      "Epoch 18: val_accuracy did not improve from 0.90000\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.2875 - accuracy: 0.8687 - val_loss: 0.2897 - val_accuracy: 0.8935\n",
      "Epoch 19/500\n",
      "27/30 [==========================>...] - ETA: 0s - loss: 0.2773 - accuracy: 0.8808\n",
      "Epoch 19: val_accuracy did not improve from 0.90000\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.2786 - accuracy: 0.8784 - val_loss: 0.2690 - val_accuracy: 0.8935\n",
      "Epoch 20/500\n",
      "26/30 [=========================>....] - ETA: 0s - loss: 0.2702 - accuracy: 0.8846\n",
      "Epoch 20: val_accuracy did not improve from 0.90000\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.2618 - accuracy: 0.8891 - val_loss: 0.2919 - val_accuracy: 0.8871\n",
      "Epoch 21/500\n",
      "28/30 [===========================>..] - ETA: 0s - loss: 0.2625 - accuracy: 0.8850\n",
      "Epoch 21: val_accuracy did not improve from 0.90000\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.2622 - accuracy: 0.8848 - val_loss: 0.2812 - val_accuracy: 0.8968\n",
      "Epoch 22/500\n",
      "28/30 [===========================>..] - ETA: 0s - loss: 0.2651 - accuracy: 0.8694\n",
      "Epoch 22: val_accuracy did not improve from 0.90000\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.2641 - accuracy: 0.8698 - val_loss: 0.2987 - val_accuracy: 0.8839\n",
      "Epoch 23/500\n",
      "26/30 [=========================>....] - ETA: 0s - loss: 0.2523 - accuracy: 0.8882\n",
      "Epoch 23: val_accuracy did not improve from 0.90000\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.2618 - accuracy: 0.8827 - val_loss: 0.3097 - val_accuracy: 0.8774\n",
      "Epoch 24/500\n",
      "25/30 [========================>.....] - ETA: 0s - loss: 0.2700 - accuracy: 0.8863\n",
      "Epoch 24: val_accuracy did not improve from 0.90000\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.2647 - accuracy: 0.8902 - val_loss: 0.2868 - val_accuracy: 0.8935\n",
      "Epoch 25/500\n",
      "26/30 [=========================>....] - ETA: 0s - loss: 0.2562 - accuracy: 0.8858\n",
      "Epoch 25: val_accuracy did not improve from 0.90000\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.2534 - accuracy: 0.8870 - val_loss: 0.2889 - val_accuracy: 0.8968\n",
      "Epoch 26/500\n",
      "27/30 [==========================>...] - ETA: 0s - loss: 0.2741 - accuracy: 0.8773\n",
      "Epoch 26: val_accuracy did not improve from 0.90000\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.2760 - accuracy: 0.8741 - val_loss: 0.3115 - val_accuracy: 0.8903\n",
      "Epoch 27/500\n",
      "28/30 [===========================>..] - ETA: 0s - loss: 0.2729 - accuracy: 0.8817\n",
      "Epoch 27: val_accuracy did not improve from 0.90000\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.2735 - accuracy: 0.8827 - val_loss: 0.2857 - val_accuracy: 0.9000\n",
      "Epoch 28/500\n",
      "28/30 [===========================>..] - ETA: 0s - loss: 0.3062 - accuracy: 0.8705\n",
      "Epoch 28: val_accuracy did not improve from 0.90000\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.3043 - accuracy: 0.8741 - val_loss: 0.3042 - val_accuracy: 0.8903\n",
      "Epoch 29/500\n",
      "25/30 [========================>.....] - ETA: 0s - loss: 0.2566 - accuracy: 0.8875\n",
      "Epoch 29: val_accuracy improved from 0.90000 to 0.90323, saving model to test.h5\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 0.2504 - accuracy: 0.8902 - val_loss: 0.2770 - val_accuracy: 0.9032\n",
      "Epoch 30/500\n",
      "27/30 [==========================>...] - ETA: 0s - loss: 0.2447 - accuracy: 0.8819\n",
      "Epoch 30: val_accuracy did not improve from 0.90323\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.2507 - accuracy: 0.8784 - val_loss: 0.2771 - val_accuracy: 0.9000\n",
      "Epoch 31/500\n",
      "26/30 [=========================>....] - ETA: 0s - loss: 0.2531 - accuracy: 0.8822\n",
      "Epoch 31: val_accuracy did not improve from 0.90323\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.2518 - accuracy: 0.8816 - val_loss: 0.2905 - val_accuracy: 0.8710\n",
      "Epoch 32/500\n",
      "27/30 [==========================>...] - ETA: 0s - loss: 0.2541 - accuracy: 0.8831\n",
      "Epoch 32: val_accuracy did not improve from 0.90323\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.2574 - accuracy: 0.8805 - val_loss: 0.2839 - val_accuracy: 0.8935\n",
      "Epoch 33/500\n",
      "26/30 [=========================>....] - ETA: 0s - loss: 0.2485 - accuracy: 0.8810\n",
      "Epoch 33: val_accuracy did not improve from 0.90323\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.2479 - accuracy: 0.8794 - val_loss: 0.2985 - val_accuracy: 0.8742\n",
      "Epoch 34/500\n",
      "26/30 [=========================>....] - ETA: 0s - loss: 0.2626 - accuracy: 0.8858\n",
      "Epoch 34: val_accuracy did not improve from 0.90323\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.2638 - accuracy: 0.8837 - val_loss: 0.2741 - val_accuracy: 0.8935\n",
      "Epoch 35/500\n",
      "28/30 [===========================>..] - ETA: 0s - loss: 0.2474 - accuracy: 0.8783\n",
      "Epoch 35: val_accuracy improved from 0.90323 to 0.91290, saving model to test.h5\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 0.2493 - accuracy: 0.8773 - val_loss: 0.2655 - val_accuracy: 0.9129\n",
      "Epoch 36/500\n",
      "27/30 [==========================>...] - ETA: 0s - loss: 0.2635 - accuracy: 0.8750\n",
      "Epoch 36: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.2611 - accuracy: 0.8762 - val_loss: 0.2845 - val_accuracy: 0.8806\n",
      "Epoch 37/500\n",
      "26/30 [=========================>....] - ETA: 0s - loss: 0.2519 - accuracy: 0.8834\n",
      "Epoch 37: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.2650 - accuracy: 0.8837 - val_loss: 0.2749 - val_accuracy: 0.8935\n",
      "Epoch 38/500\n",
      "26/30 [=========================>....] - ETA: 0s - loss: 0.2548 - accuracy: 0.8798\n",
      "Epoch 38: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.2438 - accuracy: 0.8837 - val_loss: 0.2731 - val_accuracy: 0.8935\n",
      "Epoch 39/500\n",
      "28/30 [===========================>..] - ETA: 0s - loss: 0.2478 - accuracy: 0.8739\n",
      "Epoch 39: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.2513 - accuracy: 0.8751 - val_loss: 0.3132 - val_accuracy: 0.8613\n",
      "Epoch 40/500\n",
      "25/30 [========================>.....] - ETA: 0s - loss: 0.2608 - accuracy: 0.8612\n",
      "Epoch 40: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.2672 - accuracy: 0.8590 - val_loss: 0.2663 - val_accuracy: 0.9065\n",
      "Epoch 41/500\n",
      "27/30 [==========================>...] - ETA: 0s - loss: 0.2272 - accuracy: 0.8970\n",
      "Epoch 41: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.2365 - accuracy: 0.8913 - val_loss: 0.2855 - val_accuracy: 0.8742\n",
      "Epoch 42/500\n",
      "27/30 [==========================>...] - ETA: 0s - loss: 0.2256 - accuracy: 0.9051\n",
      "Epoch 42: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.2278 - accuracy: 0.9010 - val_loss: 0.2818 - val_accuracy: 0.8935\n",
      "Epoch 43/500\n",
      "26/30 [=========================>....] - ETA: 0s - loss: 0.2199 - accuracy: 0.8978\n",
      "Epoch 43: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.2233 - accuracy: 0.8977 - val_loss: 0.2964 - val_accuracy: 0.8677\n",
      "Epoch 44/500\n",
      "28/30 [===========================>..] - ETA: 0s - loss: 0.2309 - accuracy: 0.9029\n",
      "Epoch 44: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.2269 - accuracy: 0.9031 - val_loss: 0.2754 - val_accuracy: 0.8806\n",
      "Epoch 45/500\n",
      "27/30 [==========================>...] - ETA: 0s - loss: 0.2380 - accuracy: 0.8831\n",
      "Epoch 45: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.2421 - accuracy: 0.8794 - val_loss: 0.2763 - val_accuracy: 0.8968\n",
      "Epoch 46/500\n",
      "28/30 [===========================>..] - ETA: 0s - loss: 0.2138 - accuracy: 0.8962\n",
      "Epoch 46: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.2161 - accuracy: 0.8956 - val_loss: 0.2957 - val_accuracy: 0.8774\n",
      "Epoch 47/500\n",
      "25/30 [========================>.....] - ETA: 0s - loss: 0.2389 - accuracy: 0.8825\n",
      "Epoch 47: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.2313 - accuracy: 0.8870 - val_loss: 0.3121 - val_accuracy: 0.8613\n",
      "Epoch 48/500\n",
      "26/30 [=========================>....] - ETA: 0s - loss: 0.2421 - accuracy: 0.8870\n",
      "Epoch 48: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.2394 - accuracy: 0.8870 - val_loss: 0.2876 - val_accuracy: 0.8806\n",
      "Epoch 49/500\n",
      "25/30 [========================>.....] - ETA: 0s - loss: 0.2192 - accuracy: 0.8938\n",
      "Epoch 49: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.2224 - accuracy: 0.8934 - val_loss: 0.2873 - val_accuracy: 0.8774\n",
      "Epoch 50/500\n",
      "27/30 [==========================>...] - ETA: 0s - loss: 0.2148 - accuracy: 0.8912\n",
      "Epoch 50: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.2170 - accuracy: 0.8881 - val_loss: 0.2826 - val_accuracy: 0.8968\n",
      "Epoch 51/500\n",
      "28/30 [===========================>..] - ETA: 0s - loss: 0.2010 - accuracy: 0.9040\n",
      "Epoch 51: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1995 - accuracy: 0.9042 - val_loss: 0.2920 - val_accuracy: 0.8742\n",
      "Epoch 52/500\n",
      "27/30 [==========================>...] - ETA: 0s - loss: 0.2138 - accuracy: 0.9016\n",
      "Epoch 52: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.2150 - accuracy: 0.9020 - val_loss: 0.3017 - val_accuracy: 0.8710\n",
      "Epoch 53/500\n",
      "27/30 [==========================>...] - ETA: 0s - loss: 0.2415 - accuracy: 0.8819\n",
      "Epoch 53: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.2340 - accuracy: 0.8881 - val_loss: 0.3129 - val_accuracy: 0.8516\n",
      "Epoch 54/500\n",
      "28/30 [===========================>..] - ETA: 0s - loss: 0.2247 - accuracy: 0.8906\n",
      "Epoch 54: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.2261 - accuracy: 0.8902 - val_loss: 0.2840 - val_accuracy: 0.8806\n",
      "Epoch 55/500\n",
      "26/30 [=========================>....] - ETA: 0s - loss: 0.2323 - accuracy: 0.9014\n",
      "Epoch 55: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.2303 - accuracy: 0.8988 - val_loss: 0.3041 - val_accuracy: 0.8452\n",
      "Epoch 56/500\n",
      "26/30 [=========================>....] - ETA: 0s - loss: 0.2340 - accuracy: 0.8858\n",
      "Epoch 56: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.2249 - accuracy: 0.8913 - val_loss: 0.2780 - val_accuracy: 0.8774\n",
      "Epoch 57/500\n",
      "25/30 [========================>.....] - ETA: 0s - loss: 0.2229 - accuracy: 0.8963\n",
      "Epoch 57: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.2242 - accuracy: 0.8934 - val_loss: 0.2795 - val_accuracy: 0.8742\n",
      "Epoch 58/500\n",
      "25/30 [========================>.....] - ETA: 0s - loss: 0.2254 - accuracy: 0.8975\n",
      "Epoch 58: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.2240 - accuracy: 0.8977 - val_loss: 0.2730 - val_accuracy: 0.8806\n",
      "Epoch 59/500\n",
      "27/30 [==========================>...] - ETA: 0s - loss: 0.2156 - accuracy: 0.9005\n",
      "Epoch 59: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.2159 - accuracy: 0.8999 - val_loss: 0.2975 - val_accuracy: 0.8806\n",
      "Epoch 60/500\n",
      "25/30 [========================>.....] - ETA: 0s - loss: 0.2201 - accuracy: 0.9013\n",
      "Epoch 60: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.2184 - accuracy: 0.8977 - val_loss: 0.2810 - val_accuracy: 0.8710\n",
      "Epoch 61/500\n",
      "26/30 [=========================>....] - ETA: 0s - loss: 0.2069 - accuracy: 0.9135\n",
      "Epoch 61: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.2074 - accuracy: 0.9139 - val_loss: 0.2886 - val_accuracy: 0.8613\n",
      "Epoch 62/500\n",
      "26/30 [=========================>....] - ETA: 0s - loss: 0.2006 - accuracy: 0.9062\n",
      "Epoch 62: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.2078 - accuracy: 0.9042 - val_loss: 0.2980 - val_accuracy: 0.8677\n",
      "Epoch 63/500\n",
      "26/30 [=========================>....] - ETA: 0s - loss: 0.2030 - accuracy: 0.9099\n",
      "Epoch 63: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.2017 - accuracy: 0.9085 - val_loss: 0.2995 - val_accuracy: 0.8677\n",
      "Epoch 64/500\n",
      "25/30 [========================>.....] - ETA: 0s - loss: 0.2049 - accuracy: 0.9050\n",
      "Epoch 64: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.2259 - accuracy: 0.8977 - val_loss: 0.3088 - val_accuracy: 0.8484\n",
      "Epoch 65/500\n",
      "25/30 [========================>.....] - ETA: 0s - loss: 0.1969 - accuracy: 0.8988\n",
      "Epoch 65: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.2036 - accuracy: 0.8967 - val_loss: 0.3027 - val_accuracy: 0.8613\n",
      "Epoch 66/500\n",
      "25/30 [========================>.....] - ETA: 0s - loss: 0.1998 - accuracy: 0.9062\n",
      "Epoch 66: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.2090 - accuracy: 0.9010 - val_loss: 0.2862 - val_accuracy: 0.8710\n",
      "Epoch 67/500\n",
      "25/30 [========================>.....] - ETA: 0s - loss: 0.2109 - accuracy: 0.8975\n",
      "Epoch 67: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.2085 - accuracy: 0.8988 - val_loss: 0.2962 - val_accuracy: 0.8613\n",
      "Epoch 68/500\n",
      "25/30 [========================>.....] - ETA: 0s - loss: 0.1991 - accuracy: 0.8975\n",
      "Epoch 68: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.2105 - accuracy: 0.8913 - val_loss: 0.3238 - val_accuracy: 0.8613\n",
      "Epoch 69/500\n",
      "24/30 [=======================>......] - ETA: 0s - loss: 0.2051 - accuracy: 0.9062\n",
      "Epoch 69: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.2081 - accuracy: 0.9020 - val_loss: 0.3026 - val_accuracy: 0.8710\n",
      "Epoch 70/500\n",
      "24/30 [=======================>......] - ETA: 0s - loss: 0.1757 - accuracy: 0.9193\n",
      "Epoch 70: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1925 - accuracy: 0.9096 - val_loss: 0.2995 - val_accuracy: 0.8742\n",
      "Epoch 71/500\n",
      "27/30 [==========================>...] - ETA: 0s - loss: 0.2196 - accuracy: 0.9005\n",
      "Epoch 71: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.2194 - accuracy: 0.9010 - val_loss: 0.2847 - val_accuracy: 0.8774\n",
      "Epoch 72/500\n",
      "25/30 [========================>.....] - ETA: 0s - loss: 0.2004 - accuracy: 0.9013\n",
      "Epoch 72: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.2013 - accuracy: 0.8999 - val_loss: 0.2951 - val_accuracy: 0.8774\n",
      "Epoch 73/500\n",
      "27/30 [==========================>...] - ETA: 0s - loss: 0.2105 - accuracy: 0.8912\n",
      "Epoch 73: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.2052 - accuracy: 0.8956 - val_loss: 0.3176 - val_accuracy: 0.8484\n",
      "Epoch 74/500\n",
      "25/30 [========================>.....] - ETA: 0s - loss: 0.2077 - accuracy: 0.9013\n",
      "Epoch 74: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.2220 - accuracy: 0.8945 - val_loss: 0.2827 - val_accuracy: 0.8742\n",
      "Epoch 75/500\n",
      "24/30 [=======================>......] - ETA: 0s - loss: 0.1940 - accuracy: 0.9049\n",
      "Epoch 75: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1968 - accuracy: 0.9074 - val_loss: 0.3212 - val_accuracy: 0.8484\n",
      "Epoch 76/500\n",
      "27/30 [==========================>...] - ETA: 0s - loss: 0.2027 - accuracy: 0.8947\n",
      "Epoch 76: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.2006 - accuracy: 0.8956 - val_loss: 0.2954 - val_accuracy: 0.8774\n",
      "Epoch 77/500\n",
      "27/30 [==========================>...] - ETA: 0s - loss: 0.1953 - accuracy: 0.9167\n",
      "Epoch 77: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1952 - accuracy: 0.9150 - val_loss: 0.3234 - val_accuracy: 0.8613\n",
      "Epoch 78/500\n",
      "26/30 [=========================>....] - ETA: 0s - loss: 0.2010 - accuracy: 0.9062\n",
      "Epoch 78: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1967 - accuracy: 0.9085 - val_loss: 0.3034 - val_accuracy: 0.8742\n",
      "Epoch 79/500\n",
      "28/30 [===========================>..] - ETA: 0s - loss: 0.1849 - accuracy: 0.9174\n",
      "Epoch 79: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1807 - accuracy: 0.9182 - val_loss: 0.3387 - val_accuracy: 0.8613\n",
      "Epoch 80/500\n",
      "25/30 [========================>.....] - ETA: 0s - loss: 0.1935 - accuracy: 0.9013\n",
      "Epoch 80: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.2071 - accuracy: 0.8945 - val_loss: 0.3246 - val_accuracy: 0.8581\n",
      "Epoch 81/500\n",
      "26/30 [=========================>....] - ETA: 0s - loss: 0.1835 - accuracy: 0.9099\n",
      "Epoch 81: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1874 - accuracy: 0.9053 - val_loss: 0.3248 - val_accuracy: 0.8548\n",
      "Epoch 82/500\n",
      "26/30 [=========================>....] - ETA: 0s - loss: 0.2501 - accuracy: 0.8738\n",
      "Epoch 82: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.2424 - accuracy: 0.8805 - val_loss: 0.3068 - val_accuracy: 0.8710\n",
      "Epoch 83/500\n",
      "27/30 [==========================>...] - ETA: 0s - loss: 0.2055 - accuracy: 0.9144\n",
      "Epoch 83: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.2083 - accuracy: 0.9117 - val_loss: 0.3142 - val_accuracy: 0.8581\n",
      "Epoch 84/500\n",
      "26/30 [=========================>....] - ETA: 0s - loss: 0.1808 - accuracy: 0.9195\n",
      "Epoch 84: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1816 - accuracy: 0.9171 - val_loss: 0.2926 - val_accuracy: 0.8710\n",
      "Epoch 85/500\n",
      "25/30 [========================>.....] - ETA: 0s - loss: 0.2011 - accuracy: 0.8963\n",
      "Epoch 85: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.2012 - accuracy: 0.8988 - val_loss: 0.3558 - val_accuracy: 0.8452\n",
      "Epoch 86/500\n",
      "27/30 [==========================>...] - ETA: 0s - loss: 0.2703 - accuracy: 0.8692\n",
      "Epoch 86: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.2645 - accuracy: 0.8730 - val_loss: 0.3016 - val_accuracy: 0.8548\n",
      "Epoch 87/500\n",
      "27/30 [==========================>...] - ETA: 0s - loss: 0.1969 - accuracy: 0.9086\n",
      "Epoch 87: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1930 - accuracy: 0.9117 - val_loss: 0.2673 - val_accuracy: 0.8677\n",
      "Epoch 88/500\n",
      "26/30 [=========================>....] - ETA: 0s - loss: 0.2283 - accuracy: 0.8894\n",
      "Epoch 88: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.2173 - accuracy: 0.8967 - val_loss: 0.3167 - val_accuracy: 0.8387\n",
      "Epoch 89/500\n",
      "27/30 [==========================>...] - ETA: 0s - loss: 0.2043 - accuracy: 0.9039\n",
      "Epoch 89: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.2026 - accuracy: 0.9064 - val_loss: 0.2683 - val_accuracy: 0.8742\n",
      "Epoch 90/500\n",
      "25/30 [========================>.....] - ETA: 0s - loss: 0.1881 - accuracy: 0.9125\n",
      "Epoch 90: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1907 - accuracy: 0.9107 - val_loss: 0.2873 - val_accuracy: 0.8774\n",
      "Epoch 91/500\n",
      "27/30 [==========================>...] - ETA: 0s - loss: 0.1878 - accuracy: 0.9132\n",
      "Epoch 91: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1860 - accuracy: 0.9128 - val_loss: 0.3003 - val_accuracy: 0.8710\n",
      "Epoch 92/500\n",
      "27/30 [==========================>...] - ETA: 0s - loss: 0.2044 - accuracy: 0.8935\n",
      "Epoch 92: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1997 - accuracy: 0.8988 - val_loss: 0.2910 - val_accuracy: 0.8710\n",
      "Epoch 93/500\n",
      "27/30 [==========================>...] - ETA: 0s - loss: 0.1990 - accuracy: 0.9016\n",
      "Epoch 93: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1977 - accuracy: 0.9031 - val_loss: 0.2911 - val_accuracy: 0.8774\n",
      "Epoch 94/500\n",
      "25/30 [========================>.....] - ETA: 0s - loss: 0.1684 - accuracy: 0.9200\n",
      "Epoch 94: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1832 - accuracy: 0.9150 - val_loss: 0.3202 - val_accuracy: 0.8581\n",
      "Epoch 95/500\n",
      "24/30 [=======================>......] - ETA: 0s - loss: 0.2186 - accuracy: 0.9036\n",
      "Epoch 95: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.2122 - accuracy: 0.9042 - val_loss: 0.3119 - val_accuracy: 0.8613\n",
      "Epoch 96/500\n",
      "26/30 [=========================>....] - ETA: 0s - loss: 0.2176 - accuracy: 0.8978\n",
      "Epoch 96: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.2119 - accuracy: 0.8988 - val_loss: 0.3057 - val_accuracy: 0.8710\n",
      "Epoch 97/500\n",
      "26/30 [=========================>....] - ETA: 0s - loss: 0.2039 - accuracy: 0.9050\n",
      "Epoch 97: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.2077 - accuracy: 0.9031 - val_loss: 0.2938 - val_accuracy: 0.8742\n",
      "Epoch 98/500\n",
      "25/30 [========================>.....] - ETA: 0s - loss: 0.1857 - accuracy: 0.9212\n",
      "Epoch 98: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1863 - accuracy: 0.9203 - val_loss: 0.3149 - val_accuracy: 0.8613\n",
      "Epoch 99/500\n",
      "24/30 [=======================>......] - ETA: 0s - loss: 0.1818 - accuracy: 0.9102\n",
      "Epoch 99: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1869 - accuracy: 0.9107 - val_loss: 0.2965 - val_accuracy: 0.8806\n",
      "Epoch 100/500\n",
      "27/30 [==========================>...] - ETA: 0s - loss: 0.2178 - accuracy: 0.9039\n",
      "Epoch 100: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.2205 - accuracy: 0.9020 - val_loss: 0.3062 - val_accuracy: 0.8613\n",
      "Epoch 101/500\n",
      "26/30 [=========================>....] - ETA: 0s - loss: 0.1878 - accuracy: 0.9147\n",
      "Epoch 101: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1903 - accuracy: 0.9117 - val_loss: 0.3100 - val_accuracy: 0.8710\n",
      "Epoch 102/500\n",
      "28/30 [===========================>..] - ETA: 0s - loss: 0.1906 - accuracy: 0.9141\n",
      "Epoch 102: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1917 - accuracy: 0.9139 - val_loss: 0.3042 - val_accuracy: 0.8677\n",
      "Epoch 103/500\n",
      "27/30 [==========================>...] - ETA: 0s - loss: 0.1865 - accuracy: 0.9144\n",
      "Epoch 103: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1920 - accuracy: 0.9096 - val_loss: 0.2903 - val_accuracy: 0.8742\n",
      "Epoch 104/500\n",
      "25/30 [========================>.....] - ETA: 0s - loss: 0.1744 - accuracy: 0.9262\n",
      "Epoch 104: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1835 - accuracy: 0.9214 - val_loss: 0.3114 - val_accuracy: 0.8774\n",
      "Epoch 105/500\n",
      "26/30 [=========================>....] - ETA: 0s - loss: 0.1817 - accuracy: 0.9099\n",
      "Epoch 105: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1824 - accuracy: 0.9096 - val_loss: 0.3141 - val_accuracy: 0.8710\n",
      "Epoch 106/500\n",
      "27/30 [==========================>...] - ETA: 0s - loss: 0.2118 - accuracy: 0.9016\n",
      "Epoch 106: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.2116 - accuracy: 0.9020 - val_loss: 0.3146 - val_accuracy: 0.8645\n",
      "Epoch 107/500\n",
      "28/30 [===========================>..] - ETA: 0s - loss: 0.1819 - accuracy: 0.9085\n",
      "Epoch 107: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.1833 - accuracy: 0.9085 - val_loss: 0.3356 - val_accuracy: 0.8613\n",
      "Epoch 108/500\n",
      "28/30 [===========================>..] - ETA: 0s - loss: 0.1925 - accuracy: 0.9074\n",
      "Epoch 108: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.1912 - accuracy: 0.9085 - val_loss: 0.3055 - val_accuracy: 0.8774\n",
      "Epoch 109/500\n",
      "25/30 [========================>.....] - ETA: 0s - loss: 0.1932 - accuracy: 0.9125\n",
      "Epoch 109: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1902 - accuracy: 0.9117 - val_loss: 0.3333 - val_accuracy: 0.8742\n",
      "Epoch 110/500\n",
      "28/30 [===========================>..] - ETA: 0s - loss: 0.1833 - accuracy: 0.9062\n",
      "Epoch 110: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.1840 - accuracy: 0.9074 - val_loss: 0.3227 - val_accuracy: 0.8742\n",
      "Epoch 111/500\n",
      "26/30 [=========================>....] - ETA: 0s - loss: 0.1782 - accuracy: 0.9111\n",
      "Epoch 111: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1828 - accuracy: 0.9107 - val_loss: 0.3223 - val_accuracy: 0.8710\n",
      "Epoch 112/500\n",
      "28/30 [===========================>..] - ETA: 0s - loss: 0.1953 - accuracy: 0.9040\n",
      "Epoch 112: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.1941 - accuracy: 0.9042 - val_loss: 0.3196 - val_accuracy: 0.8677\n",
      "Epoch 113/500\n",
      "27/30 [==========================>...] - ETA: 0s - loss: 0.1718 - accuracy: 0.9213\n",
      "Epoch 113: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1757 - accuracy: 0.9203 - val_loss: 0.3280 - val_accuracy: 0.8677\n",
      "Epoch 114/500\n",
      "26/30 [=========================>....] - ETA: 0s - loss: 0.1599 - accuracy: 0.9279\n",
      "Epoch 114: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1621 - accuracy: 0.9247 - val_loss: 0.3214 - val_accuracy: 0.8742\n",
      "Epoch 115/500\n",
      "27/30 [==========================>...] - ETA: 0s - loss: 0.1741 - accuracy: 0.9109\n",
      "Epoch 115: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1698 - accuracy: 0.9139 - val_loss: 0.3808 - val_accuracy: 0.8484\n",
      "Epoch 116/500\n",
      "28/30 [===========================>..] - ETA: 0s - loss: 0.1944 - accuracy: 0.9107\n",
      "Epoch 116: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1906 - accuracy: 0.9139 - val_loss: 0.3564 - val_accuracy: 0.8581\n",
      "Epoch 117/500\n",
      "27/30 [==========================>...] - ETA: 0s - loss: 0.1905 - accuracy: 0.8981\n",
      "Epoch 117: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1865 - accuracy: 0.9010 - val_loss: 0.3359 - val_accuracy: 0.8645\n",
      "Epoch 118/500\n",
      "27/30 [==========================>...] - ETA: 0s - loss: 0.1706 - accuracy: 0.9259\n",
      "Epoch 118: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1727 - accuracy: 0.9247 - val_loss: 0.3342 - val_accuracy: 0.8645\n",
      "Epoch 119/500\n",
      "27/30 [==========================>...] - ETA: 0s - loss: 0.2007 - accuracy: 0.9074\n",
      "Epoch 119: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1978 - accuracy: 0.9096 - val_loss: 0.3337 - val_accuracy: 0.8710\n",
      "Epoch 120/500\n",
      "28/30 [===========================>..] - ETA: 0s - loss: 0.1662 - accuracy: 0.9252\n",
      "Epoch 120: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.1703 - accuracy: 0.9247 - val_loss: 0.3271 - val_accuracy: 0.8742\n",
      "Epoch 121/500\n",
      "28/30 [===========================>..] - ETA: 0s - loss: 0.1755 - accuracy: 0.9085\n",
      "Epoch 121: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.1761 - accuracy: 0.9096 - val_loss: 0.3713 - val_accuracy: 0.8452\n",
      "Epoch 122/500\n",
      "28/30 [===========================>..] - ETA: 0s - loss: 0.1933 - accuracy: 0.9051\n",
      "Epoch 122: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1936 - accuracy: 0.9031 - val_loss: 0.3772 - val_accuracy: 0.8484\n",
      "Epoch 123/500\n",
      "28/30 [===========================>..] - ETA: 0s - loss: 0.1854 - accuracy: 0.9096\n",
      "Epoch 123: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1863 - accuracy: 0.9085 - val_loss: 0.3338 - val_accuracy: 0.8742\n",
      "Epoch 124/500\n",
      "25/30 [========================>.....] - ETA: 0s - loss: 0.1813 - accuracy: 0.9025\n",
      "Epoch 124: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1736 - accuracy: 0.9096 - val_loss: 0.3351 - val_accuracy: 0.8645\n",
      "Epoch 125/500\n",
      "26/30 [=========================>....] - ETA: 0s - loss: 0.1689 - accuracy: 0.9231\n",
      "Epoch 125: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1693 - accuracy: 0.9214 - val_loss: 0.3304 - val_accuracy: 0.8742\n",
      "Epoch 126/500\n",
      "28/30 [===========================>..] - ETA: 0s - loss: 0.1725 - accuracy: 0.9185\n",
      "Epoch 126: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1706 - accuracy: 0.9203 - val_loss: 0.3366 - val_accuracy: 0.8710\n",
      "Epoch 127/500\n",
      "26/30 [=========================>....] - ETA: 0s - loss: 0.1927 - accuracy: 0.9062\n",
      "Epoch 127: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1899 - accuracy: 0.9074 - val_loss: 0.3326 - val_accuracy: 0.8742\n",
      "Epoch 128/500\n",
      "27/30 [==========================>...] - ETA: 0s - loss: 0.2339 - accuracy: 0.8993\n",
      "Epoch 128: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.2306 - accuracy: 0.9010 - val_loss: 0.3968 - val_accuracy: 0.8387\n",
      "Epoch 129/500\n",
      "28/30 [===========================>..] - ETA: 0s - loss: 0.2303 - accuracy: 0.8973\n",
      "Epoch 129: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.2246 - accuracy: 0.8999 - val_loss: 0.3627 - val_accuracy: 0.8613\n",
      "Epoch 130/500\n",
      "28/30 [===========================>..] - ETA: 0s - loss: 0.1912 - accuracy: 0.9062\n",
      "Epoch 130: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1943 - accuracy: 0.9064 - val_loss: 0.3520 - val_accuracy: 0.8677\n",
      "Epoch 131/500\n",
      "26/30 [=========================>....] - ETA: 0s - loss: 0.1959 - accuracy: 0.9050\n",
      "Epoch 131: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1956 - accuracy: 0.9031 - val_loss: 0.3400 - val_accuracy: 0.8742\n",
      "Epoch 132/500\n",
      "27/30 [==========================>...] - ETA: 0s - loss: 0.1791 - accuracy: 0.9190\n",
      "Epoch 132: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1783 - accuracy: 0.9193 - val_loss: 0.3551 - val_accuracy: 0.8903\n",
      "Epoch 133/500\n",
      "27/30 [==========================>...] - ETA: 0s - loss: 0.1708 - accuracy: 0.9201\n",
      "Epoch 133: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1755 - accuracy: 0.9171 - val_loss: 0.3596 - val_accuracy: 0.8839\n",
      "Epoch 134/500\n",
      "28/30 [===========================>..] - ETA: 0s - loss: 0.1721 - accuracy: 0.9208\n",
      "Epoch 134: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.1734 - accuracy: 0.9193 - val_loss: 0.3506 - val_accuracy: 0.8774\n",
      "Epoch 135/500\n",
      "28/30 [===========================>..] - ETA: 0s - loss: 0.1747 - accuracy: 0.9152\n",
      "Epoch 135: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1709 - accuracy: 0.9160 - val_loss: 0.3655 - val_accuracy: 0.8774\n",
      "Epoch 136/500\n",
      "28/30 [===========================>..] - ETA: 0s - loss: 0.1923 - accuracy: 0.9196\n",
      "Epoch 136: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.1895 - accuracy: 0.9203 - val_loss: 0.3717 - val_accuracy: 0.8452\n",
      "Epoch 137/500\n",
      "27/30 [==========================>...] - ETA: 0s - loss: 0.1696 - accuracy: 0.9259\n",
      "Epoch 137: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1717 - accuracy: 0.9225 - val_loss: 0.3724 - val_accuracy: 0.8839\n",
      "Epoch 138/500\n",
      "26/30 [=========================>....] - ETA: 0s - loss: 0.1459 - accuracy: 0.9327\n",
      "Epoch 138: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1545 - accuracy: 0.9290 - val_loss: 0.3814 - val_accuracy: 0.8581\n",
      "Epoch 139/500\n",
      "27/30 [==========================>...] - ETA: 0s - loss: 0.1855 - accuracy: 0.9097\n",
      "Epoch 139: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.1864 - accuracy: 0.9085 - val_loss: 0.3592 - val_accuracy: 0.8645\n",
      "Epoch 140/500\n",
      "28/30 [===========================>..] - ETA: 0s - loss: 0.1823 - accuracy: 0.9163\n",
      "Epoch 140: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.1807 - accuracy: 0.9160 - val_loss: 0.3580 - val_accuracy: 0.8677\n",
      "Epoch 141/500\n",
      "28/30 [===========================>..] - ETA: 0s - loss: 0.1713 - accuracy: 0.9219\n",
      "Epoch 141: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.1727 - accuracy: 0.9193 - val_loss: 0.3708 - val_accuracy: 0.8516\n",
      "Epoch 142/500\n",
      "27/30 [==========================>...] - ETA: 0s - loss: 0.1796 - accuracy: 0.9201\n",
      "Epoch 142: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1808 - accuracy: 0.9160 - val_loss: 0.3986 - val_accuracy: 0.8484\n",
      "Epoch 143/500\n",
      "28/30 [===========================>..] - ETA: 0s - loss: 0.1788 - accuracy: 0.9174\n",
      "Epoch 143: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.1815 - accuracy: 0.9160 - val_loss: 0.3578 - val_accuracy: 0.8613\n",
      "Epoch 144/500\n",
      "28/30 [===========================>..] - ETA: 0s - loss: 0.1875 - accuracy: 0.9085\n",
      "Epoch 144: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.1891 - accuracy: 0.9074 - val_loss: 0.3934 - val_accuracy: 0.8548\n",
      "Epoch 145/500\n",
      "27/30 [==========================>...] - ETA: 0s - loss: 0.1778 - accuracy: 0.9086\n",
      "Epoch 145: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1770 - accuracy: 0.9096 - val_loss: 0.3616 - val_accuracy: 0.8677\n",
      "Epoch 146/500\n",
      "25/30 [========================>.....] - ETA: 0s - loss: 0.1779 - accuracy: 0.9175\n",
      "Epoch 146: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.1791 - accuracy: 0.9214 - val_loss: 0.3671 - val_accuracy: 0.8613\n",
      "Epoch 147/500\n",
      "26/30 [=========================>....] - ETA: 0s - loss: 0.1790 - accuracy: 0.9026\n",
      "Epoch 147: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1815 - accuracy: 0.9020 - val_loss: 0.3624 - val_accuracy: 0.8548\n",
      "Epoch 148/500\n",
      "26/30 [=========================>....] - ETA: 0s - loss: 0.2215 - accuracy: 0.8942\n",
      "Epoch 148: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.2177 - accuracy: 0.8977 - val_loss: 0.3399 - val_accuracy: 0.8613\n",
      "Epoch 149/500\n",
      "28/30 [===========================>..] - ETA: 0s - loss: 0.1882 - accuracy: 0.9185\n",
      "Epoch 149: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1866 - accuracy: 0.9182 - val_loss: 0.3692 - val_accuracy: 0.8548\n",
      "Epoch 150/500\n",
      "26/30 [=========================>....] - ETA: 0s - loss: 0.1661 - accuracy: 0.9195\n",
      "Epoch 150: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1706 - accuracy: 0.9182 - val_loss: 0.3582 - val_accuracy: 0.8645\n",
      "Epoch 151/500\n",
      "25/30 [========================>.....] - ETA: 0s - loss: 0.1553 - accuracy: 0.9275\n",
      "Epoch 151: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1666 - accuracy: 0.9236 - val_loss: 0.3671 - val_accuracy: 0.8677\n",
      "Epoch 152/500\n",
      "28/30 [===========================>..] - ETA: 0s - loss: 0.1809 - accuracy: 0.9185\n",
      "Epoch 152: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1851 - accuracy: 0.9171 - val_loss: 0.3598 - val_accuracy: 0.8516\n",
      "Epoch 153/500\n",
      "27/30 [==========================>...] - ETA: 0s - loss: 0.1805 - accuracy: 0.9109\n",
      "Epoch 153: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1842 - accuracy: 0.9117 - val_loss: 0.3938 - val_accuracy: 0.8516\n",
      "Epoch 154/500\n",
      "27/30 [==========================>...] - ETA: 0s - loss: 0.1696 - accuracy: 0.9132\n",
      "Epoch 154: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1654 - accuracy: 0.9150 - val_loss: 0.4089 - val_accuracy: 0.8484\n",
      "Epoch 155/500\n",
      "26/30 [=========================>....] - ETA: 0s - loss: 0.1735 - accuracy: 0.9111\n",
      "Epoch 155: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1689 - accuracy: 0.9160 - val_loss: 0.3793 - val_accuracy: 0.8516\n",
      "Epoch 156/500\n",
      "28/30 [===========================>..] - ETA: 0s - loss: 0.1772 - accuracy: 0.9174\n",
      "Epoch 156: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.1807 - accuracy: 0.9150 - val_loss: 0.3794 - val_accuracy: 0.8452\n",
      "Epoch 157/500\n",
      "26/30 [=========================>....] - ETA: 0s - loss: 0.2176 - accuracy: 0.9123\n",
      "Epoch 157: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.2057 - accuracy: 0.9193 - val_loss: 0.3801 - val_accuracy: 0.8419\n",
      "Epoch 158/500\n",
      "28/30 [===========================>..] - ETA: 0s - loss: 0.1671 - accuracy: 0.9230\n",
      "Epoch 158: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.1725 - accuracy: 0.9203 - val_loss: 0.3890 - val_accuracy: 0.8516\n",
      "Epoch 159/500\n",
      "28/30 [===========================>..] - ETA: 0s - loss: 0.1774 - accuracy: 0.9230\n",
      "Epoch 159: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1753 - accuracy: 0.9236 - val_loss: 0.3735 - val_accuracy: 0.8613\n",
      "Epoch 160/500\n",
      "26/30 [=========================>....] - ETA: 0s - loss: 0.1762 - accuracy: 0.9183\n",
      "Epoch 160: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1714 - accuracy: 0.9214 - val_loss: 0.4217 - val_accuracy: 0.8355\n",
      "Epoch 161/500\n",
      "26/30 [=========================>....] - ETA: 0s - loss: 0.2143 - accuracy: 0.8990\n",
      "Epoch 161: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.2099 - accuracy: 0.8988 - val_loss: 0.3790 - val_accuracy: 0.8484\n",
      "Epoch 162/500\n",
      "26/30 [=========================>....] - ETA: 0s - loss: 0.1651 - accuracy: 0.9279\n",
      "Epoch 162: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1715 - accuracy: 0.9225 - val_loss: 0.3934 - val_accuracy: 0.8516\n",
      "Epoch 163/500\n",
      "26/30 [=========================>....] - ETA: 0s - loss: 0.1634 - accuracy: 0.9255\n",
      "Epoch 163: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1640 - accuracy: 0.9247 - val_loss: 0.3568 - val_accuracy: 0.8645\n",
      "Epoch 164/500\n",
      "25/30 [========================>.....] - ETA: 0s - loss: 0.1666 - accuracy: 0.9237\n",
      "Epoch 164: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1610 - accuracy: 0.9247 - val_loss: 0.3830 - val_accuracy: 0.8548\n",
      "Epoch 165/500\n",
      "27/30 [==========================>...] - ETA: 0s - loss: 0.1641 - accuracy: 0.9236\n",
      "Epoch 165: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1696 - accuracy: 0.9214 - val_loss: 0.3856 - val_accuracy: 0.8581\n",
      "Epoch 166/500\n",
      "26/30 [=========================>....] - ETA: 0s - loss: 0.1713 - accuracy: 0.9195\n",
      "Epoch 166: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1680 - accuracy: 0.9203 - val_loss: 0.3362 - val_accuracy: 0.8581\n",
      "Epoch 167/500\n",
      "26/30 [=========================>....] - ETA: 0s - loss: 0.1844 - accuracy: 0.9219\n",
      "Epoch 167: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1827 - accuracy: 0.9214 - val_loss: 0.3503 - val_accuracy: 0.8710\n",
      "Epoch 168/500\n",
      "26/30 [=========================>....] - ETA: 0s - loss: 0.1705 - accuracy: 0.9159\n",
      "Epoch 168: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1729 - accuracy: 0.9150 - val_loss: 0.3610 - val_accuracy: 0.8645\n",
      "Epoch 169/500\n",
      "26/30 [=========================>....] - ETA: 0s - loss: 0.1767 - accuracy: 0.9123\n",
      "Epoch 169: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1729 - accuracy: 0.9139 - val_loss: 0.3374 - val_accuracy: 0.8581\n",
      "Epoch 170/500\n",
      "28/30 [===========================>..] - ETA: 0s - loss: 0.1663 - accuracy: 0.9141\n",
      "Epoch 170: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.1675 - accuracy: 0.9128 - val_loss: 0.3682 - val_accuracy: 0.8581\n",
      "Epoch 171/500\n",
      "28/30 [===========================>..] - ETA: 0s - loss: 0.1823 - accuracy: 0.9152\n",
      "Epoch 171: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1829 - accuracy: 0.9139 - val_loss: 0.3616 - val_accuracy: 0.8484\n",
      "Epoch 172/500\n",
      "27/30 [==========================>...] - ETA: 0s - loss: 0.1911 - accuracy: 0.9062\n",
      "Epoch 172: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.1953 - accuracy: 0.9020 - val_loss: 0.3527 - val_accuracy: 0.8548\n",
      "Epoch 173/500\n",
      "28/30 [===========================>..] - ETA: 0s - loss: 0.1739 - accuracy: 0.9241\n",
      "Epoch 173: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.1716 - accuracy: 0.9247 - val_loss: 0.3732 - val_accuracy: 0.8516\n",
      "Epoch 174/500\n",
      "28/30 [===========================>..] - ETA: 0s - loss: 0.1754 - accuracy: 0.9208\n",
      "Epoch 174: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.1762 - accuracy: 0.9193 - val_loss: 0.3775 - val_accuracy: 0.8484\n",
      "Epoch 175/500\n",
      "28/30 [===========================>..] - ETA: 0s - loss: 0.1812 - accuracy: 0.9085\n",
      "Epoch 175: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1833 - accuracy: 0.9107 - val_loss: 0.3333 - val_accuracy: 0.8742\n",
      "Epoch 176/500\n",
      "27/30 [==========================>...] - ETA: 0s - loss: 0.1683 - accuracy: 0.9109\n",
      "Epoch 176: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1761 - accuracy: 0.9064 - val_loss: 0.3643 - val_accuracy: 0.8613\n",
      "Epoch 177/500\n",
      "28/30 [===========================>..] - ETA: 0s - loss: 0.1545 - accuracy: 0.9297\n",
      "Epoch 177: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1565 - accuracy: 0.9290 - val_loss: 0.3483 - val_accuracy: 0.8710\n",
      "Epoch 178/500\n",
      "28/30 [===========================>..] - ETA: 0s - loss: 0.1578 - accuracy: 0.9208\n",
      "Epoch 178: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.1572 - accuracy: 0.9203 - val_loss: 0.3823 - val_accuracy: 0.8677\n",
      "Epoch 179/500\n",
      "28/30 [===========================>..] - ETA: 0s - loss: 0.1607 - accuracy: 0.9196\n",
      "Epoch 179: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1619 - accuracy: 0.9193 - val_loss: 0.3370 - val_accuracy: 0.8710\n",
      "Epoch 180/500\n",
      "24/30 [=======================>......] - ETA: 0s - loss: 0.1689 - accuracy: 0.9206\n",
      "Epoch 180: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1690 - accuracy: 0.9193 - val_loss: 0.3714 - val_accuracy: 0.8677\n",
      "Epoch 181/500\n",
      "28/30 [===========================>..] - ETA: 0s - loss: 0.1688 - accuracy: 0.9163\n",
      "Epoch 181: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1690 - accuracy: 0.9160 - val_loss: 0.3256 - val_accuracy: 0.8581\n",
      "Epoch 182/500\n",
      "28/30 [===========================>..] - ETA: 0s - loss: 0.1531 - accuracy: 0.9241\n",
      "Epoch 182: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1547 - accuracy: 0.9236 - val_loss: 0.3645 - val_accuracy: 0.8677\n",
      "Epoch 183/500\n",
      "27/30 [==========================>...] - ETA: 0s - loss: 0.1641 - accuracy: 0.9120\n",
      "Epoch 183: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1646 - accuracy: 0.9128 - val_loss: 0.3627 - val_accuracy: 0.8645\n",
      "Epoch 184/500\n",
      "28/30 [===========================>..] - ETA: 0s - loss: 0.1611 - accuracy: 0.9319\n",
      "Epoch 184: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1650 - accuracy: 0.9279 - val_loss: 0.3330 - val_accuracy: 0.8677\n",
      "Epoch 185/500\n",
      "28/30 [===========================>..] - ETA: 0s - loss: 0.1511 - accuracy: 0.9208\n",
      "Epoch 185: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1504 - accuracy: 0.9203 - val_loss: 0.3510 - val_accuracy: 0.8484\n",
      "Epoch 186/500\n",
      "26/30 [=========================>....] - ETA: 0s - loss: 0.1651 - accuracy: 0.9219\n",
      "Epoch 186: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1685 - accuracy: 0.9214 - val_loss: 0.3993 - val_accuracy: 0.8581\n",
      "Epoch 187/500\n",
      "27/30 [==========================>...] - ETA: 0s - loss: 0.1677 - accuracy: 0.9201\n",
      "Epoch 187: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1643 - accuracy: 0.9203 - val_loss: 0.3605 - val_accuracy: 0.8613\n",
      "Epoch 188/500\n",
      "26/30 [=========================>....] - ETA: 0s - loss: 0.1430 - accuracy: 0.9375\n",
      "Epoch 188: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1442 - accuracy: 0.9333 - val_loss: 0.3814 - val_accuracy: 0.8645\n",
      "Epoch 189/500\n",
      "25/30 [========================>.....] - ETA: 0s - loss: 0.1456 - accuracy: 0.9350\n",
      "Epoch 189: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1499 - accuracy: 0.9354 - val_loss: 0.3809 - val_accuracy: 0.8677\n",
      "Epoch 190/500\n",
      "27/30 [==========================>...] - ETA: 0s - loss: 0.1524 - accuracy: 0.9248\n",
      "Epoch 190: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.1529 - accuracy: 0.9257 - val_loss: 0.4140 - val_accuracy: 0.8516\n",
      "Epoch 191/500\n",
      "25/30 [========================>.....] - ETA: 0s - loss: 0.1755 - accuracy: 0.9112\n",
      "Epoch 191: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1734 - accuracy: 0.9117 - val_loss: 0.3541 - val_accuracy: 0.8677\n",
      "Epoch 192/500\n",
      "24/30 [=======================>......] - ETA: 0s - loss: 0.1839 - accuracy: 0.9154\n",
      "Epoch 192: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1718 - accuracy: 0.9214 - val_loss: 0.3468 - val_accuracy: 0.8548\n",
      "Epoch 193/500\n",
      "25/30 [========================>.....] - ETA: 0s - loss: 0.1659 - accuracy: 0.9225\n",
      "Epoch 193: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1685 - accuracy: 0.9171 - val_loss: 0.3600 - val_accuracy: 0.8645\n",
      "Epoch 194/500\n",
      "26/30 [=========================>....] - ETA: 0s - loss: 0.1619 - accuracy: 0.9327\n",
      "Epoch 194: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1684 - accuracy: 0.9279 - val_loss: 0.3513 - val_accuracy: 0.8548\n",
      "Epoch 195/500\n",
      "25/30 [========================>.....] - ETA: 0s - loss: 0.1547 - accuracy: 0.9262\n",
      "Epoch 195: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1640 - accuracy: 0.9182 - val_loss: 0.3740 - val_accuracy: 0.8548\n",
      "Epoch 196/500\n",
      "24/30 [=======================>......] - ETA: 0s - loss: 0.1451 - accuracy: 0.9297\n",
      "Epoch 196: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1569 - accuracy: 0.9225 - val_loss: 0.3544 - val_accuracy: 0.8645\n",
      "Epoch 197/500\n",
      "28/30 [===========================>..] - ETA: 0s - loss: 0.1666 - accuracy: 0.9196\n",
      "Epoch 197: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1660 - accuracy: 0.9182 - val_loss: 0.3976 - val_accuracy: 0.8484\n",
      "Epoch 198/500\n",
      "25/30 [========================>.....] - ETA: 0s - loss: 0.1677 - accuracy: 0.9200\n",
      "Epoch 198: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1680 - accuracy: 0.9214 - val_loss: 0.3492 - val_accuracy: 0.8677\n",
      "Epoch 199/500\n",
      "26/30 [=========================>....] - ETA: 0s - loss: 0.1517 - accuracy: 0.9219\n",
      "Epoch 199: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1510 - accuracy: 0.9236 - val_loss: 0.3500 - val_accuracy: 0.8710\n",
      "Epoch 200/500\n",
      "24/30 [=======================>......] - ETA: 0s - loss: 0.1460 - accuracy: 0.9193\n",
      "Epoch 200: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1470 - accuracy: 0.9247 - val_loss: 0.3633 - val_accuracy: 0.8581\n",
      "Epoch 201/500\n",
      "25/30 [========================>.....] - ETA: 0s - loss: 0.1561 - accuracy: 0.9337\n",
      "Epoch 201: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1548 - accuracy: 0.9343 - val_loss: 0.3567 - val_accuracy: 0.8710\n",
      "Epoch 202/500\n",
      "28/30 [===========================>..] - ETA: 0s - loss: 0.1564 - accuracy: 0.9319\n",
      "Epoch 202: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1584 - accuracy: 0.9300 - val_loss: 0.3880 - val_accuracy: 0.8613\n",
      "Epoch 203/500\n",
      "26/30 [=========================>....] - ETA: 0s - loss: 0.1485 - accuracy: 0.9291\n",
      "Epoch 203: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1505 - accuracy: 0.9290 - val_loss: 0.3854 - val_accuracy: 0.8677\n",
      "Epoch 204/500\n",
      "23/30 [======================>.......] - ETA: 0s - loss: 0.1643 - accuracy: 0.9212\n",
      "Epoch 204: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1660 - accuracy: 0.9214 - val_loss: 0.4014 - val_accuracy: 0.8677\n",
      "Epoch 205/500\n",
      "25/30 [========================>.....] - ETA: 0s - loss: 0.1553 - accuracy: 0.9325\n",
      "Epoch 205: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1535 - accuracy: 0.9311 - val_loss: 0.3844 - val_accuracy: 0.8548\n",
      "Epoch 206/500\n",
      "26/30 [=========================>....] - ETA: 0s - loss: 0.1508 - accuracy: 0.9255\n",
      "Epoch 206: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1469 - accuracy: 0.9268 - val_loss: 0.3919 - val_accuracy: 0.8677\n",
      "Epoch 207/500\n",
      "26/30 [=========================>....] - ETA: 0s - loss: 0.1627 - accuracy: 0.9207\n",
      "Epoch 207: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1602 - accuracy: 0.9247 - val_loss: 0.4561 - val_accuracy: 0.8452\n",
      "Epoch 208/500\n",
      "27/30 [==========================>...] - ETA: 0s - loss: 0.1568 - accuracy: 0.9167\n",
      "Epoch 208: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1623 - accuracy: 0.9150 - val_loss: 0.3872 - val_accuracy: 0.8677\n",
      "Epoch 209/500\n",
      "28/30 [===========================>..] - ETA: 0s - loss: 0.1591 - accuracy: 0.9241\n",
      "Epoch 209: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1592 - accuracy: 0.9247 - val_loss: 0.3733 - val_accuracy: 0.8645\n",
      "Epoch 210/500\n",
      "26/30 [=========================>....] - ETA: 0s - loss: 0.1634 - accuracy: 0.9195\n",
      "Epoch 210: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1655 - accuracy: 0.9203 - val_loss: 0.3656 - val_accuracy: 0.8742\n",
      "Epoch 211/500\n",
      "26/30 [=========================>....] - ETA: 0s - loss: 0.1538 - accuracy: 0.9303\n",
      "Epoch 211: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1553 - accuracy: 0.9268 - val_loss: 0.3592 - val_accuracy: 0.8677\n",
      "Epoch 212/500\n",
      "25/30 [========================>.....] - ETA: 0s - loss: 0.1504 - accuracy: 0.9362\n",
      "Epoch 212: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1624 - accuracy: 0.9290 - val_loss: 0.3868 - val_accuracy: 0.8548\n",
      "Epoch 213/500\n",
      "28/30 [===========================>..] - ETA: 0s - loss: 0.1764 - accuracy: 0.9185\n",
      "Epoch 213: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.1748 - accuracy: 0.9193 - val_loss: 0.3567 - val_accuracy: 0.8677\n",
      "Epoch 214/500\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.1637 - accuracy: 0.9203\n",
      "Epoch 214: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.1637 - accuracy: 0.9203 - val_loss: 0.3866 - val_accuracy: 0.8710\n",
      "Epoch 215/500\n",
      "29/30 [============================>.] - ETA: 0s - loss: 0.1409 - accuracy: 0.9267\n",
      "Epoch 215: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.1407 - accuracy: 0.9268 - val_loss: 0.4051 - val_accuracy: 0.8645\n",
      "Epoch 216/500\n",
      "26/30 [=========================>....] - ETA: 0s - loss: 0.1621 - accuracy: 0.9231\n",
      "Epoch 216: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1607 - accuracy: 0.9247 - val_loss: 0.3567 - val_accuracy: 0.8613\n",
      "Epoch 217/500\n",
      "27/30 [==========================>...] - ETA: 0s - loss: 0.1444 - accuracy: 0.9317\n",
      "Epoch 217: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1439 - accuracy: 0.9311 - val_loss: 0.3776 - val_accuracy: 0.8677\n",
      "Epoch 218/500\n",
      "28/30 [===========================>..] - ETA: 0s - loss: 0.1491 - accuracy: 0.9308\n",
      "Epoch 218: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.1485 - accuracy: 0.9300 - val_loss: 0.3831 - val_accuracy: 0.8548\n",
      "Epoch 219/500\n",
      "26/30 [=========================>....] - ETA: 0s - loss: 0.1560 - accuracy: 0.9255\n",
      "Epoch 219: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1652 - accuracy: 0.9247 - val_loss: 0.3801 - val_accuracy: 0.8710\n",
      "Epoch 220/500\n",
      "28/30 [===========================>..] - ETA: 0s - loss: 0.1516 - accuracy: 0.9286\n",
      "Epoch 220: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.1502 - accuracy: 0.9300 - val_loss: 0.3772 - val_accuracy: 0.8484\n",
      "Epoch 221/500\n",
      "28/30 [===========================>..] - ETA: 0s - loss: 0.1999 - accuracy: 0.9029\n",
      "Epoch 221: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1966 - accuracy: 0.9053 - val_loss: 0.3795 - val_accuracy: 0.8548\n",
      "Epoch 222/500\n",
      "27/30 [==========================>...] - ETA: 0s - loss: 0.1711 - accuracy: 0.9120\n",
      "Epoch 222: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1746 - accuracy: 0.9128 - val_loss: 0.3595 - val_accuracy: 0.8645\n",
      "Epoch 223/500\n",
      "27/30 [==========================>...] - ETA: 0s - loss: 0.1486 - accuracy: 0.9387\n",
      "Epoch 223: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1539 - accuracy: 0.9354 - val_loss: 0.3701 - val_accuracy: 0.8645\n",
      "Epoch 224/500\n",
      "28/30 [===========================>..] - ETA: 0s - loss: 0.1484 - accuracy: 0.9286\n",
      "Epoch 224: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.1495 - accuracy: 0.9300 - val_loss: 0.3885 - val_accuracy: 0.8677\n",
      "Epoch 225/500\n",
      "29/30 [============================>.] - ETA: 0s - loss: 0.1607 - accuracy: 0.9181\n",
      "Epoch 225: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1605 - accuracy: 0.9182 - val_loss: 0.3630 - val_accuracy: 0.8645\n",
      "Epoch 226/500\n",
      "27/30 [==========================>...] - ETA: 0s - loss: 0.1591 - accuracy: 0.9213\n",
      "Epoch 226: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1551 - accuracy: 0.9225 - val_loss: 0.3785 - val_accuracy: 0.8645\n",
      "Epoch 227/500\n",
      "26/30 [=========================>....] - ETA: 0s - loss: 0.1504 - accuracy: 0.9255\n",
      "Epoch 227: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1466 - accuracy: 0.9290 - val_loss: 0.3946 - val_accuracy: 0.8710\n",
      "Epoch 228/500\n",
      "28/30 [===========================>..] - ETA: 0s - loss: 0.1442 - accuracy: 0.9353\n",
      "Epoch 228: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.1520 - accuracy: 0.9333 - val_loss: 0.3845 - val_accuracy: 0.8742\n",
      "Epoch 229/500\n",
      "28/30 [===========================>..] - ETA: 0s - loss: 0.1502 - accuracy: 0.9252\n",
      "Epoch 229: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.1525 - accuracy: 0.9247 - val_loss: 0.3969 - val_accuracy: 0.8548\n",
      "Epoch 230/500\n",
      "28/30 [===========================>..] - ETA: 0s - loss: 0.1497 - accuracy: 0.9286\n",
      "Epoch 230: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.1552 - accuracy: 0.9268 - val_loss: 0.3940 - val_accuracy: 0.8742\n",
      "Epoch 231/500\n",
      "25/30 [========================>.....] - ETA: 0s - loss: 0.1581 - accuracy: 0.9237\n",
      "Epoch 231: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1557 - accuracy: 0.9225 - val_loss: 0.3936 - val_accuracy: 0.8581\n",
      "Epoch 232/500\n",
      "28/30 [===========================>..] - ETA: 0s - loss: 0.1457 - accuracy: 0.9297\n",
      "Epoch 232: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.1463 - accuracy: 0.9311 - val_loss: 0.4091 - val_accuracy: 0.8645\n",
      "Epoch 233/500\n",
      "28/30 [===========================>..] - ETA: 0s - loss: 0.1672 - accuracy: 0.9051\n",
      "Epoch 233: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.1648 - accuracy: 0.9074 - val_loss: 0.3838 - val_accuracy: 0.8645\n",
      "Epoch 234/500\n",
      "28/30 [===========================>..] - ETA: 0s - loss: 0.1518 - accuracy: 0.9375\n",
      "Epoch 234: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1523 - accuracy: 0.9354 - val_loss: 0.3933 - val_accuracy: 0.8710\n",
      "Epoch 235/500\n",
      "27/30 [==========================>...] - ETA: 0s - loss: 0.1324 - accuracy: 0.9375\n",
      "Epoch 235: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1405 - accuracy: 0.9365 - val_loss: 0.4107 - val_accuracy: 0.8645\n",
      "Epoch 236/500\n",
      "27/30 [==========================>...] - ETA: 0s - loss: 0.1355 - accuracy: 0.9329\n",
      "Epoch 236: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.1343 - accuracy: 0.9333 - val_loss: 0.3872 - val_accuracy: 0.8710\n",
      "Epoch 237/500\n",
      "28/30 [===========================>..] - ETA: 0s - loss: 0.1488 - accuracy: 0.9208\n",
      "Epoch 237: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.1475 - accuracy: 0.9225 - val_loss: 0.4021 - val_accuracy: 0.8613\n",
      "Epoch 238/500\n",
      "26/30 [=========================>....] - ETA: 0s - loss: 0.1376 - accuracy: 0.9303\n",
      "Epoch 238: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1381 - accuracy: 0.9290 - val_loss: 0.4044 - val_accuracy: 0.8613\n",
      "Epoch 239/500\n",
      "28/30 [===========================>..] - ETA: 0s - loss: 0.1647 - accuracy: 0.9208\n",
      "Epoch 239: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.1670 - accuracy: 0.9203 - val_loss: 0.4425 - val_accuracy: 0.8581\n",
      "Epoch 240/500\n",
      "28/30 [===========================>..] - ETA: 0s - loss: 0.1501 - accuracy: 0.9308\n",
      "Epoch 240: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.1517 - accuracy: 0.9290 - val_loss: 0.3946 - val_accuracy: 0.8710\n",
      "Epoch 241/500\n",
      "28/30 [===========================>..] - ETA: 0s - loss: 0.1558 - accuracy: 0.9219\n",
      "Epoch 241: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.1530 - accuracy: 0.9236 - val_loss: 0.4369 - val_accuracy: 0.8613\n",
      "Epoch 242/500\n",
      "28/30 [===========================>..] - ETA: 0s - loss: 0.1394 - accuracy: 0.9364\n",
      "Epoch 242: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.1382 - accuracy: 0.9376 - val_loss: 0.4080 - val_accuracy: 0.8677\n",
      "Epoch 243/500\n",
      "28/30 [===========================>..] - ETA: 0s - loss: 0.1669 - accuracy: 0.9230\n",
      "Epoch 243: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.1668 - accuracy: 0.9236 - val_loss: 0.3726 - val_accuracy: 0.8645\n",
      "Epoch 244/500\n",
      "28/30 [===========================>..] - ETA: 0s - loss: 0.1765 - accuracy: 0.9152\n",
      "Epoch 244: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.1746 - accuracy: 0.9160 - val_loss: 0.4219 - val_accuracy: 0.8548\n",
      "Epoch 245/500\n",
      "26/30 [=========================>....] - ETA: 0s - loss: 0.1547 - accuracy: 0.9267\n",
      "Epoch 245: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1493 - accuracy: 0.9300 - val_loss: 0.4064 - val_accuracy: 0.8645\n",
      "Epoch 246/500\n",
      "25/30 [========================>.....] - ETA: 0s - loss: 0.1523 - accuracy: 0.9250\n",
      "Epoch 246: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1505 - accuracy: 0.9257 - val_loss: 0.4476 - val_accuracy: 0.8581\n",
      "Epoch 247/500\n",
      "28/30 [===========================>..] - ETA: 0s - loss: 0.1343 - accuracy: 0.9297\n",
      "Epoch 247: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.1339 - accuracy: 0.9300 - val_loss: 0.4260 - val_accuracy: 0.8613\n",
      "Epoch 248/500\n",
      "28/30 [===========================>..] - ETA: 0s - loss: 0.1492 - accuracy: 0.9275\n",
      "Epoch 248: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.1484 - accuracy: 0.9290 - val_loss: 0.3714 - val_accuracy: 0.8742\n",
      "Epoch 249/500\n",
      "27/30 [==========================>...] - ETA: 0s - loss: 0.1513 - accuracy: 0.9259\n",
      "Epoch 249: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1516 - accuracy: 0.9268 - val_loss: 0.3840 - val_accuracy: 0.8677\n",
      "Epoch 250/500\n",
      "28/30 [===========================>..] - ETA: 0s - loss: 0.1511 - accuracy: 0.9342\n",
      "Epoch 250: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.1506 - accuracy: 0.9333 - val_loss: 0.5300 - val_accuracy: 0.8323\n",
      "Epoch 251/500\n",
      "28/30 [===========================>..] - ETA: 0s - loss: 0.2056 - accuracy: 0.9074\n",
      "Epoch 251: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.2054 - accuracy: 0.9085 - val_loss: 0.3744 - val_accuracy: 0.8548\n",
      "Epoch 252/500\n",
      "28/30 [===========================>..] - ETA: 0s - loss: 0.1774 - accuracy: 0.9152\n",
      "Epoch 252: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1785 - accuracy: 0.9139 - val_loss: 0.3823 - val_accuracy: 0.8613\n",
      "Epoch 253/500\n",
      "27/30 [==========================>...] - ETA: 0s - loss: 0.1561 - accuracy: 0.9271\n",
      "Epoch 253: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1545 - accuracy: 0.9247 - val_loss: 0.4159 - val_accuracy: 0.8613\n",
      "Epoch 254/500\n",
      "28/30 [===========================>..] - ETA: 0s - loss: 0.1401 - accuracy: 0.9364\n",
      "Epoch 254: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.1392 - accuracy: 0.9354 - val_loss: 0.4865 - val_accuracy: 0.8516\n",
      "Epoch 255/500\n",
      "28/30 [===========================>..] - ETA: 0s - loss: 0.1352 - accuracy: 0.9386\n",
      "Epoch 255: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.1381 - accuracy: 0.9376 - val_loss: 0.4511 - val_accuracy: 0.8645\n",
      "Epoch 256/500\n",
      "27/30 [==========================>...] - ETA: 0s - loss: 0.1423 - accuracy: 0.9340\n",
      "Epoch 256: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1389 - accuracy: 0.9354 - val_loss: 0.4416 - val_accuracy: 0.8645\n",
      "Epoch 257/500\n",
      "28/30 [===========================>..] - ETA: 0s - loss: 0.1343 - accuracy: 0.9342\n",
      "Epoch 257: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.1378 - accuracy: 0.9290 - val_loss: 0.4383 - val_accuracy: 0.8452\n",
      "Epoch 258/500\n",
      "28/30 [===========================>..] - ETA: 0s - loss: 0.2050 - accuracy: 0.9074\n",
      "Epoch 258: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.2011 - accuracy: 0.9096 - val_loss: 0.4446 - val_accuracy: 0.8484\n",
      "Epoch 259/500\n",
      "28/30 [===========================>..] - ETA: 0s - loss: 0.1558 - accuracy: 0.9208\n",
      "Epoch 259: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1589 - accuracy: 0.9182 - val_loss: 0.3839 - val_accuracy: 0.8710\n",
      "Epoch 260/500\n",
      "28/30 [===========================>..] - ETA: 0s - loss: 0.1605 - accuracy: 0.9208\n",
      "Epoch 260: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.1568 - accuracy: 0.9225 - val_loss: 0.3795 - val_accuracy: 0.8548\n",
      "Epoch 261/500\n",
      "27/30 [==========================>...] - ETA: 0s - loss: 0.1527 - accuracy: 0.9317\n",
      "Epoch 261: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1484 - accuracy: 0.9333 - val_loss: 0.3839 - val_accuracy: 0.8516\n",
      "Epoch 262/500\n",
      "27/30 [==========================>...] - ETA: 0s - loss: 0.1483 - accuracy: 0.9236\n",
      "Epoch 262: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.1483 - accuracy: 0.9257 - val_loss: 0.4031 - val_accuracy: 0.8742\n",
      "Epoch 263/500\n",
      "27/30 [==========================>...] - ETA: 0s - loss: 0.1497 - accuracy: 0.9271\n",
      "Epoch 263: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1462 - accuracy: 0.9290 - val_loss: 0.4175 - val_accuracy: 0.8645\n",
      "Epoch 264/500\n",
      "29/30 [============================>.] - ETA: 0s - loss: 0.1700 - accuracy: 0.9300\n",
      "Epoch 264: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.1698 - accuracy: 0.9300 - val_loss: 0.4441 - val_accuracy: 0.8484\n",
      "Epoch 265/500\n",
      "28/30 [===========================>..] - ETA: 0s - loss: 0.1415 - accuracy: 0.9342\n",
      "Epoch 265: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.1422 - accuracy: 0.9333 - val_loss: 0.4123 - val_accuracy: 0.8613\n",
      "Epoch 266/500\n",
      "27/30 [==========================>...] - ETA: 0s - loss: 0.1493 - accuracy: 0.9271\n",
      "Epoch 266: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1505 - accuracy: 0.9279 - val_loss: 0.4066 - val_accuracy: 0.8613\n",
      "Epoch 267/500\n",
      "26/30 [=========================>....] - ETA: 0s - loss: 0.1402 - accuracy: 0.9387\n",
      "Epoch 267: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1442 - accuracy: 0.9354 - val_loss: 0.3963 - val_accuracy: 0.8581\n",
      "Epoch 268/500\n",
      "29/30 [============================>.] - ETA: 0s - loss: 0.1518 - accuracy: 0.9213\n",
      "Epoch 268: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.1520 - accuracy: 0.9214 - val_loss: 0.4088 - val_accuracy: 0.8581\n",
      "Epoch 269/500\n",
      "28/30 [===========================>..] - ETA: 0s - loss: 0.1577 - accuracy: 0.9297\n",
      "Epoch 269: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.1578 - accuracy: 0.9311 - val_loss: 0.4614 - val_accuracy: 0.8516\n",
      "Epoch 270/500\n",
      "26/30 [=========================>....] - ETA: 0s - loss: 0.1424 - accuracy: 0.9351\n",
      "Epoch 270: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1456 - accuracy: 0.9300 - val_loss: 0.4316 - val_accuracy: 0.8613\n",
      "Epoch 271/500\n",
      "27/30 [==========================>...] - ETA: 0s - loss: 0.1457 - accuracy: 0.9363\n",
      "Epoch 271: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1474 - accuracy: 0.9343 - val_loss: 0.4004 - val_accuracy: 0.8613\n",
      "Epoch 272/500\n",
      "27/30 [==========================>...] - ETA: 0s - loss: 0.1266 - accuracy: 0.9421\n",
      "Epoch 272: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.1396 - accuracy: 0.9365 - val_loss: 0.4049 - val_accuracy: 0.8581\n",
      "Epoch 273/500\n",
      "27/30 [==========================>...] - ETA: 0s - loss: 0.1455 - accuracy: 0.9317\n",
      "Epoch 273: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.1475 - accuracy: 0.9322 - val_loss: 0.4439 - val_accuracy: 0.8516\n",
      "Epoch 274/500\n",
      "27/30 [==========================>...] - ETA: 0s - loss: 0.1406 - accuracy: 0.9317\n",
      "Epoch 274: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.1388 - accuracy: 0.9333 - val_loss: 0.3948 - val_accuracy: 0.8548\n",
      "Epoch 275/500\n",
      "27/30 [==========================>...] - ETA: 0s - loss: 0.1572 - accuracy: 0.9282\n",
      "Epoch 275: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1609 - accuracy: 0.9279 - val_loss: 0.4514 - val_accuracy: 0.8548\n",
      "Epoch 276/500\n",
      "26/30 [=========================>....] - ETA: 0s - loss: 0.1599 - accuracy: 0.9303\n",
      "Epoch 276: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1573 - accuracy: 0.9311 - val_loss: 0.3862 - val_accuracy: 0.8742\n",
      "Epoch 277/500\n",
      "25/30 [========================>.....] - ETA: 0s - loss: 0.1451 - accuracy: 0.9362\n",
      "Epoch 277: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1435 - accuracy: 0.9365 - val_loss: 0.3955 - val_accuracy: 0.8677\n",
      "Epoch 278/500\n",
      "25/30 [========================>.....] - ETA: 0s - loss: 0.1429 - accuracy: 0.9287\n",
      "Epoch 278: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1455 - accuracy: 0.9268 - val_loss: 0.4015 - val_accuracy: 0.8581\n",
      "Epoch 279/500\n",
      "28/30 [===========================>..] - ETA: 0s - loss: 0.1329 - accuracy: 0.9330\n",
      "Epoch 279: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.1332 - accuracy: 0.9322 - val_loss: 0.3730 - val_accuracy: 0.8710\n",
      "Epoch 280/500\n",
      "26/30 [=========================>....] - ETA: 0s - loss: 0.1407 - accuracy: 0.9339\n",
      "Epoch 280: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1376 - accuracy: 0.9343 - val_loss: 0.3857 - val_accuracy: 0.8710\n",
      "Epoch 281/500\n",
      "26/30 [=========================>....] - ETA: 0s - loss: 0.1266 - accuracy: 0.9303\n",
      "Epoch 281: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1309 - accuracy: 0.9322 - val_loss: 0.4104 - val_accuracy: 0.8677\n",
      "Epoch 282/500\n",
      "25/30 [========================>.....] - ETA: 0s - loss: 0.1408 - accuracy: 0.9250\n",
      "Epoch 282: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1590 - accuracy: 0.9171 - val_loss: 0.4150 - val_accuracy: 0.8452\n",
      "Epoch 283/500\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.1817 - accuracy: 0.9139\n",
      "Epoch 283: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 0.1817 - accuracy: 0.9139 - val_loss: 0.4208 - val_accuracy: 0.8452\n",
      "Epoch 284/500\n",
      "28/30 [===========================>..] - ETA: 0s - loss: 0.1810 - accuracy: 0.9163\n",
      "Epoch 284: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 0.1842 - accuracy: 0.9128 - val_loss: 0.3918 - val_accuracy: 0.8323\n",
      "Epoch 285/500\n",
      "24/30 [=======================>......] - ETA: 0s - loss: 0.1582 - accuracy: 0.9206\n",
      "Epoch 285: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 0.1587 - accuracy: 0.9203 - val_loss: 0.4190 - val_accuracy: 0.8419\n",
      "Epoch 286/500\n",
      "27/30 [==========================>...] - ETA: 0s - loss: 0.1606 - accuracy: 0.9236\n",
      "Epoch 286: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1625 - accuracy: 0.9225 - val_loss: 0.3936 - val_accuracy: 0.8581\n",
      "Epoch 287/500\n",
      "25/30 [========================>.....] - ETA: 0s - loss: 0.1730 - accuracy: 0.9187\n",
      "Epoch 287: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1659 - accuracy: 0.9203 - val_loss: 0.4297 - val_accuracy: 0.8419\n",
      "Epoch 288/500\n",
      "27/30 [==========================>...] - ETA: 0s - loss: 0.1879 - accuracy: 0.9097\n",
      "Epoch 288: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1856 - accuracy: 0.9117 - val_loss: 0.3933 - val_accuracy: 0.8516\n",
      "Epoch 289/500\n",
      "25/30 [========================>.....] - ETA: 0s - loss: 0.1721 - accuracy: 0.9162\n",
      "Epoch 289: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1694 - accuracy: 0.9171 - val_loss: 0.4011 - val_accuracy: 0.8548\n",
      "Epoch 290/500\n",
      "24/30 [=======================>......] - ETA: 0s - loss: 0.1875 - accuracy: 0.8971\n",
      "Epoch 290: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1803 - accuracy: 0.9053 - val_loss: 0.4343 - val_accuracy: 0.8387\n",
      "Epoch 291/500\n",
      "26/30 [=========================>....] - ETA: 0s - loss: 0.1551 - accuracy: 0.9147\n",
      "Epoch 291: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1540 - accuracy: 0.9160 - val_loss: 0.4272 - val_accuracy: 0.8516\n",
      "Epoch 292/500\n",
      "27/30 [==========================>...] - ETA: 0s - loss: 0.1809 - accuracy: 0.8981\n",
      "Epoch 292: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1833 - accuracy: 0.8977 - val_loss: 0.3888 - val_accuracy: 0.8452\n",
      "Epoch 293/500\n",
      "25/30 [========================>.....] - ETA: 0s - loss: 0.1610 - accuracy: 0.9237\n",
      "Epoch 293: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1664 - accuracy: 0.9247 - val_loss: 0.4200 - val_accuracy: 0.8484\n",
      "Epoch 294/500\n",
      "24/30 [=======================>......] - ETA: 0s - loss: 0.1523 - accuracy: 0.9258\n",
      "Epoch 294: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1597 - accuracy: 0.9193 - val_loss: 0.4146 - val_accuracy: 0.8581\n",
      "Epoch 295/500\n",
      "27/30 [==========================>...] - ETA: 0s - loss: 0.1555 - accuracy: 0.9213\n",
      "Epoch 295: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1585 - accuracy: 0.9214 - val_loss: 0.4269 - val_accuracy: 0.8516\n",
      "Epoch 296/500\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.1755 - accuracy: 0.9139\n",
      "Epoch 296: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.1755 - accuracy: 0.9139 - val_loss: 0.4444 - val_accuracy: 0.8548\n",
      "Epoch 297/500\n",
      "25/30 [========================>.....] - ETA: 0s - loss: 0.1509 - accuracy: 0.9350\n",
      "Epoch 297: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1687 - accuracy: 0.9225 - val_loss: 0.4192 - val_accuracy: 0.8452\n",
      "Epoch 298/500\n",
      "28/30 [===========================>..] - ETA: 0s - loss: 0.1594 - accuracy: 0.9152\n",
      "Epoch 298: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1621 - accuracy: 0.9139 - val_loss: 0.4153 - val_accuracy: 0.8677\n",
      "Epoch 299/500\n",
      "26/30 [=========================>....] - ETA: 0s - loss: 0.1569 - accuracy: 0.9291\n",
      "Epoch 299: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1658 - accuracy: 0.9225 - val_loss: 0.4595 - val_accuracy: 0.8484\n",
      "Epoch 300/500\n",
      "25/30 [========================>.....] - ETA: 0s - loss: 0.1554 - accuracy: 0.9237\n",
      "Epoch 300: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1576 - accuracy: 0.9225 - val_loss: 0.4360 - val_accuracy: 0.8516\n",
      "Epoch 301/500\n",
      "26/30 [=========================>....] - ETA: 0s - loss: 0.1607 - accuracy: 0.9171\n",
      "Epoch 301: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1608 - accuracy: 0.9160 - val_loss: 0.4216 - val_accuracy: 0.8548\n",
      "Epoch 302/500\n",
      "27/30 [==========================>...] - ETA: 0s - loss: 0.1529 - accuracy: 0.9340\n",
      "Epoch 302: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1547 - accuracy: 0.9290 - val_loss: 0.4775 - val_accuracy: 0.8355\n",
      "Epoch 303/500\n",
      "26/30 [=========================>....] - ETA: 0s - loss: 0.1572 - accuracy: 0.9171\n",
      "Epoch 303: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1608 - accuracy: 0.9182 - val_loss: 0.4635 - val_accuracy: 0.8387\n",
      "Epoch 304/500\n",
      "25/30 [========================>.....] - ETA: 0s - loss: 0.1687 - accuracy: 0.9175\n",
      "Epoch 304: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1706 - accuracy: 0.9150 - val_loss: 0.4475 - val_accuracy: 0.8419\n",
      "Epoch 305/500\n",
      "28/30 [===========================>..] - ETA: 0s - loss: 0.1628 - accuracy: 0.9107\n",
      "Epoch 305: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1607 - accuracy: 0.9117 - val_loss: 0.4051 - val_accuracy: 0.8516\n",
      "Epoch 306/500\n",
      "25/30 [========================>.....] - ETA: 0s - loss: 0.1612 - accuracy: 0.9237\n",
      "Epoch 306: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1605 - accuracy: 0.9225 - val_loss: 0.4213 - val_accuracy: 0.8387\n",
      "Epoch 307/500\n",
      "25/30 [========================>.....] - ETA: 0s - loss: 0.1492 - accuracy: 0.9237\n",
      "Epoch 307: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.1572 - accuracy: 0.9193 - val_loss: 0.4075 - val_accuracy: 0.8548\n",
      "Epoch 308/500\n",
      "25/30 [========================>.....] - ETA: 0s - loss: 0.1671 - accuracy: 0.9200\n",
      "Epoch 308: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1735 - accuracy: 0.9182 - val_loss: 0.4505 - val_accuracy: 0.8355\n",
      "Epoch 309/500\n",
      "25/30 [========================>.....] - ETA: 0s - loss: 0.1514 - accuracy: 0.9237\n",
      "Epoch 309: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1598 - accuracy: 0.9247 - val_loss: 0.4226 - val_accuracy: 0.8613\n",
      "Epoch 310/500\n",
      "25/30 [========================>.....] - ETA: 0s - loss: 0.1565 - accuracy: 0.9200\n",
      "Epoch 310: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1544 - accuracy: 0.9257 - val_loss: 0.4428 - val_accuracy: 0.8452\n",
      "Epoch 311/500\n",
      "24/30 [=======================>......] - ETA: 0s - loss: 0.1830 - accuracy: 0.9089\n",
      "Epoch 311: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.1778 - accuracy: 0.9128 - val_loss: 0.4666 - val_accuracy: 0.8355\n",
      "Epoch 312/500\n",
      "25/30 [========================>.....] - ETA: 0s - loss: 0.1487 - accuracy: 0.9287\n",
      "Epoch 312: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1506 - accuracy: 0.9247 - val_loss: 0.4253 - val_accuracy: 0.8581\n",
      "Epoch 313/500\n",
      "26/30 [=========================>....] - ETA: 0s - loss: 0.1590 - accuracy: 0.9207\n",
      "Epoch 313: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1602 - accuracy: 0.9182 - val_loss: 0.4465 - val_accuracy: 0.8484\n",
      "Epoch 314/500\n",
      "25/30 [========================>.....] - ETA: 0s - loss: 0.1813 - accuracy: 0.9087\n",
      "Epoch 314: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1732 - accuracy: 0.9128 - val_loss: 0.4760 - val_accuracy: 0.8387\n",
      "Epoch 315/500\n",
      "25/30 [========================>.....] - ETA: 0s - loss: 0.1606 - accuracy: 0.9287\n",
      "Epoch 315: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1612 - accuracy: 0.9247 - val_loss: 0.4203 - val_accuracy: 0.8484\n",
      "Epoch 316/500\n",
      "25/30 [========================>.....] - ETA: 0s - loss: 0.1483 - accuracy: 0.9187\n",
      "Epoch 316: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1483 - accuracy: 0.9203 - val_loss: 0.4846 - val_accuracy: 0.8355\n",
      "Epoch 317/500\n",
      "25/30 [========================>.....] - ETA: 0s - loss: 0.1443 - accuracy: 0.9400\n",
      "Epoch 317: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1564 - accuracy: 0.9343 - val_loss: 0.4374 - val_accuracy: 0.8484\n",
      "Epoch 318/500\n",
      "25/30 [========================>.....] - ETA: 0s - loss: 0.2490 - accuracy: 0.8600\n",
      "Epoch 318: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.2350 - accuracy: 0.8698 - val_loss: 0.4921 - val_accuracy: 0.8194\n",
      "Epoch 319/500\n",
      "26/30 [=========================>....] - ETA: 0s - loss: 0.1880 - accuracy: 0.9075\n",
      "Epoch 319: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1881 - accuracy: 0.9074 - val_loss: 0.4126 - val_accuracy: 0.8355\n",
      "Epoch 320/500\n",
      "24/30 [=======================>......] - ETA: 0s - loss: 0.1681 - accuracy: 0.9245\n",
      "Epoch 320: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.1663 - accuracy: 0.9257 - val_loss: 0.4127 - val_accuracy: 0.8484\n",
      "Epoch 321/500\n",
      "24/30 [=======================>......] - ETA: 0s - loss: 0.1525 - accuracy: 0.9193\n",
      "Epoch 321: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1487 - accuracy: 0.9247 - val_loss: 0.4698 - val_accuracy: 0.8484\n",
      "Epoch 322/500\n",
      "25/30 [========================>.....] - ETA: 0s - loss: 0.1569 - accuracy: 0.9275\n",
      "Epoch 322: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1551 - accuracy: 0.9290 - val_loss: 0.4036 - val_accuracy: 0.8548\n",
      "Epoch 323/500\n",
      "25/30 [========================>.....] - ETA: 0s - loss: 0.1769 - accuracy: 0.9225\n",
      "Epoch 323: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1738 - accuracy: 0.9225 - val_loss: 0.4370 - val_accuracy: 0.8452\n",
      "Epoch 324/500\n",
      "24/30 [=======================>......] - ETA: 0s - loss: 0.1625 - accuracy: 0.9258\n",
      "Epoch 324: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1582 - accuracy: 0.9268 - val_loss: 0.4620 - val_accuracy: 0.8419\n",
      "Epoch 325/500\n",
      "27/30 [==========================>...] - ETA: 0s - loss: 0.1531 - accuracy: 0.9317\n",
      "Epoch 325: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1571 - accuracy: 0.9279 - val_loss: 0.4233 - val_accuracy: 0.8452\n",
      "Epoch 326/500\n",
      "27/30 [==========================>...] - ETA: 0s - loss: 0.1523 - accuracy: 0.9225\n",
      "Epoch 326: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1542 - accuracy: 0.9214 - val_loss: 0.4225 - val_accuracy: 0.8548\n",
      "Epoch 327/500\n",
      "28/30 [===========================>..] - ETA: 0s - loss: 0.1580 - accuracy: 0.9196\n",
      "Epoch 327: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1560 - accuracy: 0.9203 - val_loss: 0.4711 - val_accuracy: 0.8355\n",
      "Epoch 328/500\n",
      "25/30 [========================>.....] - ETA: 0s - loss: 0.1486 - accuracy: 0.9300\n",
      "Epoch 328: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1475 - accuracy: 0.9333 - val_loss: 0.5070 - val_accuracy: 0.8355\n",
      "Epoch 329/500\n",
      "25/30 [========================>.....] - ETA: 0s - loss: 0.1630 - accuracy: 0.9212\n",
      "Epoch 329: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1603 - accuracy: 0.9236 - val_loss: 0.4370 - val_accuracy: 0.8516\n",
      "Epoch 330/500\n",
      "26/30 [=========================>....] - ETA: 0s - loss: 0.1530 - accuracy: 0.9195\n",
      "Epoch 330: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1573 - accuracy: 0.9171 - val_loss: 0.4179 - val_accuracy: 0.8613\n",
      "Epoch 331/500\n",
      "26/30 [=========================>....] - ETA: 0s - loss: 0.1489 - accuracy: 0.9267\n",
      "Epoch 331: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1489 - accuracy: 0.9268 - val_loss: 0.4692 - val_accuracy: 0.8355\n",
      "Epoch 332/500\n",
      "27/30 [==========================>...] - ETA: 0s - loss: 0.1583 - accuracy: 0.9167\n",
      "Epoch 332: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1566 - accuracy: 0.9193 - val_loss: 0.4265 - val_accuracy: 0.8516\n",
      "Epoch 333/500\n",
      "28/30 [===========================>..] - ETA: 0s - loss: 0.1585 - accuracy: 0.9185\n",
      "Epoch 333: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1613 - accuracy: 0.9160 - val_loss: 0.4619 - val_accuracy: 0.8355\n",
      "Epoch 334/500\n",
      "25/30 [========================>.....] - ETA: 0s - loss: 0.1629 - accuracy: 0.9262\n",
      "Epoch 334: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1530 - accuracy: 0.9311 - val_loss: 0.4729 - val_accuracy: 0.8323\n",
      "Epoch 335/500\n",
      "27/30 [==========================>...] - ETA: 0s - loss: 0.1503 - accuracy: 0.9271\n",
      "Epoch 335: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1490 - accuracy: 0.9268 - val_loss: 0.4949 - val_accuracy: 0.8355\n",
      "Epoch 336/500\n",
      "28/30 [===========================>..] - ETA: 0s - loss: 0.1624 - accuracy: 0.9141\n",
      "Epoch 336: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.1597 - accuracy: 0.9171 - val_loss: 0.4768 - val_accuracy: 0.8387\n",
      "Epoch 337/500\n",
      "26/30 [=========================>....] - ETA: 0s - loss: 0.1562 - accuracy: 0.9243\n",
      "Epoch 337: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1538 - accuracy: 0.9257 - val_loss: 0.4457 - val_accuracy: 0.8516\n",
      "Epoch 338/500\n",
      "26/30 [=========================>....] - ETA: 0s - loss: 0.1355 - accuracy: 0.9339\n",
      "Epoch 338: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1416 - accuracy: 0.9290 - val_loss: 0.4538 - val_accuracy: 0.8581\n",
      "Epoch 339/500\n",
      "28/30 [===========================>..] - ETA: 0s - loss: 0.1575 - accuracy: 0.9219\n",
      "Epoch 339: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1567 - accuracy: 0.9214 - val_loss: 0.4275 - val_accuracy: 0.8452\n",
      "Epoch 340/500\n",
      "28/30 [===========================>..] - ETA: 0s - loss: 0.1649 - accuracy: 0.9252\n",
      "Epoch 340: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.1629 - accuracy: 0.9257 - val_loss: 0.4415 - val_accuracy: 0.8387\n",
      "Epoch 341/500\n",
      "25/30 [========================>.....] - ETA: 0s - loss: 0.1584 - accuracy: 0.9150\n",
      "Epoch 341: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1566 - accuracy: 0.9171 - val_loss: 0.4249 - val_accuracy: 0.8484\n",
      "Epoch 342/500\n",
      "27/30 [==========================>...] - ETA: 0s - loss: 0.1396 - accuracy: 0.9387\n",
      "Epoch 342: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1399 - accuracy: 0.9386 - val_loss: 0.4692 - val_accuracy: 0.8387\n",
      "Epoch 343/500\n",
      "28/30 [===========================>..] - ETA: 0s - loss: 0.1641 - accuracy: 0.9185\n",
      "Epoch 343: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1647 - accuracy: 0.9182 - val_loss: 0.4193 - val_accuracy: 0.8548\n",
      "Epoch 344/500\n",
      "26/30 [=========================>....] - ETA: 0s - loss: 0.1406 - accuracy: 0.9195\n",
      "Epoch 344: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1329 - accuracy: 0.9236 - val_loss: 0.4899 - val_accuracy: 0.8355\n",
      "Epoch 345/500\n",
      "26/30 [=========================>....] - ETA: 0s - loss: 0.1294 - accuracy: 0.9411\n",
      "Epoch 345: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1299 - accuracy: 0.9408 - val_loss: 0.4236 - val_accuracy: 0.8613\n",
      "Epoch 346/500\n",
      "28/30 [===========================>..] - ETA: 0s - loss: 0.1543 - accuracy: 0.9252\n",
      "Epoch 346: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1589 - accuracy: 0.9225 - val_loss: 0.4285 - val_accuracy: 0.8516\n",
      "Epoch 347/500\n",
      "28/30 [===========================>..] - ETA: 0s - loss: 0.1982 - accuracy: 0.9141\n",
      "Epoch 347: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.2003 - accuracy: 0.9107 - val_loss: 0.4749 - val_accuracy: 0.8194\n",
      "Epoch 348/500\n",
      "26/30 [=========================>....] - ETA: 0s - loss: 0.1630 - accuracy: 0.9147\n",
      "Epoch 348: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1597 - accuracy: 0.9171 - val_loss: 0.4251 - val_accuracy: 0.8419\n",
      "Epoch 349/500\n",
      "28/30 [===========================>..] - ETA: 0s - loss: 0.1689 - accuracy: 0.9163\n",
      "Epoch 349: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1687 - accuracy: 0.9171 - val_loss: 0.4121 - val_accuracy: 0.8484\n",
      "Epoch 350/500\n",
      "26/30 [=========================>....] - ETA: 0s - loss: 0.1494 - accuracy: 0.9279\n",
      "Epoch 350: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1518 - accuracy: 0.9236 - val_loss: 0.4455 - val_accuracy: 0.8452\n",
      "Epoch 351/500\n",
      "27/30 [==========================>...] - ETA: 0s - loss: 0.1584 - accuracy: 0.9248\n",
      "Epoch 351: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1521 - accuracy: 0.9279 - val_loss: 0.4623 - val_accuracy: 0.8355\n",
      "Epoch 352/500\n",
      "25/30 [========================>.....] - ETA: 0s - loss: 0.1368 - accuracy: 0.9388\n",
      "Epoch 352: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1348 - accuracy: 0.9429 - val_loss: 0.4941 - val_accuracy: 0.8419\n",
      "Epoch 353/500\n",
      "27/30 [==========================>...] - ETA: 0s - loss: 0.1524 - accuracy: 0.9306\n",
      "Epoch 353: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1552 - accuracy: 0.9279 - val_loss: 0.4650 - val_accuracy: 0.8355\n",
      "Epoch 354/500\n",
      "26/30 [=========================>....] - ETA: 0s - loss: 0.1452 - accuracy: 0.9231\n",
      "Epoch 354: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1487 - accuracy: 0.9203 - val_loss: 0.5426 - val_accuracy: 0.8323\n",
      "Epoch 355/500\n",
      "27/30 [==========================>...] - ETA: 0s - loss: 0.1441 - accuracy: 0.9190\n",
      "Epoch 355: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1458 - accuracy: 0.9182 - val_loss: 0.4644 - val_accuracy: 0.8516\n",
      "Epoch 356/500\n",
      "28/30 [===========================>..] - ETA: 0s - loss: 0.1535 - accuracy: 0.9297\n",
      "Epoch 356: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1537 - accuracy: 0.9290 - val_loss: 0.4924 - val_accuracy: 0.8452\n",
      "Epoch 357/500\n",
      "25/30 [========================>.....] - ETA: 0s - loss: 0.1643 - accuracy: 0.9175\n",
      "Epoch 357: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1631 - accuracy: 0.9193 - val_loss: 0.5544 - val_accuracy: 0.8194\n",
      "Epoch 358/500\n",
      "27/30 [==========================>...] - ETA: 0s - loss: 0.1845 - accuracy: 0.9155\n",
      "Epoch 358: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1842 - accuracy: 0.9171 - val_loss: 0.4320 - val_accuracy: 0.8258\n",
      "Epoch 359/500\n",
      "26/30 [=========================>....] - ETA: 0s - loss: 0.1454 - accuracy: 0.9291\n",
      "Epoch 359: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1487 - accuracy: 0.9268 - val_loss: 0.4189 - val_accuracy: 0.8581\n",
      "Epoch 360/500\n",
      "27/30 [==========================>...] - ETA: 0s - loss: 0.1479 - accuracy: 0.9201\n",
      "Epoch 360: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1487 - accuracy: 0.9225 - val_loss: 0.4551 - val_accuracy: 0.8516\n",
      "Epoch 361/500\n",
      "25/30 [========================>.....] - ETA: 0s - loss: 0.1533 - accuracy: 0.9225\n",
      "Epoch 361: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1528 - accuracy: 0.9214 - val_loss: 0.4228 - val_accuracy: 0.8387\n",
      "Epoch 362/500\n",
      "25/30 [========================>.....] - ETA: 0s - loss: 0.1701 - accuracy: 0.9200\n",
      "Epoch 362: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1705 - accuracy: 0.9193 - val_loss: 0.5295 - val_accuracy: 0.8129\n",
      "Epoch 363/500\n",
      "26/30 [=========================>....] - ETA: 0s - loss: 0.1450 - accuracy: 0.9231\n",
      "Epoch 363: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1523 - accuracy: 0.9203 - val_loss: 0.4380 - val_accuracy: 0.8677\n",
      "Epoch 364/500\n",
      "27/30 [==========================>...] - ETA: 0s - loss: 0.1541 - accuracy: 0.9317\n",
      "Epoch 364: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.1554 - accuracy: 0.9290 - val_loss: 0.4376 - val_accuracy: 0.8548\n",
      "Epoch 365/500\n",
      "26/30 [=========================>....] - ETA: 0s - loss: 0.1448 - accuracy: 0.9267\n",
      "Epoch 365: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1406 - accuracy: 0.9300 - val_loss: 0.4475 - val_accuracy: 0.8581\n",
      "Epoch 366/500\n",
      "26/30 [=========================>....] - ETA: 0s - loss: 0.1486 - accuracy: 0.9255\n",
      "Epoch 366: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1495 - accuracy: 0.9247 - val_loss: 0.4542 - val_accuracy: 0.8548\n",
      "Epoch 367/500\n",
      "27/30 [==========================>...] - ETA: 0s - loss: 0.1398 - accuracy: 0.9155\n",
      "Epoch 367: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1378 - accuracy: 0.9160 - val_loss: 0.4802 - val_accuracy: 0.8419\n",
      "Epoch 368/500\n",
      "28/30 [===========================>..] - ETA: 0s - loss: 0.1572 - accuracy: 0.9241\n",
      "Epoch 368: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1563 - accuracy: 0.9247 - val_loss: 0.4609 - val_accuracy: 0.8516\n",
      "Epoch 369/500\n",
      "26/30 [=========================>....] - ETA: 0s - loss: 0.1520 - accuracy: 0.9267\n",
      "Epoch 369: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1495 - accuracy: 0.9279 - val_loss: 0.4504 - val_accuracy: 0.8548\n",
      "Epoch 370/500\n",
      "27/30 [==========================>...] - ETA: 0s - loss: 0.1443 - accuracy: 0.9363\n",
      "Epoch 370: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1438 - accuracy: 0.9354 - val_loss: 0.4434 - val_accuracy: 0.8516\n",
      "Epoch 371/500\n",
      "27/30 [==========================>...] - ETA: 0s - loss: 0.1459 - accuracy: 0.9317\n",
      "Epoch 371: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1474 - accuracy: 0.9311 - val_loss: 0.4436 - val_accuracy: 0.8613\n",
      "Epoch 372/500\n",
      "26/30 [=========================>....] - ETA: 0s - loss: 0.1521 - accuracy: 0.9195\n",
      "Epoch 372: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1462 - accuracy: 0.9257 - val_loss: 0.5101 - val_accuracy: 0.8516\n",
      "Epoch 373/500\n",
      "28/30 [===========================>..] - ETA: 0s - loss: 0.1475 - accuracy: 0.9308\n",
      "Epoch 373: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1442 - accuracy: 0.9322 - val_loss: 0.4788 - val_accuracy: 0.8387\n",
      "Epoch 374/500\n",
      "26/30 [=========================>....] - ETA: 0s - loss: 0.1430 - accuracy: 0.9291\n",
      "Epoch 374: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1421 - accuracy: 0.9300 - val_loss: 0.4806 - val_accuracy: 0.8419\n",
      "Epoch 375/500\n",
      "25/30 [========================>.....] - ETA: 0s - loss: 0.1552 - accuracy: 0.9337\n",
      "Epoch 375: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1448 - accuracy: 0.9376 - val_loss: 0.4693 - val_accuracy: 0.8484\n",
      "Epoch 376/500\n",
      "27/30 [==========================>...] - ETA: 0s - loss: 0.1585 - accuracy: 0.9178\n",
      "Epoch 376: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1528 - accuracy: 0.9193 - val_loss: 0.5371 - val_accuracy: 0.8226\n",
      "Epoch 377/500\n",
      "28/30 [===========================>..] - ETA: 0s - loss: 0.1542 - accuracy: 0.9230\n",
      "Epoch 377: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1596 - accuracy: 0.9193 - val_loss: 0.4905 - val_accuracy: 0.8516\n",
      "Epoch 378/500\n",
      "25/30 [========================>.....] - ETA: 0s - loss: 0.1442 - accuracy: 0.9250\n",
      "Epoch 378: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1417 - accuracy: 0.9236 - val_loss: 0.4873 - val_accuracy: 0.8581\n",
      "Epoch 379/500\n",
      "28/30 [===========================>..] - ETA: 0s - loss: 0.1446 - accuracy: 0.9196\n",
      "Epoch 379: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1440 - accuracy: 0.9214 - val_loss: 0.5212 - val_accuracy: 0.8613\n",
      "Epoch 380/500\n",
      "27/30 [==========================>...] - ETA: 0s - loss: 0.1428 - accuracy: 0.9236\n",
      "Epoch 380: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1388 - accuracy: 0.9268 - val_loss: 0.4815 - val_accuracy: 0.8613\n",
      "Epoch 381/500\n",
      "25/30 [========================>.....] - ETA: 0s - loss: 0.1548 - accuracy: 0.9200\n",
      "Epoch 381: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1583 - accuracy: 0.9193 - val_loss: 0.4759 - val_accuracy: 0.8419\n",
      "Epoch 382/500\n",
      "28/30 [===========================>..] - ETA: 0s - loss: 0.1518 - accuracy: 0.9308\n",
      "Epoch 382: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1527 - accuracy: 0.9300 - val_loss: 0.5477 - val_accuracy: 0.8323\n",
      "Epoch 383/500\n",
      "26/30 [=========================>....] - ETA: 0s - loss: 0.1377 - accuracy: 0.9267\n",
      "Epoch 383: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1359 - accuracy: 0.9290 - val_loss: 0.4906 - val_accuracy: 0.8548\n",
      "Epoch 384/500\n",
      "28/30 [===========================>..] - ETA: 0s - loss: 0.1609 - accuracy: 0.9219\n",
      "Epoch 384: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1571 - accuracy: 0.9236 - val_loss: 0.5209 - val_accuracy: 0.8452\n",
      "Epoch 385/500\n",
      "26/30 [=========================>....] - ETA: 0s - loss: 0.1333 - accuracy: 0.9327\n",
      "Epoch 385: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1343 - accuracy: 0.9333 - val_loss: 0.5101 - val_accuracy: 0.8419\n",
      "Epoch 386/500\n",
      "28/30 [===========================>..] - ETA: 0s - loss: 0.1552 - accuracy: 0.9252\n",
      "Epoch 386: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1533 - accuracy: 0.9268 - val_loss: 0.5249 - val_accuracy: 0.8387\n",
      "Epoch 387/500\n",
      "26/30 [=========================>....] - ETA: 0s - loss: 0.1606 - accuracy: 0.9303\n",
      "Epoch 387: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1589 - accuracy: 0.9290 - val_loss: 0.4794 - val_accuracy: 0.8452\n",
      "Epoch 388/500\n",
      "26/30 [=========================>....] - ETA: 0s - loss: 0.1286 - accuracy: 0.9375\n",
      "Epoch 388: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1327 - accuracy: 0.9322 - val_loss: 0.5005 - val_accuracy: 0.8516\n",
      "Epoch 389/500\n",
      "27/30 [==========================>...] - ETA: 0s - loss: 0.1464 - accuracy: 0.9329\n",
      "Epoch 389: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1422 - accuracy: 0.9354 - val_loss: 0.4928 - val_accuracy: 0.8484\n",
      "Epoch 390/500\n",
      "25/30 [========================>.....] - ETA: 0s - loss: 0.1479 - accuracy: 0.9287\n",
      "Epoch 390: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1449 - accuracy: 0.9322 - val_loss: 0.4545 - val_accuracy: 0.8645\n",
      "Epoch 391/500\n",
      "27/30 [==========================>...] - ETA: 0s - loss: 0.1485 - accuracy: 0.9282\n",
      "Epoch 391: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1465 - accuracy: 0.9279 - val_loss: 0.4687 - val_accuracy: 0.8645\n",
      "Epoch 392/500\n",
      "25/30 [========================>.....] - ETA: 0s - loss: 0.1439 - accuracy: 0.9300\n",
      "Epoch 392: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1464 - accuracy: 0.9290 - val_loss: 0.5131 - val_accuracy: 0.8581\n",
      "Epoch 393/500\n",
      "26/30 [=========================>....] - ETA: 0s - loss: 0.1720 - accuracy: 0.9255\n",
      "Epoch 393: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1777 - accuracy: 0.9214 - val_loss: 0.4368 - val_accuracy: 0.8613\n",
      "Epoch 394/500\n",
      "27/30 [==========================>...] - ETA: 0s - loss: 0.1568 - accuracy: 0.9340\n",
      "Epoch 394: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1522 - accuracy: 0.9354 - val_loss: 0.4652 - val_accuracy: 0.8613\n",
      "Epoch 395/500\n",
      "27/30 [==========================>...] - ETA: 0s - loss: 0.1305 - accuracy: 0.9352\n",
      "Epoch 395: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1320 - accuracy: 0.9354 - val_loss: 0.4624 - val_accuracy: 0.8581\n",
      "Epoch 396/500\n",
      "25/30 [========================>.....] - ETA: 0s - loss: 0.1493 - accuracy: 0.9200\n",
      "Epoch 396: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1442 - accuracy: 0.9236 - val_loss: 0.4975 - val_accuracy: 0.8516\n",
      "Epoch 397/500\n",
      "26/30 [=========================>....] - ETA: 0s - loss: 0.1448 - accuracy: 0.9315\n",
      "Epoch 397: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1477 - accuracy: 0.9279 - val_loss: 0.4516 - val_accuracy: 0.8452\n",
      "Epoch 398/500\n",
      "27/30 [==========================>...] - ETA: 0s - loss: 0.1543 - accuracy: 0.9363\n",
      "Epoch 398: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1561 - accuracy: 0.9343 - val_loss: 0.5199 - val_accuracy: 0.8516\n",
      "Epoch 399/500\n",
      "26/30 [=========================>....] - ETA: 0s - loss: 0.1357 - accuracy: 0.9291\n",
      "Epoch 399: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1458 - accuracy: 0.9290 - val_loss: 0.5019 - val_accuracy: 0.8581\n",
      "Epoch 400/500\n",
      "26/30 [=========================>....] - ETA: 0s - loss: 0.1542 - accuracy: 0.9147\n",
      "Epoch 400: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.1504 - accuracy: 0.9171 - val_loss: 0.4797 - val_accuracy: 0.8613\n",
      "Epoch 401/500\n",
      "27/30 [==========================>...] - ETA: 0s - loss: 0.1522 - accuracy: 0.9317\n",
      "Epoch 401: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1513 - accuracy: 0.9322 - val_loss: 0.4581 - val_accuracy: 0.8645\n",
      "Epoch 402/500\n",
      "25/30 [========================>.....] - ETA: 0s - loss: 0.1560 - accuracy: 0.9200\n",
      "Epoch 402: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1604 - accuracy: 0.9182 - val_loss: 0.4548 - val_accuracy: 0.8742\n",
      "Epoch 403/500\n",
      "25/30 [========================>.....] - ETA: 0s - loss: 0.1376 - accuracy: 0.9337\n",
      "Epoch 403: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1418 - accuracy: 0.9343 - val_loss: 0.4641 - val_accuracy: 0.8677\n",
      "Epoch 404/500\n",
      "26/30 [=========================>....] - ETA: 0s - loss: 0.1644 - accuracy: 0.9243\n",
      "Epoch 404: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1615 - accuracy: 0.9236 - val_loss: 0.4723 - val_accuracy: 0.8677\n",
      "Epoch 405/500\n",
      "25/30 [========================>.....] - ETA: 0s - loss: 0.1528 - accuracy: 0.9275\n",
      "Epoch 405: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1510 - accuracy: 0.9290 - val_loss: 0.4725 - val_accuracy: 0.8710\n",
      "Epoch 406/500\n",
      "26/30 [=========================>....] - ETA: 0s - loss: 0.1462 - accuracy: 0.9267\n",
      "Epoch 406: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1470 - accuracy: 0.9279 - val_loss: 0.4643 - val_accuracy: 0.8613\n",
      "Epoch 407/500\n",
      "28/30 [===========================>..] - ETA: 0s - loss: 0.1832 - accuracy: 0.9096\n",
      "Epoch 407: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1791 - accuracy: 0.9117 - val_loss: 0.5098 - val_accuracy: 0.8548\n",
      "Epoch 408/500\n",
      "25/30 [========================>.....] - ETA: 0s - loss: 0.1569 - accuracy: 0.9162\n",
      "Epoch 408: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1556 - accuracy: 0.9150 - val_loss: 0.4824 - val_accuracy: 0.8581\n",
      "Epoch 409/500\n",
      "27/30 [==========================>...] - ETA: 0s - loss: 0.1231 - accuracy: 0.9398\n",
      "Epoch 409: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1338 - accuracy: 0.9333 - val_loss: 0.4965 - val_accuracy: 0.8548\n",
      "Epoch 410/500\n",
      "28/30 [===========================>..] - ETA: 0s - loss: 0.1371 - accuracy: 0.9297\n",
      "Epoch 410: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.1348 - accuracy: 0.9311 - val_loss: 0.5289 - val_accuracy: 0.8323\n",
      "Epoch 411/500\n",
      "26/30 [=========================>....] - ETA: 0s - loss: 0.1531 - accuracy: 0.9267\n",
      "Epoch 411: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1530 - accuracy: 0.9268 - val_loss: 0.4559 - val_accuracy: 0.8516\n",
      "Epoch 412/500\n",
      "27/30 [==========================>...] - ETA: 0s - loss: 0.1388 - accuracy: 0.9340\n",
      "Epoch 412: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1419 - accuracy: 0.9311 - val_loss: 0.4734 - val_accuracy: 0.8839\n",
      "Epoch 413/500\n",
      "27/30 [==========================>...] - ETA: 0s - loss: 0.1525 - accuracy: 0.9317\n",
      "Epoch 413: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1536 - accuracy: 0.9311 - val_loss: 0.5216 - val_accuracy: 0.8645\n",
      "Epoch 414/500\n",
      "25/30 [========================>.....] - ETA: 0s - loss: 0.1377 - accuracy: 0.9350\n",
      "Epoch 414: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1337 - accuracy: 0.9354 - val_loss: 0.4856 - val_accuracy: 0.8548\n",
      "Epoch 415/500\n",
      "27/30 [==========================>...] - ETA: 0s - loss: 0.1506 - accuracy: 0.9375\n",
      "Epoch 415: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1492 - accuracy: 0.9354 - val_loss: 0.4813 - val_accuracy: 0.8645\n",
      "Epoch 416/500\n",
      "27/30 [==========================>...] - ETA: 0s - loss: 0.1344 - accuracy: 0.9421\n",
      "Epoch 416: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1289 - accuracy: 0.9462 - val_loss: 0.4608 - val_accuracy: 0.8645\n",
      "Epoch 417/500\n",
      "24/30 [=======================>......] - ETA: 0s - loss: 0.1424 - accuracy: 0.9297\n",
      "Epoch 417: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1465 - accuracy: 0.9290 - val_loss: 0.4846 - val_accuracy: 0.8742\n",
      "Epoch 418/500\n",
      "25/30 [========================>.....] - ETA: 0s - loss: 0.1423 - accuracy: 0.9300\n",
      "Epoch 418: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1417 - accuracy: 0.9333 - val_loss: 0.5271 - val_accuracy: 0.8484\n",
      "Epoch 419/500\n",
      "27/30 [==========================>...] - ETA: 0s - loss: 0.1338 - accuracy: 0.9317\n",
      "Epoch 419: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1328 - accuracy: 0.9333 - val_loss: 0.4543 - val_accuracy: 0.8677\n",
      "Epoch 420/500\n",
      "25/30 [========================>.....] - ETA: 0s - loss: 0.1454 - accuracy: 0.9287\n",
      "Epoch 420: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1425 - accuracy: 0.9322 - val_loss: 0.5403 - val_accuracy: 0.8548\n",
      "Epoch 421/500\n",
      "26/30 [=========================>....] - ETA: 0s - loss: 0.1368 - accuracy: 0.9267\n",
      "Epoch 421: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1348 - accuracy: 0.9268 - val_loss: 0.4639 - val_accuracy: 0.8806\n",
      "Epoch 422/500\n",
      "28/30 [===========================>..] - ETA: 0s - loss: 0.1358 - accuracy: 0.9286\n",
      "Epoch 422: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1365 - accuracy: 0.9290 - val_loss: 0.5238 - val_accuracy: 0.8581\n",
      "Epoch 423/500\n",
      "26/30 [=========================>....] - ETA: 0s - loss: 0.1422 - accuracy: 0.9351\n",
      "Epoch 423: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1383 - accuracy: 0.9397 - val_loss: 0.5204 - val_accuracy: 0.8581\n",
      "Epoch 424/500\n",
      "28/30 [===========================>..] - ETA: 0s - loss: 0.1384 - accuracy: 0.9308\n",
      "Epoch 424: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1417 - accuracy: 0.9300 - val_loss: 0.5034 - val_accuracy: 0.8742\n",
      "Epoch 425/500\n",
      "26/30 [=========================>....] - ETA: 0s - loss: 0.1481 - accuracy: 0.9243\n",
      "Epoch 425: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1506 - accuracy: 0.9247 - val_loss: 0.5032 - val_accuracy: 0.8645\n",
      "Epoch 426/500\n",
      "26/30 [=========================>....] - ETA: 0s - loss: 0.1274 - accuracy: 0.9375\n",
      "Epoch 426: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1239 - accuracy: 0.9419 - val_loss: 0.4944 - val_accuracy: 0.8710\n",
      "Epoch 427/500\n",
      "26/30 [=========================>....] - ETA: 0s - loss: 0.1369 - accuracy: 0.9399\n",
      "Epoch 427: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1410 - accuracy: 0.9354 - val_loss: 0.4703 - val_accuracy: 0.8645\n",
      "Epoch 428/500\n",
      "26/30 [=========================>....] - ETA: 0s - loss: 0.1369 - accuracy: 0.9375\n",
      "Epoch 428: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1403 - accuracy: 0.9376 - val_loss: 0.4692 - val_accuracy: 0.8645\n",
      "Epoch 429/500\n",
      "25/30 [========================>.....] - ETA: 0s - loss: 0.1322 - accuracy: 0.9350\n",
      "Epoch 429: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1297 - accuracy: 0.9386 - val_loss: 0.4968 - val_accuracy: 0.8645\n",
      "Epoch 430/500\n",
      "27/30 [==========================>...] - ETA: 0s - loss: 0.1453 - accuracy: 0.9271\n",
      "Epoch 430: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1420 - accuracy: 0.9290 - val_loss: 0.5023 - val_accuracy: 0.8742\n",
      "Epoch 431/500\n",
      "27/30 [==========================>...] - ETA: 0s - loss: 0.1230 - accuracy: 0.9468\n",
      "Epoch 431: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1203 - accuracy: 0.9483 - val_loss: 0.5076 - val_accuracy: 0.8645\n",
      "Epoch 432/500\n",
      "26/30 [=========================>....] - ETA: 0s - loss: 0.1455 - accuracy: 0.9267\n",
      "Epoch 432: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1407 - accuracy: 0.9290 - val_loss: 0.4829 - val_accuracy: 0.8710\n",
      "Epoch 433/500\n",
      "24/30 [=======================>......] - ETA: 0s - loss: 0.1365 - accuracy: 0.9440\n",
      "Epoch 433: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1433 - accuracy: 0.9419 - val_loss: 0.5080 - val_accuracy: 0.8774\n",
      "Epoch 434/500\n",
      "26/30 [=========================>....] - ETA: 0s - loss: 0.1510 - accuracy: 0.9363\n",
      "Epoch 434: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1491 - accuracy: 0.9386 - val_loss: 0.4942 - val_accuracy: 0.8710\n",
      "Epoch 435/500\n",
      "27/30 [==========================>...] - ETA: 0s - loss: 0.1371 - accuracy: 0.9433\n",
      "Epoch 435: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1335 - accuracy: 0.9440 - val_loss: 0.4910 - val_accuracy: 0.8677\n",
      "Epoch 436/500\n",
      "25/30 [========================>.....] - ETA: 0s - loss: 0.1482 - accuracy: 0.9262\n",
      "Epoch 436: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1503 - accuracy: 0.9247 - val_loss: 0.4927 - val_accuracy: 0.8742\n",
      "Epoch 437/500\n",
      "26/30 [=========================>....] - ETA: 0s - loss: 0.1528 - accuracy: 0.9291\n",
      "Epoch 437: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1476 - accuracy: 0.9311 - val_loss: 0.4830 - val_accuracy: 0.8581\n",
      "Epoch 438/500\n",
      "28/30 [===========================>..] - ETA: 0s - loss: 0.1370 - accuracy: 0.9408\n",
      "Epoch 438: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1372 - accuracy: 0.9419 - val_loss: 0.4446 - val_accuracy: 0.8774\n",
      "Epoch 439/500\n",
      "25/30 [========================>.....] - ETA: 0s - loss: 0.1391 - accuracy: 0.9312\n",
      "Epoch 439: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1405 - accuracy: 0.9322 - val_loss: 0.4460 - val_accuracy: 0.8742\n",
      "Epoch 440/500\n",
      "25/30 [========================>.....] - ETA: 0s - loss: 0.1331 - accuracy: 0.9425\n",
      "Epoch 440: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1319 - accuracy: 0.9408 - val_loss: 0.4668 - val_accuracy: 0.8677\n",
      "Epoch 441/500\n",
      "24/30 [=======================>......] - ETA: 0s - loss: 0.1347 - accuracy: 0.9310\n",
      "Epoch 441: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1294 - accuracy: 0.9376 - val_loss: 0.4672 - val_accuracy: 0.8645\n",
      "Epoch 442/500\n",
      "25/30 [========================>.....] - ETA: 0s - loss: 0.1677 - accuracy: 0.9212\n",
      "Epoch 442: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1568 - accuracy: 0.9247 - val_loss: 0.5017 - val_accuracy: 0.8581\n",
      "Epoch 443/500\n",
      "26/30 [=========================>....] - ETA: 0s - loss: 0.1415 - accuracy: 0.9267\n",
      "Epoch 443: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1390 - accuracy: 0.9290 - val_loss: 0.4577 - val_accuracy: 0.8710\n",
      "Epoch 444/500\n",
      "26/30 [=========================>....] - ETA: 0s - loss: 0.1342 - accuracy: 0.9291\n",
      "Epoch 444: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1349 - accuracy: 0.9279 - val_loss: 0.4656 - val_accuracy: 0.8677\n",
      "Epoch 445/500\n",
      "25/30 [========================>.....] - ETA: 0s - loss: 0.1471 - accuracy: 0.9287\n",
      "Epoch 445: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1459 - accuracy: 0.9290 - val_loss: 0.4959 - val_accuracy: 0.8613\n",
      "Epoch 446/500\n",
      "24/30 [=======================>......] - ETA: 0s - loss: 0.1526 - accuracy: 0.9336\n",
      "Epoch 446: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1510 - accuracy: 0.9322 - val_loss: 0.4459 - val_accuracy: 0.8677\n",
      "Epoch 447/500\n",
      "28/30 [===========================>..] - ETA: 0s - loss: 0.1336 - accuracy: 0.9364\n",
      "Epoch 447: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.1342 - accuracy: 0.9365 - val_loss: 0.4836 - val_accuracy: 0.8677\n",
      "Epoch 448/500\n",
      "26/30 [=========================>....] - ETA: 0s - loss: 0.1273 - accuracy: 0.9471\n",
      "Epoch 448: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1292 - accuracy: 0.9451 - val_loss: 0.5152 - val_accuracy: 0.8710\n",
      "Epoch 449/500\n",
      "26/30 [=========================>....] - ETA: 0s - loss: 0.1350 - accuracy: 0.9399\n",
      "Epoch 449: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1371 - accuracy: 0.9386 - val_loss: 0.5705 - val_accuracy: 0.8613\n",
      "Epoch 450/500\n",
      "25/30 [========================>.....] - ETA: 0s - loss: 0.1428 - accuracy: 0.9337\n",
      "Epoch 450: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1463 - accuracy: 0.9311 - val_loss: 0.4667 - val_accuracy: 0.8806\n",
      "Epoch 451/500\n",
      "26/30 [=========================>....] - ETA: 0s - loss: 0.1313 - accuracy: 0.9339\n",
      "Epoch 451: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.1306 - accuracy: 0.9333 - val_loss: 0.5176 - val_accuracy: 0.8677\n",
      "Epoch 452/500\n",
      "25/30 [========================>.....] - ETA: 0s - loss: 0.1405 - accuracy: 0.9275\n",
      "Epoch 452: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1376 - accuracy: 0.9311 - val_loss: 0.5447 - val_accuracy: 0.8613\n",
      "Epoch 453/500\n",
      "25/30 [========================>.....] - ETA: 0s - loss: 0.1197 - accuracy: 0.9438\n",
      "Epoch 453: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.1255 - accuracy: 0.9429 - val_loss: 0.5291 - val_accuracy: 0.8710\n",
      "Epoch 454/500\n",
      "24/30 [=======================>......] - ETA: 0s - loss: 0.1564 - accuracy: 0.9271\n",
      "Epoch 454: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1493 - accuracy: 0.9333 - val_loss: 0.5285 - val_accuracy: 0.8516\n",
      "Epoch 455/500\n",
      "25/30 [========================>.....] - ETA: 0s - loss: 0.1420 - accuracy: 0.9325\n",
      "Epoch 455: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1361 - accuracy: 0.9365 - val_loss: 0.4983 - val_accuracy: 0.8710\n",
      "Epoch 456/500\n",
      "24/30 [=======================>......] - ETA: 0s - loss: 0.1352 - accuracy: 0.9375\n",
      "Epoch 456: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.1349 - accuracy: 0.9354 - val_loss: 0.4979 - val_accuracy: 0.8677\n",
      "Epoch 457/500\n",
      "23/30 [======================>.......] - ETA: 0s - loss: 0.1416 - accuracy: 0.9389\n",
      "Epoch 457: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.1368 - accuracy: 0.9386 - val_loss: 0.4900 - val_accuracy: 0.8613\n",
      "Epoch 458/500\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.1337 - accuracy: 0.9376\n",
      "Epoch 458: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.1337 - accuracy: 0.9376 - val_loss: 0.5209 - val_accuracy: 0.8581\n",
      "Epoch 459/500\n",
      "27/30 [==========================>...] - ETA: 0s - loss: 0.1367 - accuracy: 0.9398\n",
      "Epoch 459: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 0.1346 - accuracy: 0.9419 - val_loss: 0.4907 - val_accuracy: 0.8645\n",
      "Epoch 460/500\n",
      "25/30 [========================>.....] - ETA: 0s - loss: 0.1272 - accuracy: 0.9475\n",
      "Epoch 460: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.1307 - accuracy: 0.9419 - val_loss: 0.5252 - val_accuracy: 0.8677\n",
      "Epoch 461/500\n",
      "25/30 [========================>.....] - ETA: 0s - loss: 0.1232 - accuracy: 0.9337\n",
      "Epoch 461: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1247 - accuracy: 0.9322 - val_loss: 0.5240 - val_accuracy: 0.8774\n",
      "Epoch 462/500\n",
      "23/30 [======================>.......] - ETA: 0s - loss: 0.1235 - accuracy: 0.9334\n",
      "Epoch 462: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.1315 - accuracy: 0.9322 - val_loss: 0.5454 - val_accuracy: 0.8645\n",
      "Epoch 463/500\n",
      "25/30 [========================>.....] - ETA: 0s - loss: 0.1159 - accuracy: 0.9425\n",
      "Epoch 463: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1146 - accuracy: 0.9440 - val_loss: 0.5525 - val_accuracy: 0.8710\n",
      "Epoch 464/500\n",
      "27/30 [==========================>...] - ETA: 0s - loss: 0.1182 - accuracy: 0.9398\n",
      "Epoch 464: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1179 - accuracy: 0.9376 - val_loss: 0.5434 - val_accuracy: 0.8677\n",
      "Epoch 465/500\n",
      "24/30 [=======================>......] - ETA: 0s - loss: 0.1243 - accuracy: 0.9453\n",
      "Epoch 465: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1324 - accuracy: 0.9365 - val_loss: 0.5101 - val_accuracy: 0.8677\n",
      "Epoch 466/500\n",
      "25/30 [========================>.....] - ETA: 0s - loss: 0.1412 - accuracy: 0.9287\n",
      "Epoch 466: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1440 - accuracy: 0.9268 - val_loss: 0.5140 - val_accuracy: 0.8806\n",
      "Epoch 467/500\n",
      "26/30 [=========================>....] - ETA: 0s - loss: 0.1300 - accuracy: 0.9471\n",
      "Epoch 467: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1251 - accuracy: 0.9483 - val_loss: 0.4899 - val_accuracy: 0.8774\n",
      "Epoch 468/500\n",
      "26/30 [=========================>....] - ETA: 0s - loss: 0.1420 - accuracy: 0.9303\n",
      "Epoch 468: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1467 - accuracy: 0.9279 - val_loss: 0.5052 - val_accuracy: 0.8677\n",
      "Epoch 469/500\n",
      "26/30 [=========================>....] - ETA: 0s - loss: 0.1388 - accuracy: 0.9363\n",
      "Epoch 469: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1397 - accuracy: 0.9343 - val_loss: 0.5604 - val_accuracy: 0.8581\n",
      "Epoch 470/500\n",
      "27/30 [==========================>...] - ETA: 0s - loss: 0.1295 - accuracy: 0.9398\n",
      "Epoch 470: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1313 - accuracy: 0.9397 - val_loss: 0.5655 - val_accuracy: 0.8581\n",
      "Epoch 471/500\n",
      "26/30 [=========================>....] - ETA: 0s - loss: 0.1237 - accuracy: 0.9435\n",
      "Epoch 471: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1188 - accuracy: 0.9451 - val_loss: 0.5331 - val_accuracy: 0.8645\n",
      "Epoch 472/500\n",
      "25/30 [========================>.....] - ETA: 0s - loss: 0.1301 - accuracy: 0.9438\n",
      "Epoch 472: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1349 - accuracy: 0.9386 - val_loss: 0.5024 - val_accuracy: 0.8677\n",
      "Epoch 473/500\n",
      "26/30 [=========================>....] - ETA: 0s - loss: 0.1410 - accuracy: 0.9255\n",
      "Epoch 473: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1350 - accuracy: 0.9290 - val_loss: 0.5353 - val_accuracy: 0.8516\n",
      "Epoch 474/500\n",
      "25/30 [========================>.....] - ETA: 0s - loss: 0.1279 - accuracy: 0.9463\n",
      "Epoch 474: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1300 - accuracy: 0.9440 - val_loss: 0.5514 - val_accuracy: 0.8452\n",
      "Epoch 475/500\n",
      "27/30 [==========================>...] - ETA: 0s - loss: 0.1712 - accuracy: 0.9039\n",
      "Epoch 475: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1688 - accuracy: 0.9053 - val_loss: 0.5765 - val_accuracy: 0.8323\n",
      "Epoch 476/500\n",
      "25/30 [========================>.....] - ETA: 0s - loss: 0.1373 - accuracy: 0.9413\n",
      "Epoch 476: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1415 - accuracy: 0.9397 - val_loss: 0.4747 - val_accuracy: 0.8677\n",
      "Epoch 477/500\n",
      "24/30 [=======================>......] - ETA: 0s - loss: 0.1326 - accuracy: 0.9284\n",
      "Epoch 477: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.1362 - accuracy: 0.9300 - val_loss: 0.5436 - val_accuracy: 0.8548\n",
      "Epoch 478/500\n",
      "25/30 [========================>.....] - ETA: 0s - loss: 0.1342 - accuracy: 0.9388\n",
      "Epoch 478: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1292 - accuracy: 0.9440 - val_loss: 0.5271 - val_accuracy: 0.8581\n",
      "Epoch 479/500\n",
      "27/30 [==========================>...] - ETA: 0s - loss: 0.1372 - accuracy: 0.9352\n",
      "Epoch 479: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1374 - accuracy: 0.9322 - val_loss: 0.5065 - val_accuracy: 0.8710\n",
      "Epoch 480/500\n",
      "24/30 [=======================>......] - ETA: 0s - loss: 0.1451 - accuracy: 0.9336\n",
      "Epoch 480: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1406 - accuracy: 0.9354 - val_loss: 0.5235 - val_accuracy: 0.8581\n",
      "Epoch 481/500\n",
      "26/30 [=========================>....] - ETA: 0s - loss: 0.1210 - accuracy: 0.9375\n",
      "Epoch 481: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1211 - accuracy: 0.9376 - val_loss: 0.5012 - val_accuracy: 0.8710\n",
      "Epoch 482/500\n",
      "25/30 [========================>.....] - ETA: 0s - loss: 0.1177 - accuracy: 0.9425\n",
      "Epoch 482: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.1219 - accuracy: 0.9419 - val_loss: 0.5110 - val_accuracy: 0.8677\n",
      "Epoch 483/500\n",
      "25/30 [========================>.....] - ETA: 0s - loss: 0.1328 - accuracy: 0.9250\n",
      "Epoch 483: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1353 - accuracy: 0.9247 - val_loss: 0.5866 - val_accuracy: 0.8613\n",
      "Epoch 484/500\n",
      "25/30 [========================>.....] - ETA: 0s - loss: 0.1239 - accuracy: 0.9350\n",
      "Epoch 484: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.1219 - accuracy: 0.9354 - val_loss: 0.5309 - val_accuracy: 0.8774\n",
      "Epoch 485/500\n",
      "29/30 [============================>.] - ETA: 0s - loss: 0.1299 - accuracy: 0.9440\n",
      "Epoch 485: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 0.1298 - accuracy: 0.9440 - val_loss: 0.5938 - val_accuracy: 0.8516\n",
      "Epoch 486/500\n",
      "27/30 [==========================>...] - ETA: 0s - loss: 0.1222 - accuracy: 0.9456\n",
      "Epoch 486: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 0.1201 - accuracy: 0.9483 - val_loss: 0.5619 - val_accuracy: 0.8548\n",
      "Epoch 487/500\n",
      "27/30 [==========================>...] - ETA: 0s - loss: 0.1334 - accuracy: 0.9398\n",
      "Epoch 487: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 0.1355 - accuracy: 0.9386 - val_loss: 0.5597 - val_accuracy: 0.8452\n",
      "Epoch 488/500\n",
      "24/30 [=======================>......] - ETA: 0s - loss: 0.1326 - accuracy: 0.9297\n",
      "Epoch 488: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.1342 - accuracy: 0.9311 - val_loss: 0.4956 - val_accuracy: 0.8613\n",
      "Epoch 489/500\n",
      "25/30 [========================>.....] - ETA: 0s - loss: 0.1485 - accuracy: 0.9262\n",
      "Epoch 489: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.1495 - accuracy: 0.9268 - val_loss: 0.5754 - val_accuracy: 0.8581\n",
      "Epoch 490/500\n",
      "23/30 [======================>.......] - ETA: 0s - loss: 0.1314 - accuracy: 0.9348\n",
      "Epoch 490: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1333 - accuracy: 0.9354 - val_loss: 0.4858 - val_accuracy: 0.8806\n",
      "Epoch 491/500\n",
      "27/30 [==========================>...] - ETA: 0s - loss: 0.1264 - accuracy: 0.9410\n",
      "Epoch 491: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1284 - accuracy: 0.9408 - val_loss: 0.5046 - val_accuracy: 0.8742\n",
      "Epoch 492/500\n",
      "26/30 [=========================>....] - ETA: 0s - loss: 0.1358 - accuracy: 0.9387\n",
      "Epoch 492: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1338 - accuracy: 0.9397 - val_loss: 0.4992 - val_accuracy: 0.8645\n",
      "Epoch 493/500\n",
      "29/30 [============================>.] - ETA: 0s - loss: 0.1428 - accuracy: 0.9300\n",
      "Epoch 493: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.1426 - accuracy: 0.9300 - val_loss: 0.5110 - val_accuracy: 0.8806\n",
      "Epoch 494/500\n",
      "25/30 [========================>.....] - ETA: 0s - loss: 0.1245 - accuracy: 0.9388\n",
      "Epoch 494: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1298 - accuracy: 0.9354 - val_loss: 0.5102 - val_accuracy: 0.8742\n",
      "Epoch 495/500\n",
      "29/30 [============================>.] - ETA: 0s - loss: 0.1277 - accuracy: 0.9343\n",
      "Epoch 495: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.1276 - accuracy: 0.9343 - val_loss: 0.4985 - val_accuracy: 0.8710\n",
      "Epoch 496/500\n",
      "25/30 [========================>.....] - ETA: 0s - loss: 0.1275 - accuracy: 0.9425\n",
      "Epoch 496: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1258 - accuracy: 0.9440 - val_loss: 0.4942 - val_accuracy: 0.8742\n",
      "Epoch 497/500\n",
      "25/30 [========================>.....] - ETA: 0s - loss: 0.1349 - accuracy: 0.9413\n",
      "Epoch 497: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1364 - accuracy: 0.9397 - val_loss: 0.5212 - val_accuracy: 0.8516\n",
      "Epoch 498/500\n",
      "25/30 [========================>.....] - ETA: 0s - loss: 0.1278 - accuracy: 0.9362\n",
      "Epoch 498: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1258 - accuracy: 0.9365 - val_loss: 0.5035 - val_accuracy: 0.8774\n",
      "Epoch 499/500\n",
      "26/30 [=========================>....] - ETA: 0s - loss: 0.1391 - accuracy: 0.9267\n",
      "Epoch 499: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1386 - accuracy: 0.9279 - val_loss: 0.5098 - val_accuracy: 0.8548\n",
      "Epoch 500/500\n",
      "25/30 [========================>.....] - ETA: 0s - loss: 0.1156 - accuracy: 0.9425\n",
      "Epoch 500: val_accuracy did not improve from 0.91290\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.1260 - accuracy: 0.9354 - val_loss: 0.5578 - val_accuracy: 0.8484\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.2110 - accuracy: 0.9175\n",
      "Test loss: 0.21103845536708832\n",
      "Test accuracy: 0.9175257682800293\n",
      "Test Stats\n",
      "28/28 [==============================] - 1s 3ms/step\n",
      "F-Score on Test Set: 0.9146919431279621\n",
      "Accuracy on Test Set: 0.9175257731958762\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.95      0.92       437\n",
      "           1       0.95      0.89      0.91       436\n",
      "\n",
      "    accuracy                           0.92       873\n",
      "   macro avg       0.92      0.92      0.92       873\n",
      "weighted avg       0.92      0.92      0.92       873\n",
      "\n",
      "[[415  22]\n",
      " [ 50 386]]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "X_train_val = load('../../BEST SET/X_train_fft.joblib')\n",
    "y_train_val = load('../../BEST SET/y_Train.joblib')\n",
    "X_test = load('../../BEST SET/X_test_fft.joblib')\n",
    "y_test = load('../../BEST SET/y_Test.joblib')\n",
    "\n",
    "quarter = X_train_val[::4]\n",
    "quarter_labels = y_train_val[::4]\n",
    "second_qtr = X_train_val[1::4]\n",
    "second_qtr_labels = y_train_val[1::4]\n",
    "third_qtr = X_train_val[2::4]\n",
    "third_qtr_labels = y_train_val[2::4]\n",
    "fourth_qtr = X_train_val[3::4]\n",
    "fourth_qtr_labels = y_train_val[3::4]\n",
    "\n",
    "X_train_val = np.append(quarter, second_qtr, axis=0)\n",
    "X_train_val = np.append(X_train_val, third_qtr, axis=0)\n",
    "y_train_val = np.append(quarter_labels, second_qtr_labels, axis=0)\n",
    "y_train_val = np.append(y_train_val, third_qtr_labels, axis=0)\n",
    "X_test = np.append(X_test, fourth_qtr, axis=0)\n",
    "y_test = np.append(y_test, fourth_qtr_labels, axis=0)\n",
    "\n",
    "\n",
    "input_sh = X_train_val.shape\n",
    "input_sh = X_train_val.shape\n",
    "\n",
    "\n",
    "best_model = create_model(\"Adam\", 3, 126, 0.25314584250529726, 68, 0.3938336292925794, 39, 0.2584061977691302)\n",
    "\n",
    "# Define a ModelCheckpoint callback to save the best model weights\n",
    "checkpoint = ModelCheckpoint(filepath='test.h5', \n",
    "                              monitor='val_accuracy', \n",
    "                              save_best_only=True,\n",
    "                              mode='max',\n",
    "                              verbose=1)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.25, random_state=42)\n",
    "\n",
    "# Train the model with the callback\n",
    "history = best_model.fit(X_train, y_train,\n",
    "                    epochs=500,\n",
    "                    validation_data=(X_val, y_val),\n",
    "                    callbacks=[checkpoint])\n",
    "\n",
    "# After training, load the best weights\n",
    "best_model.load_weights('test.h5')\n",
    "\n",
    "# Evaluate the model using the best weights\n",
    "loss, accuracy = best_model.evaluate(X_test, y_test)\n",
    "print(f'Test loss: {loss}')\n",
    "print(f'Test accuracy: {accuracy}')\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, f1_score\n",
    "print(\"Test Stats\")\n",
    "predictions_test = best_model.predict(X_test)\n",
    "binary_predictions = np.where(predictions_test >= 0.5, 1, 0)\n",
    "\n",
    "f1_scores = f1_score(y_test, binary_predictions)\n",
    "print(\"F-Score on Test Set:\", f1_scores)\n",
    "\n",
    "accuracy_test = accuracy_score(y_test, binary_predictions)\n",
    "print(\"Accuracy on Test Set:\", accuracy_test)\n",
    "print(classification_report(y_test, binary_predictions))\n",
    "print(confusion_matrix(y_test, binary_predictions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test on INTL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31/31 [==============================] - 1s 8ms/step - loss: 2.1697 - accuracy: 0.7233\n",
      "Test loss: 2.1696653366088867\n",
      "Test accuracy: 0.723296046257019\n",
      "Test Stats\n",
      "31/31 [==============================] - 0s 2ms/step\n",
      "F-Score on Test Set: 0.6777251184834123\n",
      "Accuracy on Test Set: 0.7232960325534079\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.88      0.76       484\n",
      "           1       0.83      0.57      0.68       499\n",
      "\n",
      "    accuracy                           0.72       983\n",
      "   macro avg       0.75      0.73      0.72       983\n",
      "weighted avg       0.75      0.72      0.72       983\n",
      "\n",
      "[[425  59]\n",
      " [213 286]]\n"
     ]
    }
   ],
   "source": [
    "from joblib import load, dump\n",
    "import numpy as np\n",
    "import tensorflow as tf \n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from tensorflow.keras.optimizers import SGD, Adam, RMSprop\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "import optuna\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "X_train_val = load('../../BEST SET/X_train_fft.joblib')\n",
    "y_train_val = load('../../BEST SET/y_Train.joblib')\n",
    "X_test = load('../../BEST SET/international_fft.joblib')\n",
    "y_test = load('../../BEST SET/international_labels.joblib')\n",
    "\n",
    "input_sh = X_train_val.shape\n",
    "\n",
    "\n",
    "best_model = create_model(\"RMSprop\", 2, 124, 0.01786723081280845, 76,0.4373552242267542)\n",
    "\n",
    "best_model.load_weights('./Model Dumps/LSTM-Base2.h5')\n",
    "\n",
    "# Evaluate the model using the best weights\n",
    "loss, accuracy = best_model.evaluate(X_test, y_test)\n",
    "print(f'Test loss: {loss}')\n",
    "print(f'Test accuracy: {accuracy}')\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, f1_score\n",
    "print(\"Test Stats\")\n",
    "predictions_test = best_model.predict(X_test)\n",
    "binary_predictions = np.where(predictions_test >= 0.5, 1, 0)\n",
    "\n",
    "f1_scores = f1_score(y_test, binary_predictions)\n",
    "print(\"F-Score on Test Set:\", f1_scores)\n",
    "\n",
    "accuracy_test = accuracy_score(y_test, binary_predictions)\n",
    "print(\"Accuracy on Test Set:\", accuracy_test)\n",
    "print(classification_report(y_test, binary_predictions))\n",
    "print(confusion_matrix(y_test, binary_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
